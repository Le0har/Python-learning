[
    {
        "Название статьи": "Магия динамического маппинга. Реализация универсальной обработки файлов нефиксированной структуры на Python",
        "Дата публикации": "2024-06-05, 15:58",
        "Автор статьи": "spectr_dev ",
        "Статья целиком": [
            "Привет! На связи Никита Ильин из Spectr, Backend-разработчик с опытом более 5 лет.",
            "Один из проектов, с которым мы работаем, — IBP-платформа для планирования и прогнозирования спроса и продаж в ритейле. В статье поговорим о конкретной реализации для одной из задач в рамках этой платформы на Python и Django. При этом сама концепция может быть реализована абсолютно на любом фреймворке или платформе: Spring, .NET, Laravel.",
            "Мы разрабатываем IBP-платформу для крупной корпорации, где на основе данных, которые поступают из смежных систем, строятся прогнозы и аналитика. И одна из областей — работа с огромным количеством файлов из внешних источников: чтение, обработка, загрузка и запись всех этих данных в БД. При этом существует большое количество различных источников и форматов этих файлов.",
            "Глобально задача состоит в том, чтобы осуществить загрузку из внешних источников в единую внутреннюю систему для последующего анализа и прогноза.",
            "Информация об источниках.",
            "Сейчас в системе около 350 источников. При этом одновременно может поступать до 100 штук новых.",
            "Важное уточнение: один источник — это файл уникальной структуры. Если названия столбцов различаются, то это уже новый файл, новый источник данных.",
            "Информация о файле. Это обычный классический csv-формат. Разделителем может быть либо «;» либо «,». Число колонок — от 5 до 200, но распарсить нужно только количество, которое обозначено в техническом задании. В нашем случае доходило до 40 колонок, все остальное — системные поля. Число строк всегда большое — от 5 млн до 20 млн.",
            "Ниже представлен классический пример файла с условными обозначениями.",
            "Отлично, вся информация о данных у нас на руках! Что же делаем? Тут возможны два пути.",
            "Первая идея, которая у нас возникла, — статическая реализация или, по-другому, «решение в лоб». При такой реализации мы для каждого источника пишем свой парсер и применяем его — эффективно и быстро. Поговорим о преимуществах чуть подробнее.",
            "Быстрая разработка. Раньше уже существовало какое-то количество источников и мы уже знали, как работать по этому пайплайну: без лишних оптимизаций и размышлений. То есть мы приоритезируем источники, отдаем наиболее важные по значимости в первую очередь, а сами занимаемся другими.",
            "Работает надежно, как швейцарские часы. Когда мы пишем парсер на конкретный источник, мы можем написать на него тест. И перед тем как выкатывать, надо посмотреть — а точно ли поведение такое же, как ожидается? Тут мы говорим про стабильность и проверенность того, что наш парсер работает.",
            "Без непредвиденных side-эффектов. Все парсеры строго императивны и изолированы и, благодаря этому, более стабильны. Ведь когда мы говорим про enterprise-разработку, мы точно не хотим, чтобы один наш парсер сломал всю систему.",
            "Сходить на нужный внешний сервер по SFTP.",
            "Именно SFTP, потому что это спецификация нашей задачи, мы так договорились, нам так комфортно общаться между серверами.",
            "Забрать файл.",
            "Применить парсер.",
            "Сохранить в БД.",
            "Работа аналитика. Сверка с локальной структурой БД и проектирование перевода из внешнего имени во внутреннее. Нам нужно узнать, что, где и как хранится, какая между всем этим связь. Данные формируются в виде спецификации, требований и отдаются разработчику.",
            "Работа Backend-разработчика.",
            "Написание кода для валидации файла и его перевода. То есть это сам код, который осуществляет разбор данных в соответствии с ожидаемой структурой.",
            "Добавление конфигурации в общую структуру парсеров: на какой сервер идем, куда и как данные сохраняем. Мы обобщили все на уровне кода, а теперь надо это явно прописать, куда мы идем, чтобы получить данные о конкретном источнике.",
            "Например, когда у нас SFTP-сервер, я не думаю, что вы будете писать одни и те же четыре строчки кода в каждом парсере. Скорее всего, это будет какая-то функция или класс, которому передаем имя файла и доступы для SFTP-сервера, чтобы он пустил в систему.",
            "Работа тестировщика. Отладка, тесты на деве, стейдже на обрезанных данных. На этом этапе мы выявляем и устраняем ошибки в коде парсера. То есть тестировщик должен проверить две вещи:",
            "работает ли код. То есть он берет маленький кусочек файла, например 100–1000 строчек, и смотрит, встали ли данные в БД, в интерфейс, не свалилось ли что-то и работает ли функционал в целом;",
            "какая скорость. Нужно понять, удовлетворяем ли мы скорости загрузки и нет ли каких-то проблем.",
            "Работа тестировщика и DevOps-инженера. Пуско-наладка на реальных данных. Далее мы все выкатываем на прод, делаем первые итерации, проверяем, все ли работает: встали ли данные в интерфейс, ничего ли не потерялось, выдерживаем ли по ресурсам.",
            "Время разработки пропорционально количеству парсеров. Для разработки одного парсера нам нужно 5 дней. Сверка со структурой БД занимает 1 день, написание парсера — 2 дня, на отладку/тесты и пуско-наладку закладываем еще по одномуу дню.",
            "На один парсер — 5 дней, а на 100 — целых 1,5 года. Процесс, конечно, можно оптимизировать и вести параллельно: аналитику не ждать разработчика, а разработчику не ждать тестировщика. Но тем не менее все это в сумме — большой объем очень рутинной работы.",
            "Время доработок пропорционально количеству парсеров. Классический пример: есть 200 источников, при этом появилось требование о том, что столбец — это не целое число, а число с плавающей запятой. И теперь нужно это отвалидировать, чтобы все было в порядке, иначе данные не будут сходиться. А в случае переиспользования кода (DRY) нужно еще приложить усилия к тому, чтобы подумать, как это сделать. К тому же нужно заново пройти по пайплайну. Мы сделали изменения — значит, нам нужно все заново проверить, посмотреть, выкатить и замерить все парсеры.",
            "По итогу формула будет такой: время множественного изменения = время одного изменения * число парсеров. То есть время на расширение числа парсеров или валидируемых ими полей эквивалентно времени разработки. Не получится делать быстрее дублирующуюся работу, придется делать с такой же скоростью.",
            "Расширение числа парсеров или валидируемых ими полей запускает пайплайн заново. Подобная ситуация случилась лично с нами. Изначально было N парсеров, далее убрали 10 и потом добавили еще 15 сверху. А после этого в 20 имеющихся парсерах изменился состав файла и добавился еще один внешний сервер. Приходится начинать все сначала: аналитика –> разработка –> тестирование.",
            "Ключевой вопрос в этой всей ситуации — как преодолеть проблемы «решения в лоб» и сделать результат нашей работы максимально самодостаточным? Мы подумали об этом и пришли к другой идее — динамической реализации.",
            "Давайте вспомним No-code-приложения, например Tilda. Или такой конструктор мобильных приложений, где вы тащите формы, а затем система сама выполняет работу. Код генерируется — вы наслаждаетесь. Примерно то же самое мы сделали в рамках нашего проекта.",
            "В один момент мы подумали: «А что, если разработать пользовательский интерфейс, который позволит самостоятельно решать задачи, минуя аналитика, разработчика и тестировщика?» То есть пользователь сможет сам создать описание для своих действий в админке в виде шаблона. Затем наш магический механизм обработает созданный шаблон. А динамический парсер интерпретирует файлы, соответствующие структуре, описанной в шаблоне, без необходимости дополнительной ручной обработки.",
            "Эту идею мы назвали — шаблон динамического маппинга.",
            "На картинке ниже представлено, как это все можно изобразить с точки зрения пользовательского интерфейса. В этом списке темплейт — наш источник. Мы его назвали шаблоном.",
            "Далее представлено, какие примерно атрибуты могут быть у этого шаблона:",
            "название (name) — на что смотреть в интерфейсе;",
            "внешняя система (external system) — на какой сервер нужно пойти, чтобы достать конкретный файлик (уникальное системное имя шаблона);",
            "имя файлика и директория, в которой он лежит (filepath) — путь к файлу на сервере, с которым связан этот шаблон, и здесь же и имя файла;",
            "системное обозначение для источника (system name) — выбор из внешних систем, куда мы будем подключаться, чтобы туда идти по пути выше.",
            "Дальше мы уже говорим о том, что внутри этого источника. Это находится в отдельной сущности — attributes. Эта сущность включает в себя такие элементы, как:",
            "name — чтобы человеку было на что смотреть в интерфейсе;",
            "system_name — уникальное системное имя поля шаблона, которое мы должны искать в файле;",
            "type — тип данных поля, такие как float, str, str_alpha_numeric, date, int, bool;",
            "field_representations (представления поля) — JSON-структура, представляющая отображение поля на БД;",
            "template (шаблон, Foreign Key) — связь шаблона с общей инфой, к которому относится данное поле.",
            "На каждом этапе присутствует валидация, которая проверяет, например, наличие файлика, полей в файлике, соответствие типу. И, в конечном итоге, это все может записаться в БД.",
            "Сейчас мы построили чисто концептуальное решение. Давайте разберемся, какой результат нам бы принес этот подход — поговорим о его преимуществах.",
            "Элемент продукта закончен",
            "Такой формат реализации сокращает все возможные согласования. Вместо написания отдельного парсера для каждого нового файла и его источника создаем шаблоны, описывающие структуру файла и определяющие соответствие полей самостоятельно. Это существенно сокращает процесс работы, к тому же позволяет сразу же проверить результат. И это будет работать уже завтра. Сегодня написал — завтра это уже готово, сегодня придумал — завтра уже на проде валидируешь новый файл.",
            "Настройка и поддержка, которая оптимизирует время",
            "В случае со статической реализацией каждый день на протяжении полутора лет придется заниматься переводом спецификаций в код. Естественно, это не творческая и скучная задача. Мы все-таки хотим закрыть эту задачу и, конечно, сделать это наиболее интересным для нас способом.",
            "При динамической реализации настройка и поддержка будут намного интереснее, чем просто сконструировать «решение в лоб» и сидеть полтора года переводить спецификации.",
            "Возникает ряд вопросов. А что если где-то что-то отвалится? А почему данные не загружаются? А как вообще это все сохранить в табличку и как это все будет выглядеть? А как эти 20 млн строчек обработать? Явно придется над всем этим поразмыслить. 5 часов подумать — 1 час написать код.",
            "Масштабируемость",
            "Появляются новые требования:",
            "добавить в N-количестве источников проверку на дубликаты и действие, которое надо совершать, если они есть;",
            "а еще в M-источниках добавить проверку на отрицательность.",
            "При этом непонятно, будут ли все эти поля в файле.",
            "Так у нас появились дополнительные атрибуты у поля шаблона:",
            "required (обязательное) — флаг, указывающий, является ли поле обязательным для заполнения;",
            "can_be_negative (может быть отрицательным) — флаг, указывающий, может ли поле содержать отрицательные значения;",
            "contained_in_md_table (содержится в таблице md) — имя таблицы md, в которой содержится это поле (если применимо);",
            "contained_in_md_attribute (содержится в атрибуте md) — имя атрибута md, в котором содержится это поле (если применимо);",
            "duplicates_in_table (дубликаты в таблице) — имя таблицы, в которой разрешены дубликаты этого поля (если применимо);",
            "duplicates_attribute (атрибут дубликатов) — атрибут, определяющий дубликаты этого поля (если применимо);",
            "duplicates_action (действие с дубликатами) — выбор из действий по обработке дубликатов: обновление или пропуск.",
            "И на все это есть 5 дней — вспоминаем сроки разработки одного парсера. В такой системе мы на этапе валидации прописываем новые условия один раз, а дальше клиент уже сам работает с шаблонами и сам отвечает за выбранные им параметры.",
            "admin API (CRUD — в админке) — могут вносить изменения и создавать новые записи;",
            "user API (Read — для всех пользователей) — есть возможность только читать.",
            "Python и Django — это решение удобно для нас, к тому же мы используем его с начала работы над проектом.",
            "Blazingly-Fast Polars — о том, почему мы выбрали именно этот инструмент, рассказывал наш тимлид в статье Битва медведей: Pandas против Polars. Если кратко: этот вариант для наших вариантов использования работает быстрее, чем Pandas.",
            "Paramiko — библиотека для подключения по SFTP. Очень красиво и надежно.",
            "SQLAlchemy — в качестве дополнительной ОRМ. Удобный интерфейс, быстро и красиво.",
            "Здесь мы перевели сущность модели в сущность шаблона, которая была до этого. Из интересного — здесь есть функция temporary_table_name, в ней мы получаем temporary-имя. Это название временной таблицы. О том, для чего нам нужна временная таблица, поговорим чуть позже.",
            "Далее то же самое делаем для TemplateField. Можно просто взять, перевести на другой язык — и все готово.",
            "Как я и говорил, у админов будет полный набор CRUD-операций: чтение, создание, обновление. Но следующий момент более интересный.",
            "Вот эти два поинта ниже нам нужны для того, чтобы у клиента в интерфейсе была возможность посмотреть, какие у нас есть таблички и атрибуты и поля этой таблички.",
            "Классические представления — обычные классы доступа. Здесь идет пагинация по страницам. Далее, когда мы предоставляем список шаблонов, мы передаем только основную информацию о них — название. А когда отдаем только один шаблон, мы его отдаем вместе с атрибутивным составом, чтобы пользователь мог убедиться, что у него все правильно загружается.",
            "На этом этапе мы получаем какой-то путь файла у этого темплейта, то есть не пишем явно — идти на такой-то сервер. А просто говорим — взять у темплейта название файла. И на выходе получается путь файла.",
            "Далее через библиотеку мы подключаемся по SFTP, забираем файл.",
            "У темплейта есть филды — template.fields. Мы их забираем — это и будут наши правила валидации.",
            "django_file = InMemoryUploadedFile",
            "validate_file(django_file, list(template.fields.all()))",
            "BasicFileHeadersValidator. В процессе валидации проверяем наличие данных, дублирование колонок, наличие требуемых колонок. Если какой-то из этих пунктов не проходит, мы отправляем пользователю ошибку, так как нам незачем загружать файл, у которого нет требуемых нам колонок. Мы знаем, что он заведомо сохранит его туда, куда нам не надо.",
            "BasicFileReaderValidator / GeneralValuesFieldsValidator. Базовое чтение (проверка на кодировку, соответствие строк размеру), перевод названий колонок согласно шаблону, нормализация данных и проверка их на соответствие типам, генерация дата-фрейма.",
            "Даже если взять 10 млн строк, с учетом того, что параллельно работает 100 источников, cкорее всего, они упадут по памяти.",
            "Что с этим делать? Классический вариант — разбиение на чанки. В результате этого получается кусок файла, c которым можно продолжать работать. Далее с ним проводим соответствующие этапы валидации и формируем соответствующий дата-фрейм. Но в такой структуре важно, чтобы мы не захотели сделать дополнительную логику — например, агрегацию, дезагрегацию, суммирование, фильтрацию, категоризацию. Для всех этих работ мы используем сохранение во временную таблицу, а потом при необходимости все это сохраняем в основную БД.",
            "Если требование состоит в том, что решение нужно кастомное, то это делает уже другой разработчик в рамках другой задачи. Он идет во временную таблицу, забирает все нужные данные и, соответственно, с ними делает то, что нужно ему. При этом временную таблицу нужно каждый раз чистить перед загрузкой, иначе вы упадете по памяти в БД или будете работать со старыми данными.",
            "Что будем использовать дальше? Во-первых, SQLAlchemy. Собираем из дата-фрейма название колонок, делаем сущности колонок, берем название таблицы и с помощью контекстного менеджера вставляем какое-то количество записей. В нашем случае — 1 млн.",
            "Что такое контекстный менеджер",
            "В Python можно использовать удобный декоратор для этого паттерна. Перед началом мы напишем такой connection_url, где мы вставляем доступ к БД. Здесь конструкция try-finally говорит о том, что мы пытаемся отдать наш движок подключения к БД. Но в любом случае, какие бы ошибки ни были, этот движок будет в конечном итоге закрываться. Это нужно для того, чтобы в БД не висело открытое подключение.",
            "Берем нужную нам модель представления данных, написанную на Django.",
            "Здесь есть специфичный для Django код. Но я почти уверен, что таким образом можно получить табличку из основной БД на любом другом языке.",
            "Дальше мы смотрим поля у этой таблицы и пытаемся найти ее первичный ключ. Для нашего проекта специфично то, что может быть три разных первичных ключа, в зависимости от таблички: обычный ID, External ID либо Name ID.",
            "Итак, мы получили название первичного ключа. Далее мы идем в БД и проверяем, какие записи с этими ID уже есть.",
            "Мы берем их, кладем в оперативную память.",
            "После этого мы идем по дата-фрейму и проверяем, есть ли у нас такая запись.",
            "Если не смогли найти с этим ID объект среди тех, что мы вытащили из БД, то сохраняем ее как новый объект, кладем в какую-то структуру (в нашем случае список) и затем позже создадим их в БД.",
            "Но если мы все-таки смогли найти этот ID, то берем и меняем у найденного объекта все атрибуты.",
            "Условно, в файле одно значение, в БД другое значение для этой строчки — поменяли, сохранили в структуру. И здесь происходит множественное сохранение/обновление. Важно использовать batch_size. Потому что создавать запрос на миллион строчек — невыгодно. Делаем batch_size, разбиваем структуру на множество запросов и делаем с ним.",
            "batch_size=1000 — число объектов в каждом запросе.",
            "Заклинание освоено! А теперь поговорим о недостатках такого решения.",
            "Трудности в отладке и тестировании. Это интересно, но приходится думать, тратить время, пытаться понять, где ошибка, почему не можем распарсить — затратно дебажить.",
            "Строгость в соблюдении принципов. Важно соблюдать всю последовательность действий, про которую мы говорили ранее (этапы реализации динамического решения). Мы не должны запихнуть весь файл в систему, а на выходе говорить, что сделаем полностью кастомную работу.",
            "Ограничение в функциональности. Здесь как раз идет речь об отсутствии некоторых кастомных полей, все динамическое. И если вы тронете хотя бы одну строчку, то для всех остальных это изменение автоматически применится. Один файл, но его тяжело поддерживать.",
            "Дополнительное время на валидацию. Многие знают, что функции в Python вызывать довольно дорого, поэтому вы можете столкнуться с тем, что валидация может занимать много времени. Ни в коем случае не пытайтесь запустить код, который кажется примерно рабочим. Посмотрите, какие есть практики использования функции, почему нельзя использовать лямбду-функцию в цикле и т. д.",
            "Дополнительное время на сохранение во временную БД и/или основную БД. Мы говорим, что у нас есть временная таблица, и после нее вы можете делать некоторые кастомные операции, которые вы хотите. Но в то же время мы тратим минуту-две на то, чтобы сохранить эти данные, и, плюс ко всему, они занимают какую-то дополнительную память на диске.",
            "Как мы можем обойти эти минусы:",
            "Расширение поддерживаемых форматов файлов. Здесь имеются в виду Excel и все подобные структуры. Нам не потребуется писать отдельный парсер для каждого такого формата. Можно просто добавить новый вариант ридера, добавить его в общую структуру и смотреть на расширение файла перед загрузкой.",
            "Оптимизация производительности. Пытаемся ускорить валидацию, оптимизировав все возможные куски, меняя пайплайн вызова валидаторов.",
            "Развитие интерфейсов и возможных конфигураций. Админу нужно дать возможность делать batch_size для источника. То есть он знает, что, например, у него будет 100 валидированных колонок, и здесь batch_size в 100 тыс. — это слишком много, а 1,5 тыс. — уже нормально. Пусть у него будет возможность заменить на любое значение, которое он сам посчитает нужным. Дальше, если говорить о других возможных интерфейсах, у нас есть проверка на отрицательность на случай дублирования (мы меняем эту строчку или пропускаем ее и говорим, что нам все равно, есть ли она).",
            "И на этом все! В статье я поделился своим опытом, основанным на решении конкретной задачи. Надеюсь, что материал поможет вам выбрать правильное решение в аналогичной ситуации и покажет, как можно творчески подходить к задачам. А если у вас появятся вопросы — буду рад на них ответить в комментариях к этой статье!",
            "Статья подготовлена по мотивам доклада Никиты Ильина, Backend-разработчика в Spectr, на митапе #DevTalks. Ссылка на запись доклада:"
        ]
    },
    {
        "Название статьи": "Получение списка людей посещающих определенные места",
        "Дата публикации": "2024-06-05, 15:12",
        "Автор статьи": "fire64 ",
        "Статья целиком": [
            "Представьте: вы ведете Telegram-канал о животных и хотите пригласить в него посетителей зоопарка. Или вам нужно собрать контакты потенциальных клиентов, посещающих определенный торговый центр. Как это сделать?",
            "Полиция может легко получить такую информацию от мобильных операторов, но что делать обычному человеку?",
            "Ответ – использовать Telegram и его функцию \"Люди рядом\" в сочетании с Python-скриптом.",
            "\"Люди рядом\": эта функция Telegram показывает контакты пользователей, находящихся поблизости, с примерным расстоянием до них (500 м, 1 км, 2 км и 3 км). Отображаются первые 100 ближайших контактов.",
            "Python-скрипт: с помощью библиотеки telethon можно получить доступ к этой информации и автоматизировать процесс сбора контактов.",
            "Установка:",
            "Скачайте и установите Python с официального сайта: https://www.python.org/downloads/",
            "Установите необходимые модули:",
            "Регистрация приложения Telegram:",
            "Зарегистрируйте свое приложение на сайте Telegram: https://core.telegram.org/api/obtaining_api_id",
            "Важно: используйте свой реальный номер телефона, привязанный к Telegram-аккаунту, а не бота.",
            "Создание скрипта:",
            "Создайте файл с расширением .py и вставьте код скрипта (https://pastebin.com/pYPA8PF0).",
            "Замените следующие значения:",
            "api_id = (ваш API ID)",
            "api_hash = (ваш API Hash)",
            "phone_number = '' (ваш номер телефона)",
            "Запустите скрипт.",
            "Выберите на карте нужное местоположение.",
            "Укажите радиус поиска (500, 1000, 2000 или 3000 метров).",
            "Нажмите кнопку \"Начать поиск\".",
            "Скрипт автоматически получит список пользователей Telegram, находящихся в заданном радиусе, и добавит их в ваши контакты.",
            "Данные обновляются Telegram каждые 30 минут.",
            "Отображаются только первые 100 пользователей.",
            "Поиск работает только в регионе, к которому привязан ваш номер телефона.",
            "Важно использовать этот метод этично и уважительно по отношению к другим пользователям. Не рассылайте спам и не используйте полученную информацию в незаконных целях.",
            "Помните: эта информация предназначена только для ознакомления. Перед использованием подобных скриптов убедитесь, что вы не нарушаете правила Telegram и законодательство вашей страны."
        ]
    },
    {
        "Название статьи": "Автоматизация Juniper на Python",
        "Дата публикации": "2024-06-05, 15:05",
        "Автор статьи": "svg30 ",
        "Статья целиком": [
            "Добрый день, Habr. Меня зовут Сергей, я старший эксперт в компании Ростелеком. В зоне моей ответственности эксплуатация сетевого оборудования компании (в основном маршрутизаторы и коммутаторы). Когда счет устройств, с которыми необходимо работать, идет на тысячи, обойтись без автоматизации решительно невозможно. И значительная часть моей деятельности - автоматизация работы с оборудованием различных моделей и производителей, для чего, как правило, я использую скрипты на Python. В нашей компании используется оборудование различных производителей, но значительная доля оборудования - Juniper. Поэтому, в данной статье я хотел бы описать возможные подходы к автоматизации сбора информации и обслуживанию оборудования именно данного производителя. Отличительной чертой оборудования Juniper является то, что и коммутаторы, и маршрутизаторы работают под управлением одной и той же операционной системы JunOS. Конечно, доступные функции и команды отличаются в зависимости от модели, но общая операционная система позволяет использовать один и тот же подход в работе со всем спектром оборудования вендора. Так, вне зависимости от модели применяется единый подход внесения изменений в конфигурацию, когда изменения применяются только после их фиксации командой commit. У некоторых других производителей это поведение различается от модели к модели, что усложняет процесс автоматизации.",
            "В качестве подопытного для демонстрации практических действий будет выступать коммутатор Juniper EX2200.",
            "Стандартный алгоритм работы с оборудованием Juniper (да и не только с ним) без автоматизации выглядит очень просто: подключаемся к устройству по протоколу SSH (вряд ли кто-то сейчас будет использовать для этого Telnet, если только это не единственный возможный вариант), вводим в консоли команды и туда же получаем ответ устройства. Поэтому логично, что самый простой способ автоматизации работы - автоматизация подключения к устройству и отправки на него команд. И для этого в Python есть отличная библиотека - netmiko. Она не только позволяет работать с оборудованием через различные протоколы (SSH, telnet, serial), но и для большого количества известного оборудования берет на себя заботу о поиске приглашений командной строки, отключению постраничного вывода, переходу в режим конфигурации и обратно и многом другом. Да, для некоторых вендоров различные модели ведут себя несколько по-разному и требуется \"обработка напильником\", но для Juniper все работает отлично.",
            "Установить библиотеку можно, например, так: pip install netmiko Чтобы сразу перейти к практике, вот пример подключения к устройству и вывод результата работы команды show version",
            "Указываем необходимые данные для подключения в виде IP, логина и пароля (можно указать ключ для подключения как в закомментированных строках), а так же тип устройства, к которому подключаемся. Указание на тип устройства позволит библиотеке правильно обрабатывать вывод (например, отсекать от информации баннер в виде {master:0}, который добавляет juniper при ответе) и выполнить подготовительные команды:",
            "Отдельно хочу обратить внимание на вторую команду. Стандартное поведение junos - при вводе части команды и пробела, система автоматически дополняет команду до полного названия. А в ответе от устройства в первой строке библиотека ожидает увидеть ту команду, которую она отправила на сервер. Если бы мы не отключили данную опцию, при вводе сокращенной команды, например sho ver вместо show version, отклик от устройства не совпадал бы с переданным запросом и выскочило бы исключение по таймауту. Но, важно помнить, что наше поведение спасает от сокращенного ввода команд, но не может спасти, например, от нескольких пробелов. Если в пример выше мы вызовем dev.send_command('show version'), то получим сообщение:",
            "Если писать команду целиком, то такое вряд ли случится, но при автоматическом формировании команды из нескольких частей вполне можно и пропустить два подряд пробела. А потом долго искать, что же не так.",
            "На этом, в части получения информации, можно было бы закончить. Смотрим на устройстве вывод конкретной команды, определяем, где в выводе находится интересующая нас информация, и выполняем команду с помощью send_command. А дальше в дело вступает самый обычный Python со всеми его возможностями по обработке текста, которые мы используем для разбора ответа от устройства. Но иногда удобнее воспользоваться дополнительными возможностями, которые предоставляет нам JunOS. К примеру, требуется получить значение какого-то конкретного параметра, а выбирать его из большой простыни вывода не очень удобно. Тогда мы можем попросить JunOS выдать ответ уже в структурированном виде, который и будем в дальнейшем разбирать. Можно получить вывод в формате XML, для чего просто добавить после команды | display xml. К примеру, если мы хотим получить список всех маршрутизаторов из протокола IS-IS, то можем сделать этот так (очевидно, что тут уже нам нужен будет маршрутизатор):",
            "А начиная с 15 версии JunOS так же есть возможность вывода в JSON добавив | display json.",
            "Для изменения конфигурации в библиотеке netmiko у объекта BaseConnection имеется метод send_config_set. Метод принимает одну или список команд конфигурации, проверяет, активен ли режим конфигурирования, если нет, заходит в него и выполняет переданные команды. Попробуем изменить, к примеру, описание одного из портов.",
            "На выходе получим следующее:",
            "Если теперь посмотреть описание этого порта на устройстве, то мы увидим что оно совершенно не изменилось",
            "В этом нет ничего удивительного, если мы посмотрим на сообщение, которое было выведено после вызова метода send_config_set. Там нам сразу напоминают, что The configuration has been changed but not committed. То есть изменения то мы в базу конфигурации внесли, но не выполнили commit и изменения не применились. Поэтому, при работе с juniper мы должны в метод send_config_set передать не только список команд, но и параметр exit_config_mode = False, чтобы автоматически не выходить из режима конфигурации и можно было бы применить изменения. Дополнительно можно через вызов команды show | compare посмотреть, какие же изменения мы собираемся применить. Вот работающий код",
            "На выходе получим",
            "Первая часть вывода показывает, какие изменения мы будем применять, вторая - результат выполнения команды commit. Сообщение commit complete показывает, что изменения успешно применились.",
            "В принципе, тут можно было бы и закончить с данной библиотекой, но хотелось бы чуть глубже копнуть особенности JunOS. Ранее я писал, а потом показывал на примере, что в JunOS (впрочем, как и у многих других вендоров) изменения в конфигурации не вступают в силу сразу, а требуют явного указания на их применение через команду commit. То есть в процессе конфигурирования можно даже удалить интерфейс, через который вы в данный момент подключены, если это повышает удобство конфигурирования (например, для выполнения команды copy interface). Пока вы не применили изменения, можно не волноваться. Но есть нюанс. Когда вы заходите в режим конфигурирования с помощью команды configure и начинаете вносить изменения, вы правите общую конфигурационную базу, а значит все изменения, которые вы внесли, доступны и для других пользователей.",
            "Например, кто-то из консоли решил поправить интерфейс ge-0/0/11 и ввел команду set interfaces ge-0/0/11 description TEST1. После чего на некоторое время задумался, что делать дальше. А в этот момент вы запускаете написанный ранее скрипт по изменению интерфейса ge-0/0/10. В выводе вы можете с удивление увидеть, что почему-то show | compare показывает, что вы меняете два интерфейса вместо одного.",
            "Хорошо, что команды тут безобидные, а если бы кто-то действительно удалил интерфейс, на котором наше устройство подключено - седых волос бы сильно прибавилось у многих. Чтобы избежать подобных неприятных моментов у JunOS есть специальные режимы конфигурирования - exclusive и private (есть еще, но это основные, на мой взгляд). В режиме exclusive никто, кроме нас, не может изменять базу настроек. Для использования данного режима изменим наш скрипт, добавив в вызов send_config_set параметр config_mode_command='configure exclusive'",
            "Второй рассматриваемый режим - private. В этом режиме вы работает с собственной копией конфигурационной базы, поэтому правки могут вносить несколько пользователей параллельно, не мешая друг другу. Код для этого режима не сильно отличается от предыдущего",
            "Для обоих режимов работы, в отличие от просто configure, все изменения будут сброшены после выхода. Кроме того, если кто-то параллельно уже внес какие-то изменения в общую базу (в режиме configure), то, что configure private, что configure exclusive, не смогут войти в режим конфигурирования, выдав в консоли error: shared configuration database modified, а в случае netmiko - выдав по таймауту исключение Pattern not detected: '(?s:Entering configuration mode.*\\\\].*#)' in output Еще одна интересная возможность JunOS - автоматический откат последних изменений. Если вы опасаетесь незапланированных последствий изменений, можете использовать commit confirmed. Данная команда применяет изменения и ожидает заданное количество времени (задается в минутах). Если в течение этого времени не совершен повторный commit, изменения откатываются до предыдущего состояния. Netmiko так же поддерживает данную команду. Просто добавьте параметры в вызов dev.commit(confirm = True, confirm_delay = 5) и через 5 минут, если вы повторно не вызовете commit(), все изменения будут отменены.",
            "В связи с огромной популярностью Python в части автоматизации работы с сетевым оборудованием, многие компании выпускают библиотеки для работы с их оборудованием. Не исключение и Juniper, который выпустил библиотеку PyEZ. Данная библиотека работает по протоколу NETCONF, который надо не забыть включить перед использованием. Для этого выполним команду set system services netconf ssh port 830 Перед использованием библиотеку необходимо установить pip install junos-eznc",
            "Ниже - простейший код подключения к коммутатору и вывод информации о нем.",
            "В результате будет выведена базовая информация об устройстве",
            "Основной класс для подключения - Device. При создании туда передается вся нужная информация, такая как адрес устройства, логин и пароль или ключ для входа. Кроме того, можно передавать дополнительные параметры, например gather_facts=False, если нам не нужно собирать информацию об устройстве при подключении.",
            "После того, как мы подключились к устройству можно начинать работать с ним. Метод работы \"в лоб\" - вызов метода dev.cli() с передачей в него команды, которую необходимо выполнить. Однако, данный метод не является рекомендованным, о чем нам будет сообщено в возвращаемом значении. Например, при выполнении print(dev.cli('show version')) мы получим следующее:",
            "Библиотека сама подсказывает, что основной метод работы с устройством - вызов rpc процедур. Узнать имя процедуры и название параметров можно двумя способами: вызвать команду прямо на коробке, указав | display xml rpc",
            "вызвать функцию cli_to_rpc_string которая и вернет правильное название функции и параметров",
            "Теперь, зная название функции и параметров, можем ее вызвать. Однако, вывод будет в формате xml, поэтому необходимо будет несколько дополнительных действий, чтобы отобразить результат",
            "Тут мы уже можем разобрать этот xml, ну или сразу получать только нужные нам значения. Вот, для примера, получение значения оптической мощности входящего сигнала из полученного ответа",
            "Примерно того же можно добиться, если включить фильтрацию при отправке запроса. Для этого надо передать параметр use_filter=True при создании подключения.",
            "Чтобы закрыть тему с вызовом методов надо еще добавить, что информацию можно получать не только в формате xml, но и в json, и в виде текста (правда тут тоже будет xml, но один общий тег output внутри которого будет текстовый вывод команды). Для этого при вызове методе передать нужный формат",
            "Небольшое замечание по поводу метода cli. Да, данный метод не рекомендуемый. Но иногда бывает, что другое решение найти трудно. К примеру, однажды мне надо было собирать информацию с резервного RE маршрутизатора. Обычно для этого заходишь на резервный RE командой request routing-engine login other-routing-engine и дальше смотришь, что нужно. Библиотека подсказала, что для этого есть метод request_login_to_other_routing_engine, который вроде как даже работал, но дальше новые вызовы выводили информацию все так же с основного RE. Сходу победить не получилось, поэтому пришлось решить задачу в лоб через cli.",
            "Но все же, чаще всего результатом запроса информации является некая таблица. Например, данных по интерфейсам или маршрутам. Основная полезность данной библиотеки - механизм получения таких данных. Можно, конечно, вызвать rpc.get_interface_information(media=True) и обрабатывать полученный вывод. Но есть метод проще.",
            "Импортируем нужную нам таблицу и запрашиваем данные. Данные приходят уже в готовом для дальнейшей работы виде.",
            "Вот пример простейшего вывода всех интерфейсов в виде таблицы",
            "В составе библиотеки уже имеются готовые определения для наиболее востребованных запросов. Вот тут содержится их полный перечень. Но это еще не все. Поскольку подготовленных определений много, но охватывают они все же далеко не все, мы можем самостоятельно определять нужные для нас таблицы и получать данные. Для примера, создадим таблицу для получения данных с оптических модулей. Описание таблиц производится с использованием YAML синтаксиса. Вот часть вывода команды show interfaces diagnostics optics | display xml, которая нам будет нужна для создания yml файла",
            "Создадим файл optic.yml с определением того, как мы будем получать данные из нашего вывода ```",
            "В файле мы описываем две сущности Table и View. Если попробовать объяснить по-простому, то Table описывает откуда мы вообще берем данные, какая именно часть этих данных будет использоваться для наполнения таблицы и что будет ключом. А View показывает, как мы будем из исходных данных формировать элементы таблицы. Возможно объяснение немного путанное, но, надеюсь, при разборе конкретного примера станет понятнее. Итак, начала мы описываем таблицу - OpticTable rpc - указываем имя процедуры, которая и является главным источником данных для заполнения таблицы. Имя этой процедуры мы можем получить разными способами, например вызвав команду и указав display xml rpc",
            "item - название элемента, из которого мы будем формировать строки нашей таблицы. В нашем случае данные для строки нашей таблицы содержатся в элементе <physical-interface>. Поскольку этот элемент находится прямо внутри основного элемента ответа <interface-information>, то мы указываем только его имя. Но если элемент является вложенным, надо будет указать весь путь. key - каждая строка нашей таблицы состоит из двух частей - ключа и данных. То, как будет сформирован ключ, указываем после ключевого слова key. Тут может быть как одно поле, так и несколько. В рассматриваемом случае это имя интерфейса, которое указано сразу в дочернем элементе нашего <physical-interface> view - то, что из полученных в результате вызова процедуры данных будет использовано для заполнения строки таблицы, указывается в отдельной сущности View. Название этой сущности тут мы и укажем.",
            "OpticView Тут в списке fields мы просто указываем, какие элементы необходимо выбрать, чтобы заполнить таблицу. Обратим внимание, что необходимые нам данные содержатся не напрямую в элементе <physical-interface>, который был указан в качестве item при описании таблицы, поэтому необходимо указать полный пусть до данного элемента. Так же мы можем брать не только значение внутри элемента, но и из его свойств и привести его к указанному типу данных, как продемонстрировано в строке temp: {optics-diagnostics/module-temperature/@celsius: float} Создав файл с описанием нужной нам таблицы, мы можем загрузить его встроенными средствами библиотеки и создать нужные нами классы:",
            "После этого можно будет использовать новые классы точно так же, как и встроенные:",
            "В результате получим требуемую информацию:",
            "Для настройки нашего устройства в библиотеке PyEZ имеется специальный класс Config",
            "Далее мы можем приступать к внесению изменений в конфигурацию устройства. Здесь работает стандартная схема Juniper: сначала изменения вносятся в базу конфигурации, но не применяются, для применения необходимо вызвать commit. Библиотека позволяет подходить к процессу очень гибко. Так, новые настройки можно вносить в 3 форматах:",
            "txt или conf - стандартный формат на базе фигурных скобок, который мы видим в консоли оборудования, если вывести show configuration",
            "xml - настройки в формате xml, пример такой конфигурации можно посмотреть с помощью show configuration | display xml",
            "set - форматирование в формате set. Фактически стандартный способ конфигурирования через консоль. Пример можно посмотреть с помощью show configuration | display set Изменения можно вносить как напрямую передав строку с настройками, так и загрузив данные из файла. Для примера создадим 3 файла в разных форматах и поменяем description на 3 разных интерфейсах config.txt",
            "config.xml",
            "config.set",
            "Далее, загружаем все три конфигурации. Для этого используется метод load. Если мы загружаем изменения из файла, формат определяется по расширению файла. Если же передаем в виде строки, то формат надо будет указать явно.",
            "В последней строке мы с помощью функции diff выводим все отличия новой конфигурации от текущей. В результате вывод будет такой:",
            "Обратим внимание на два момента. Мы пока не применили наши изменения и рабочая конфигурация осталась такой же, что и была. Но если мы закомментируем в нашем скрипте все строки с cfg.load, оставив только вызов diff, то, при повторном запуске, вывод будет точно таким же, как и выше. То есть, несмотря на то, что наш скрипт завершился, мы меняли общую базу конфигурации и изменения там так и остались. Чтобы начать вносить изменения заново, нужно откатить уже внесенные изменения. Это можно сделать через rollback()",
            "Мы откатили изменения к исходному состоянию, загрузили только одно изменение и вывели разницу. Результат, как и ожидалось",
            "Как я и писал ранее, в части про Netmiko, править общую базу бывает весьма опасно и есть два альтернативных подхода: режим private, где мы работаем с собственной копией базы конфигурации, и режим exclusive, когда мы блокируем общую базу и правим ее единолично. Для примера работы в режиме private запустим следующий код",
            "Мы дважды входим в режим конфигурирования и выводим разницу между рабочей конфигураций и новой версией Ожидаемо получим",
            "После выхода из режима конфигурирования и последующего входа изменения, внесенные в общую базу, остались. Теперь добавим mode='private'",
            "При первом запуске нас поджидает разочарование в виде вылетевшего исключения:",
            "Изменения в общей базе остались и зайти в режим private не получается. Откатим изменения вручную и повторно выполним код.",
            "Тут тоже все, как и ожидалось. После выхода из режима private все изменения обнулились. С режимом exlusive тоже все просто. Вот код",
            "Вывод",
            "Мы блокируем базу конфигурации с помощью lock(), вносим изменения. Но как только мы делаем unlock(), все изменения обнуляются.",
            "Еще одна приятная возможность загрузки файлов конфигураций - использование их как шаблонов, которые могут формироваться динамически. Для их формирования можно использовать синтаксис Jinja2. Для примера переделаем файл xml для правки сразу нескольких интерфейсов config.xml",
            "А сам скрипт",
            "В методе load() мы вместо параметра path указываем template_path, кроме того, в template_vars мы передаем все переменные, которые будут использоваться при обработке шаблона. В результате вывод будет следующий",
            "После того, как мы загрузили новые изменения в конфигурацию, нам остается сделать две вещи:",
            "Проверить, что изменения применимы (в консоли это была бы команда commit check)",
            "Применить изменения Первое мы выполняем при помощи метода commit_check, который возвращает True, если изменения могут быть применены, либо вызывает исключение, из которого можно получить информацию, что же именно в новом конфиге не понравилось. Ну и применение изменений производится при помощи метода commit(). Данный метод может принимать следующие параметры:",
            "Параметр",
            "Описание",
            "comment",
            "Текстовое описание коммита.",
            "confirm",
            "Возможность автоматического отката изменений. Указывается число минут до отката",
            "sync",
            "Булевское значение. Если True будет выполнена команда commit synchronize, которая синхронизирует изменения на обоих RE в конфигурациях, где есть два RE",
            "detail",
            "Булевское значение. Если True, the commit() выведет дополнительную информацию по выполнению. Используется для отладки",
            "force_sync",
            "Булевское значение. Если True, выполняет commit synchronize force.",
            "full",
            "Булевское значение. Если True все процессы будут принудительно перечитывать конфигурацию, даже если изменения их не затрагивают.",
            "Проверим на практике. Выведем описанием интерфейсов до изменений, загрузим и применим изменения и повторно посмотрим описания.",
            "Результат работы",
            "На этом я бы хотел завершить данную статью. Конечно, показанные тут подходы не единственные возможные для автоматизации Juniper, есть еще множество других возможностей, от простого вызова команды ssh и работы с вводом/выводом до использования еще более высокоуровневых библиотек, вроде Napalm, и систем управления конфигурациями, вроде Ansible. Если будет интерес, про это можно написать отдельные статьи. Буду рад комментариям и с удовольствием отвечу на вопросы читателей."
        ]
    },
    {
        "Название статьи": "Как в Tele2 автоматизировали тестирование SAP ERP с помощью Python",
        "Дата публикации": "2024-06-05, 13:34",
        "Автор статьи": "a_valeeva ",
        "Статья целиком": [
            "Привет, Хабр! Меня зовут Анастасия Валеева, я – руководитель группы обеспечения качества в Tele2. Наша команда работает в большинстве своём с SAP ERP, и мы не понаслышке знаем, что автоматизация данной платформы — дело далеко не тривиальное. В этой статье я хочу поделиться с вами, как и зачем мы автоматизировали тестирование с помощью Python.",
            "Зачем мы это придумали",
            "SAP ERP – гибкий инструмент в руках нашей команды. Мы дорабатываем функциональность системы под потребности конкретного бизнес сегмента. Эти изменения производятся по запросу бизнес-пользователей. Объём и влияние доработок могут быть различными, но одно остаётся неизменным – каждая доработка является уникальной. Таким образом, это не простое устранение багов и улучшения текущего функционала, не изменение версионности продукта после оптимизации, а, как правило, абсолютно новый «продукт» в системе. В случае автоматизации функционального тестирования нам потребуется писать автотест на каждую доработку/разработку, что занимает больше времени, чем ручное тестирование (написание автотеста, отладка, оптимизация) + данный автотест с каждой новой разработкой будет уже неактуален, и нужно будет создавать новые и новые из раза в раз. Делаем выводы, что автоматизировать функциональные тексты для нас нерелевантно. А вот регрессионные тесты, которые мы проводим после каждого изменения системы, представляют собой более шаблонные варианты, шаги повторяются, и от их автоматизации есть профит.",
            "Сейчас мы работаем с SAP ERP и интегрированными продуктами (FileNet, BW, Fiori), однако, импортозамещение идёт полным ходом, и мы проводим пилотный проект по миграции на новую платформу. Так или иначе, созданный нами инструмент для автотестов универсален и может быть применён в работе с новой системой.",
            "Как выбирали инструмент автоматизации",
            "Из множества инструментов автоматизации мы выбрали для ознакомления четыре наиболее совместимых с SAP ERP:",
            "SAP Scripting;",
            "Tricentis Tosca;",
            "eCatt;",
            "CBTA.",
            "Анализируя, мы исходили из трёх основных для нас факторов: скорость освоения, простота и гибкость, а также бюджет. По каждому из инструментов мы отметили свои плюсы и минусы, собрали информацию в единую таблицу. И вот что у нас получилось.",
            "По количеству зелёных блоков мы увидели, что нашим критериям в большей степени соответствует SAP Scripting.",
            "Принцип работы данного инструмента состоит в том, что он записывает все действия пользователя в системе, на выходе формирует файл в формате .vbs, который в последующем можно запускать в SAP. Соответственно, при запуске этого файла система будет повторять ваши предварительно записанные шаги. Кроме того, данный файл можно корректировать: удалять лишнее, дописывать недостающее или даже полностью переписать. Для этого необходимо открыть файл либо в блокноте, либо в любом другом редакторе, работающем с кодом.",
            "В процессе пилотирования SAP Scripting помимо технических вопросов мы решали несколько административных задач: удобство использования, гибкость, кастомизация, универсальность, прозрачность.",
            "Мы хотели внедрить такой инструмент, который будет полезен не только группе тестирования, но и другим смежным группам нашего подразделения. И поскольку мы говорим об автоматизации, одним из основополагающих факторов для нас было минимальное участие человека в этом процессе. Согласитесь, часто хочется просто нажать на волшебную кнопку \"РАБОТАТЬ\", чтобы оно всё само заработало :)",
            "Добиться данного магического эффекта «работает само» нам помог Python. За это отвечала коллега из моей команды — она написала скрипт для робота, который сейчас работает буквально по одному клику.",
            "Что касается прозрачности, то мы пошли по пути, доступному для любого пользователя. Для этого «прикрутили» Python к файлу Excel. Это означает, что сейчас провести регресс может любой сотрудник — достаточно зайти в файл автотеста и нажать кнопку «СТАРТ».",
            "Бизнес-процесс состоит из набора бизнес-операций. Например, создание логистического заказа состоит из заведения заказа, смены статуса подписания договора, деблокирования заказа и создания счёта-фактуры. Для обеспечения полного регрессионного тестирования мы автоматизируем всю цепочку шагов. На выходе получаем Excel-документ со скриншотами и подробной информацией по каждому шагу тестирования. Причём регресс может запустить любой пользователь, не только тестировщик, это доступно в том числе для менеджеров со стороны бизнеса. А полученные данные (скрипты) можно использовать также для генерации тестовых данных.",
            "Существует несколько способов выполнения автотестов.",
            "1. Отдельно по каждому бизнес-процессу. По каждому модулю финансовой системы SAP ERP создан файл Excel, в котором есть кнопка вызова макроса. По вызову этой кнопки запускается Visual Basic for Applications. VBA обращается к системе SAP и вызывает на выполнение ранее записанный скрипт vbs.Таким образом, мы можем выполнять тестирование по отдельному модулю или бизнес-операции.",
            "2. По всему модулю или нескольким модулям.Для этих нужд как раз используется Python. Наш робот обращается к SAP, открывая рабочее окно. Далее вызывает необходимые файлы Excel, которые работают по описанному принципу макросов на VBA. Таким образом, мы получаем следующую цепочку:",
            "При этом пользователю необходимо только единожды нажать кнопку ВЫПОЛНИТЬ.",
            "Запуск SAP GUI",
            "Заведение функции для чтения файла Excel",
            "Подключение к Excel",
            "На каждом листе в Excel есть подробная входная и выходная информация, при этом входную информацию можно корректировать. Большая часть листов связана между собой, чтобы можно было провести всю цепочку на одних данных, а последующие шаги не зависели от дополнительных действий пользователя.",
            "Все скриншоты, которые создаются в процессе регресса, генерируются вместе с документами и проводками. Лишние скриншоты можно удалить прямо на странице в Excel. При необходимости сотрудник может по номеру документа найти нужную проводку или операцию в SAP. Это является прозрачным и удобным способом анализа логов тестирования.",
            "Рядом с каждым шагом в файле появляется текстовое описание, статус «успешно» или «не успешно» пройден шаг и цветовой индикатор — зеленый означает успешно пройденный этап, красный сигнализирует об ошибках.",
            "Если ошибка является блокирующей для системы и дальнейшее прохождение шагов невозможно, то скрипт остановится, выдаст информационное сообщение и сохранит изменения в файл. Если ошибка не влияет на последующие шаги, то скрипт продолжит работу, а в конце выдаст лог в Excel с отображением корректных и некорректных шагов. При таком раскладе у нас появляется возможность увидеть проблему в моменте и исправить её.",
            "Также, завершение работы скрипта сопровождается звуковым оповещением.",
            "Дополнительно мы настроили автоматическое удаление листов из общей папки через три дня после их создания.",
            "Что в итоге",
            "Мы посчитали, сколько рабочего времени ручных тестировщиков мы экономим при использовании инструмента автоматизации. Получилось, что на один кейс при использовании SAP Scripting мы тратим 31 секунду против 148 секунд при ручном тестировании. Таким образом, 80% времени инженеров высвободилось на другие задачи, и мы смогли повысить эффективность тестирования.",
            "Данный вариант автоматизации является гибким к изменениям. В случае переезда на другую финансовую систему мы перенаправим нашего робота на Python на вызов нужной нам программы. Сейчас одна из наших основных задач – обеспечить качество работы текущего функционала и уже на этой надёжной основе реализовывать улучшения и внедрять новые фичи. Для нашей команды автоматизация тестирования SAP ERP стала интересным и полезным опытом, а бизнесу предоставила доступную, понятную и безотказную систему проверки рабочих процессов."
        ]
    },
    {
        "Название статьи": "Быстрый интерфейс, быстрый деплой",
        "Дата публикации": "2024-06-05, 11:01",
        "Автор статьи": "funtastick ",
        "Статья целиком": [
            "Салют! Не так давно создатели знаменитого pydantic выпустили новый фреймворк — FastUI, который позволяет создавать пользовательские интерфейсы с помощью декларативного кода на Python. В этой статье рассмотрим создание простого приложения и деплой его в Cloud Apps. ❯ Обзор По заявлению авторов фреймворка, фронтенду не нужно (и не следует) знать ничего о приложении, которое вы создаете, вместо этого он должен просто предоставить все компоненты, необходимые для создания интерфейса, а затем бэкенд может предоставить необходимые данные и параметры компонентов. Реализовано это таким образом, что FastUI инкапсулирует описание компонентов интерфейса и в виде классов, затем запускается простое React приложение, которое обращается к эндпоинтам за данными и компонентами. ❯ Пример Для примера давайте напишем простое приложение, предоставляющее информацию о городах из списка с возможностью пагинации. Данные для экспериментов любезно предоставили создатели фреймворка. Для начала опишем pydantic модель и функцию для чтения данных. from pydantic import BaseModel, Field, TypeAdapter import json from pathlib import Path class City(BaseModel): id: int city: str = Field(title=\"Name\") city_ascii: str = Field(title=\"City Ascii\") lat: float = Field(title=\"Latitude\") lng: float = Field(title=\"Longitude\") country: str = Field(title=\"Country\") iso2: str = Field(title=\"ISO2\") iso3: str = Field(title=\"ISO3\") admin_name: str = Field(title=\"Admin Name\") capital: str = Field(title=\"Capital\") population: float = Field(title=\"Population\") def cities_list() -> list[City]: cities_file = Path(__file__).parent / \"cities.json\" with open(cities_file, \"r\", encoding=\"utf-8\") as f: data = json.load(f) cities = [City(**city) for city in data] return cities Далее напишем каркас для нашего примера, с помощью FastAPI. Опишем два роута, первый возвращает необходимые компоненты и данные, а второй — простое React приложение, которое отвечает за запрос и отображение компонентов, полученных из предыдущего. from fastapi import FastAPI from fastapi.responses import HTMLResponse from fastui import AnyComponent, FastUI from fastui import components as c, prebuilt_html from fastui.components.display import DisplayLookup, DisplayMode from fastui.events import BackEvent, GoToEvent app = FastAPI() @app.get(\"/api/cities\", response_model=FastUI, response_model_exclude_none=True) def cities_view(page: int = 1, country=None): cities = cities_list() page_size = 10 # Количество записей в таблице, отображаемых на странице filter_form_initial = {} return c.Page( # Page - базовый контейнер для остальных компонентов components=[ c.Table( # Table - базовая разметка таблицы data=cities[(page - 1) * page_size : page * page_size], #Создаём срез данных для заполнения таблицы data_model=City, #Передаём модель данных columns=[ # Описываем столбцы таблицы DisplayLookup( #Указываем содержимое и размер столбца в процентах field=\"city\", table_width_percent=33 ), DisplayLookup(field=\"country\", table_width_percent=33), DisplayLookup(field=\"population\", table_width_percent=33), ], ), c.Pagination(page=page, page_size=page_size, total=len(cities)), #Кнопки для пагинации ] ) @app.get(\"/{path:path}\") async def html_landing() -> HTMLResponse: \"\"\"Простое React приложение, идёт последним, т.к. соответствует всем маршрутам\"\"\" return HTMLResponse(prebuilt_html(title=\"Большие города\")) Результат работы представлен на рисунке ниже: ❯ Деплой Для деплоя приложений на FastUI можно воспользоваться сервисом Apps, к сожалению рассмотренный фреймворк только набирает популярность, поэтому мы воспользуемся опцией: «деплой из Dockerfile». Для этого достаточно создать Dockerfile и разместить его в корне репозитория. FROM python:3.11 COPY . /app WORKDIR /app RUN pip install -r requirements.txt CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] EXPOSE 8000 Обратите внимание, что при отсутствии в Dockerfile параметра EXPOSE, APPS будет слушать порт 8080 контейнера. Далее достаточно предоставить сервису доступ к аккаунту на github. Затем остаётся следить за логами деплоя: В случае успешного развёртывания появиться удобный дашборд с графиками нагрузки на виртуальную машину: ❯ Заключение В данной статье мы рассмотрели лишь малую часть возможностей фреймворка, однако можно отметить, что FastUI предоставляет новый подход к созданию веб-приложений и позволяет существенно ускорить разработку. Возможно, захочется почитать и это: ➤ Timeweb Cloud CLI ➤ Бесплатный прокси к Docker Hub ➤ Фантастически быстрый деплой веб-приложения ➤ Учимся летать: симуляция эволюции на Rust ➤ Age of Empires – культовая попытка сделать Цивилизацию в реал-тайме Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩"
        ]
    },
    {
        "Название статьи": "Как я создавал аудиоплеер на python с FFmpeg",
        "Дата публикации": "2024-06-04, 18:10",
        "Автор статьи": "Niamorro ",
        "Статья целиком": [
            "Всех приветствую. Сегодня хочу поделиться опытом создания своего первого проекта на Python. Мой проект — это простой аудиоплеер, и я хочу рассказать, как я его создавал, с какими сложностями столкнулся и что из этого вышло.",
            "Выбор языка для первого проекта — это всегда непросто. Я выбрал Python по нескольким причинам:",
            "Простота синтаксиса. Python очень читабельный и понятный, что идеально подходит для новичков.",
            "Богатая стандартная библиотека и сообщество. Множество готовых решений и библиотек, которые можно использовать в своих проектах.",
            "Популярность в разработке. Python — один из самых популярных языков программирования, и навыки работы с ним будут полезны в будущем.",
            "Моя цель была написать простой аудиоплеер, который мог бы играть основные аудиоформаты. Я хотел, чтобы пользователь мог выбирать треки, ставить их на паузу и останавливать, так же изменять скорость проигрывания.",
            "Выбор библиотек занял действительно много времени, так как нужно было выбрать библиотеки которые обновляются, и имеют необходимый мне функционал. Я использовал несколько библиотек:",
            "PySide6: библиотека для создания интерфейсов созданная разработчиками Qt, имеет хорошую поддержку сообщества и регулярные обновления, в дополнение к ней использовал qdarktheme для стилизации интерфейса.",
            "FFmpeg: Универсальный инструмент для обработки видео и аудио.",
            "Sounddevice: Библиотека для воспроизведения и записи звука в Python.",
            "Mutagen: Библиотека для извлечения данных из аудиофайлов.",
            "Выбор файла:",
            "Пользователь выбирает аудиофайл из меню \"Файл\". Поддерживаемые форматы включают MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Выбранный файл передаётся в FFmpeg через подпроцесс для извлечения необработанных аудиоданных. Используемая команда:",
            "Чтение аудиоданных:Аудиоданные считываются блоками и сохраняются в массив NumPy для эффективной обработки.",
            "Регулировка громкости:Регулировка громкости осуществляется путём умножения аудиомассива на коэффициент громкости.",
            "Регулировка скорости воспроизведения:Скорость воспроизведения (например, 2x) управляется через библиотеку sounddevice путём изменения частоты дискретизации.",
            "Поток вывода:Обработанные аудиоданные передаются на аудиовыход через библиотеку sounddevice.",
            "Управление воспроизведением:Элементы управления, такие как воспроизведение/пауза, следующий/предыдущий трек и перемотка, обрабатываются через класс AudioTrigger.",
            "Воспроизведение/Пауза:Использует класс AudioTrigger для начала/остановки аудиопотока.",
            "Следующий/Предыдущий трек:Обновляет текущий индекс трека и загружает следующий/предыдущий трек в плейлисте.",
            "Перемотка:Регулирует позицию воспроизведения, пересчитывая индекс позиции на основе значения ползунка.",
            "Виджет очереди треков:Отображает добавленные ранее папки.",
            "Виджет плейлиста:Отображает содержимое папки.",
            "Виджет информации о треке:Показывает метаданные и обложку для воспроизводимого трека.",
            "Если хотите ознакомиться с исходным кодом или внести свой вклад в проект,",
            "приглашаю вас посетить страницу GitHub проекта. Там вы найдёте весь исходный код аудиоплеера.",
            "Также у проекта есть веб-сайт, где вы можете скачать готовые .exe и .deb пакеты для Windows и Linux. Здесь же доступна подробная документация по установке и использованию программы.",
            "Работа с FFmpeg требовала правильной организации буферизации аудиоданных, чтобы избежать прерываний и задержек при воспроизведении.",
            "Решение: Буферизация данных в массив NumPy.",
            "Треки воспроизводились с неправильной скоростью из-за некорректной частоты дискретизации.",
            "Решение: Я считываю частоту дискретизации трека и открываю аудиопоток с настройками именно для того трека, который в данный момент должен воспроизводиться.",
            "В результате я создал аудиоплеер с основными функциональными возможностями:",
            "Проигрывание аудиофайлов: Поддерживаются популярные форматы MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Управление воспроизведением: Воспроизведение, пауза, остановка, перемотка, следующий/предыдущий трек.",
            "Регулировка скорости воспроизведения: Возможность воспроизводить треки быстрее или медленнее.",
            "Плейлист: Добавление папок с треками.",
            "Информация о треках: Отображение метаданных и обложек альбомов.",
            "Тёмная тема: Благодаря qdarktheme, аудиоплеер имеет современный и стильный интерфейс.",
            "Добавить поддержку потокового аудио: Возможность воспроизводить музыку из интернет-радиостанций и стриминговых сервисов.",
            "Расширить функциональность плейлиста: Добавить возможность создания и сохранения пользовательских плейлистов.",
            "Поддержка эквалайзера: Добавить эквалайзер для настройки звука.",
            "Создание аудиоплеера на Python оказалось полезным опытом. Я научился работать с аудио на низком уровне, обрабатывать потоки и создавать пользовательский интерфейс.",
            "Буду рад любым отзывам и предложениям по улучшению плеера. Спасибо за внимание!",
            "Ссылки:",
            "Исходный код на GitHub",
            "Сайт проекта с загрузкой пакетов"
        ]
    },
    {
        "Название статьи": "Как мониторинг связан с тестированием. Преимущества мониторинга для бизнеса: как экономить время и деньги",
        "Дата публикации": "2024-06-04, 15:59",
        "Автор статьи": "luffity ",
        "Статья целиком": [
            "Привет! Проходя множество собеседований, я не раз слышал вопросы по типу: «Что такое мониторинг?», «Как это связано с тестированием?», «Зачем это нужно?». Для меня, волей случая ставшего специалистом по мониторингу чуть больше года назад, это тривиальные вопросы, однако многие компании либо не знают, что это такое, либо не видят в этом пользы. На одном из последних интервью я услышал интересное мнение от QA Lead о том, что assert должен быть в каждом тесте. Смелое заявление, подумал я. Поэтому, собственно, вы и читаете эту статью.Разберёмся, что такое мониторинг и с чем его едят. А главное, зачем он нужен вообще.",
            "Думаю, начать с небольшого введения обо мне будет наиболее верным для погружения в тему. Сейчас я занимаю должность middle SDET в ООО «МИТ» (если проще, то DIXY Group). Попал я туда как AQA, причём единственным. Я находился в группе по мониторингу корпоративных сервисов, соответственно, кроме меня там были только специалист по мониторингу (мой Lead), DevOps и системный админ. Похоже на стандартное начало анекдота…",
            "Моим заданием на испытательный срок стало написание сервиса по мониторингу интернет-соединения на торговых точках компании. И тут я подумал: какой, блин, мониторинг? С другой стороны, работа на дороге не валяется, тем более настолько приятная. Дело было вечером, делать было нечего…",
            "По итогу этого задания ко мне пришло осознание того, как тесты могут трансформироваться в нечто большее, чем проверки, прогоняемые раз в релиз. Они могут быть целой экосистемой, даже можно сказать: «Глазом Бога» (кто понял отсылку к Форсажу, спасибо).",
            "Начнём мы, конечно же, с поверхности и определим для себя, что такое мониторинг и как он связан с тестированием. В самом распространённом смысле под тестированием понимают сверку ожидаемого и фактического результатов на конечном наборе тестов. Если мы берём стандартные парадигмы тестирования, то такие проверки могут выполняться при добавлении новой фичи, раз в спринт, при релизе и много когда ещё. Однако все эти тестовые прогоны не преследуют цель отслеживать состояние продукта (программного или бизнесового) постоянно.",
            "Как раз здесь и вступает в игру мониторинг. Если о-о-очень грубо сказать, то это, по сути, те же наборы тестов, только несущие цель постоянного наблюдения за состоянием продукта. Но тут стоит уточнить, что это не тестовые кейсы в их привычном понимании.",
            "Думаю, будет проще понять наглядно.",
            "Это всем понятный простой тест. Мы переходим на страницу и ожидаем заголовок. Как можно заметить, для того чтобы исполнить свою главную функцию – сопоставить ожидаемый и фактический результаты, тест содержит в себе assert. Это несомненно верный подход к написанию тестов, так как это позволит более точно валидировать ошибку, а также правильно отобразить её в отчётах, например Allure.",
            "А теперь взглянем на код скрипта мониторинга, который проверяет доступность ресурса.",
            "Сразу бросается в глаза отсутствие assert. Но в таком случае, как такой скрипт вообще можно считать информативным, если он не выводит ошибки? Именно поэтому мы добавим дополнительное действие. Например, найдём какую-то кнопку и нажмём на неё. Теперь, если ресурс не прогрузился или сломался, мы получим TimeoutException и сообщение о том, что именно скрипт не смог сделать.",
            "Возникает вопрос: почему бы тогда точно так же не поставить assert и не ждать лишнее время для выпадения TimeoutException ? Справедливо! Однако возьмём во внимание, что данный скрипт не нацелен на то, чтобы просто проверить доступность ресурса и отследить ошибку в отчёте. Если мы предполагаем, что скрипт гоняется бесконечно, пока смерть сервера не разлучит вас, то отчётом в данном случае будет не Allure, например (хотя я и его прикрутил к скриптам для Project Manager’а), а сервисы для графического отображения типа Grafana или сервисы мониторинга типа Prometheus. Да и сам скрипт, помимо успеха или провала теста, должен собирать ещё кучу полезных данных. В данном примере это может быть время прохождения скрипта, что может дать нам представление о том, в каком состоянии находится сервис. Особенно если учесть, что всегда можно настроить параметры интернет-соединения или любые другие моменты, имитирующие пользователя. И тут мы плавно перейдём к другому вопросу.",
            "Теперь стоит сказать и о том, что мониторинг может быть как на микро-, так и на макроуровне. Под микроуровнем обычно понимают низкоуровневый мониторинг, например, физического оборудования, элемента большого сервиса или что-то подобное. На макроуровне мониторинг предстает как UX-тестирование или тестирование пользовательских путей.",
            "Немного про микроуровень. Вернёмся к проекту по мониторингу интернет-соединения на торговых точках компании. По сути, скрипт достаёт из базы данных ID магазинов, конвертирует их в IP-адреса маршрутизаторов в магазинах и пингует их несколько раз. Помимо того, что в таблице в Grafana этот скрипт отображает «Up» или «Down» в зависимости от доступности каналов, он также собирает время отклика, хранит историю падений и содержит в себе данные об операторе SIM-карты, номере телефона и многое другое. Не очень-то похоже на тест.",
            "Теперь про макроуровень. Высокоуровневый мониторинг уже больше похож на UI/UX-тестирование. В его основе лежит постоянное отслеживание пользовательского пути через UI. Например, для сайта доставки продуктов — от захода пользователя на сайт и выбора товаров до оплаты. Помимо прочего, такой скрипт также собирает множество данных.",
            "В чём, собственно, разница? Основными критериями, отличающими мониторинг от тестирования, являются:",
            "Цель в постоянном наблюдении. Мониторинг — большой брат для ваших сервисов, который безустанно следит за ними;",
            "Сбор данных. Помимо отчётов о тестировании, мониторинг собирает ещё кучу данных;",
            "Быстрое реагирование. Думаю, тут и пояснять не надо. Если у вас есть тестовый сервер или синтетика, то критической баге будет сложно пролезть в прод;",
            "Имитация пользователя. Хоть UX-тесты и тесты пользовательских путей позволяют имитировать действия пользователя, но пишут их далеко не в первую очередь (информация со 100+ собеседований. Всем API подавай, а на пользователей мы кладём...).",
            "Что в итоге польза? Ну, тут я расскажу лучше пару «До и После» примеров.",
            "До разработки мною сервиса мониторинга интернет-соединения на валидацию проблем и выезд оперативной группы на точку уходило два-три рабочих дня. Более того, часто это были ложные вызовы, так как при неработающем основном канале включался резервный. Мониторинг позволил проходить весь процесс за два часа. Процент ложных вызовов за год его работы составляет не более 0,2%. А уж сколько денег это экономит компании, говорить не приходится, если учитывать, что к этому мониторингу подключена вся первая линия поддержки. Во всех магазинах Дикси. По всей России. Даже не думал, что час простоя торговой точки может стоить так много…",
            "А как вам такая новость в ленте: «Основной сайт и сайт доставки магазина Дикси не работают!»? Именно такую новость прочло руководство компании, заваривая утренний кофе. Да, узнавать о падении основных сервисов компании из новостей — это, видимо, не весело. Мне кажется, кофе точно не полезет после такого. Стоит ли говорить, что после этого случая мониторинг был внедрён на все сервисы?",
            "Забавно, правда?",
            "Но остался ещё один вопрос. Специалист по мониторингу и специалист по тестированию — это один и тот же профессионал? Мне кажется, специалист по мониторингу ближе к SDET, чем к AQA. Всё-таки я считаю, что автоматизатор тестирования должен знать и уметь меньше. AQA как бы и должен иметь представление о контейнеризации, но как бы и просто собрать контейнер в Docker достаточно. Специалист по мониторингу должен бы и под каждый свой мониторинг собрать контейнер, и доставить его, и обслужить если что, и k8s знать бы по-хорошему, ноды и воркеры – лучшие друзья. И опять-таки, ты же не знаешь, что может быть важно для бизнеса. Возможно, придётся выйти за рамки PyTest, Selenium и Appium. Уметь разобраться в различных библиотеках, знать асинхронные подходы, парадигмы проектирования, сильные и слабые стороны рабочего языка программирования — всё это важные навыки специалиста по мониторингу. Так что да, SDET более подходящее описание для его деятельности.",
            "Ссылочка на телегу"
        ]
    },
    {
        "Название статьи": "Кратко про Seq2Seq-модели",
        "Дата публикации": "2024-06-04, 09:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Seq2Seq модели — это архитектуры ML, предназначенные для задач, связанных с последовательными данными, типо машинного перевода, суммирования текста, создания описаний к пикчам и прочие задачи, где требуется преобразование одной последовательности в другую.",
            "В этой статье в общих деталях рассмотрим то, как реализуются Seq2Seq модели.",
            "Seq2Seq модели состоят из двух основных частей: энкодера и декодера.",
            "Энкодер преобразует входную последовательность в контекстный вектор, содержащий обобщённое представление всей входной информации. Этот вектор затем используется декодером для генерации выходной последовательности, о декодере чуть ниже.",
            "Перед тем, как подать данные в энкодер, текстовые данные преобразуются в числовые представления с помощью эмбеддинга. Это делается с помощью слоя Embedding, который преобразует каждый токен во входной последовательности в вектор фиксированной размерности. Например, слово milk может быть представлено как вектор размерности 300.",
            "Основу энкодера RNN, обычно реализованные с использованием LSTM или GRU. Эти сети обрабатывают входную последовательность пошагово:",
            "На каждом шаге RNN принимает эмбеддинговое представление текущего токена и скрытое состояние от предыдущего шага.",
            "Выход каждого шага включает новое скрытое состояние, которое передаётся на следующий шаг вместе со следующим токеном.",
            "В конце последовательности RNN генерирует контекстный вектор, который является финальным скрытым состоянием. Этот вектор обобщает всю информацию из входной последовательности и передаётся в декодер для дальнейшей генерации выходной последовательности. Контекстный вектор — это своего рода сжатая версия входной последовательности, включающая в себя её смысл.",
            "Для улучшения качества представления входной последовательности часто используются двунаправленные RNN. В этом случае два RNN работают параллельно: один — слева направо, другой — справа налево. Их состояния объединяются на каждом шаге, что позволяет учитывать как предшествующие, так и последующие слова для каждого токена в последовательности.",
            "Пример реализации энкодера на Keras с LSTM:",
            "Здесь входные данные сначала проходят через эмбеддинговый слой, который преобразует их в векторы фиксированной размерности. Затем эти векторы подаются в LSTM, который на выходе даёт финальные скрытые состояния, использующиеся в качестве контекстного вектора.",
            "В отличие от энкодера, декодер генерирует данные на основе предыдущих предсказаний и контекстного вектора, предоставленного энкодером.",
            "Подобно энкодеру, декодер принимает токены, которые сначала преобразуются в числовые представления с помощью эмбеддинга. Однако, в случае декодера на вход подаются не только реальные данные, но и предсказанные токены на предыдущих шагах.",
            "Декодер также реализован с использованием RNN, как LSTM или GRU. На каждом шаге декодер принимает:",
            "Контекстный вектор от энкодера.",
            "Предыдущий предсказанный токен (или начальный токен для первого шага).",
            "Скрытое состояние от предыдущего шага декодера. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности.",
            "На каждом шаге RNN декодера производит новое скрытое состояние и выходной вектор. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности",
            "Для улучшения качества генерации используется механизм внимания, который позволяет декодеру фокусироваться на различных частях входной последовательности на каждом шаге генерации выходной последовательности. Механизм внимания вычисляет веса для каждого состояния энкодера, определяя важность каждого токена входной последовательности в текущий момент времени.",
            "Пример реализации декодера на Keras с LSTM и механизмом внимания:",
            "Декодер принимает начальные состояния от энкодера и генерирует выходную последовательность.",
            "Машинный перевод — это одна из наиболее базовых задач для Seq2Seq моделей. Реализуем Seq2Seq модельку для перевода с английского на французский язык с использованием Keras.",
            "Для этой задачи будем использовать датасеты французских и английских предложений (они есть на kaggle):",
            "Следующий пример - текстовое суммирование. Это задача генерации краткого представления текста. Реализуем Seq2Seq модель с использованием механизма внимания.",
            "Для этой задачи будем использовать датасет новостей, где заголовок является суммарным представлением статьи:",
            "Реализуем генерацию описаний к изображениям — это задача, где Seq2Seq модели используются для генерации текста, описывающего содержание изображения. Будем использовать предобученную модель InceptionV3 для экстракции признаков изображения и Seq2Seq модельку для генерации текста:",
            "Seq2Seq модели - это очень мощный инструмент для решения задач, связанных с последовательными данными. Они позволяют преобразовывать входные последовательности в выходные с высокой точностью, в особенности при использовании механизмов внимания (об этом не забываем).",
            "В завершение хочу порекомендовать бесплатные вебинары курса ML Advanced:",
            "Современные модели прогнозирования типа TimesNet и TimeGPT",
            "H2O, TPOT, Autokeras - а вы что, за меня и модели строить будете?"
        ]
    },
    {
        "Название статьи": "Как подружить Llama-3 и YouTube имея всего 40 строк кода",
        "Дата публикации": "2024-06-03, 21:57",
        "Автор статьи": "evg_dc ",
        "Статья целиком": [
            "Сделаем Телеграм бота которому можно кинуть ссылку на YouTube видео и поговорить с ним о содержимом этого видео.",
            "За основу возьмем бота работающего на Llama 3-70b из моей прошлой статьи. Можно использовать и любую другую языковую модель включая ChatGPT или локальную запущенную на Ollama.",
            "Создать Телеграм бота и получить его токен (как это сделать, смотрите инструкции на просторах интернета, например здесь).",
            "Зарегистрироваться на Groq и получить api key (нужен VPN).Заходим по этой ссылке, регистрируем аккаунт, генерим ключ. Платежная карта не требуется.",
            "Замените в коде GROQ_API_KEY на api ключ полученный в Groq и TELEGRAM_BOT_TOKEN на токен телеграм бота, все должно быть в кавычках.",
            "После получения сообщения от пользователя ищем в тексте сообщения ссылку на YouTube видео. Делаем это перебирая все слова и проверяя их на наличие URL. Если ссылка на видео найдена, используя библиотеку youtube-transcript-api забираем транскрипцию.",
            "Далее, полученную транскрипцию подставляем языковой модели в виде сообщения от функции. Здесь мы немного обманываем модель, потому что такой функции нет, но лучше делать так чем ставить транскрипцию в системное сообщение. Модель заточена под работу с функциями и все правильно поймет.",
            "Как и в предыдущей версии, бот будет запоминать последние 6 сообщений и поддерживать диалог.",
            "Запускаем скрипт и в Телеграм боте задаем вопрос со ссылкой на видео:",
            "Посмотреть как это работает можно в YouTubeGPT.",
            "Еще есть НашGPT - это как ChatGPT только модель Llama 3-70b."
        ]
    },
    {
        "Название статьи": "Python в Excel жив?",
        "Дата публикации": "2024-06-03, 17:24",
        "Автор статьи": "Gonchar_POTT ",
        "Статья целиком": [
            "Уже больше месяца экспериментирую, исследую, как разные схемы (паттерны) осознанного дыхания влияют на вариабельность сердечного ритма (Heart Rate Variability, HRV на чуждом языке). В скромной, но не совсем уж крошечной Excel-таблице со столбцами “Паттерн”, “HRV”, “Пульс” набралось 258 записей и мне понадобилось выбрать победителя -- дыхательный паттерн, дающий на выходе максимальное значение HRV. Не вручную же сортировать эти записи!",
            "Да, я знаю: есть Pivot Table & Power Query. Но Pivot Table мне не по душе необходимостью после каждого изменения таблицы-источника делать REFRESH, во-первых, избыточной сложностью создания, во-вторых. И просто так не нравятся Pivot Table, что главное. Что же касается Power Query, то сочетание слов вызывает у меня трепет и учащенное сердцебиение: не понимаю, что это за зверь такой и насколько он страшный или полезный.",
            "Поэтому для подсчета результатов -- и выбора победителя -- применил относительно недавно появившуюся в Excel функцию GROUPBY в связке с XLOOKUP. И, раз уж пришлось группировать данные, решил сравнить нативные функции Excel с GROUPBY от Pandas (мы ведь помним, что сейчас Python можно запустить внутри Excel).",
            "Написал простой код:",
            "Поместил код через =PY( в ячейку А1 Excel",
            "И он прекрасно справился с задачей и выдал таблицу с результатами:",
            "breathing_pattern",
            "HRV",
            "HR",
            "8",
            "physiological sighs moderate",
            "59",
            "65",
            "7",
            "physiological sighs light",
            "57",
            "62",
            "1",
            "4.4-6.6",
            "56",
            "59",
            "6",
            "following pulse",
            "55",
            "61",
            "0",
            "4.2-0-6.4-0",
            "53",
            "62",
            "3",
            "6-6",
            "53",
            "61",
            "4",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "61",
            "2",
            "5-5",
            "52",
            "63",
            "5",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "Комбинация функций GROUPBY и XLOOKUP тоже отработала без изъянов:",
            "breathing_pattern",
            "HRV",
            "HR",
            "physiological sighs moderate",
            "59",
            "65",
            "physiological sighs light",
            "57",
            "63",
            "4.4-6.6",
            "56",
            "60",
            "following pulse",
            "55",
            "61",
            "4.2-0-6.4-0",
            "54",
            "63",
            "6-6",
            "54",
            "61",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "62",
            "5-5",
            "53",
            "63",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "* Для внимательных: разница в данных между двумя таблицами -- плод Python-овского округления до целых чисел.",
            "* В “нативном” подходе нет отсечки паттернов с количеством замеров менее шести.",
            "Выводы и наблюдения по теме:",
            "В Python итоговая таблица сама автоматически изменяет размеры при добавлении новых паттернов. GROUPBY by Excel ведет себя так же, а вот связка GROUPBY&XLOOKUP уже потребует редактирования формул: нужно изменять адреса диапазонов ячеек, к которым обращается XLOOKUP.",
            "Вопреки большему размеру Python-код мне кажется проще и для написания, и для чтения-понимания. Хотя писать код в ячейке Excel -- весьма извращенное удовольствие.",
            "Исполнение кода Python требует интернет-соединения.",
            "Выводы и наблюдения не совсем по теме:",
            "Для меня лично схема дыхания “physiological sighs light” (легкие физиогические вздохи) -- оптимальный выбор.",
            "Более шести месяцев я придумывал, зачем мне может понадобиться живущий в Excel Python и наконец нашел.",
            "Буду благодарен за советы и критику. Постараюсь ответить на вопросы."
        ]
    },
    {
        "Название статьи": "Майним крипто-коины с помощью Python и компьютерного зрения",
        "Дата публикации": "2024-06-03, 13:58",
        "Автор статьи": "temabed ",
        "Статья целиком": [
            "После внезапного обогащения энтузиастов, которые поиграли в начале года в приложение Notcoin в телеграм, подобные проекты стали расти как грибы. Да и грибников заметно поприбавилось. Но в данной статье мы не будем касаться тем блокчейна или финансов, а рассмотрим простой пример применения компьютерного зрения для фарма поинтов в самом популярном, после Notcoin, проекте - хомяке комбате. Название явно на что-то намекает, но да ладно.",
            "Это не первый проект, который я автоматизирую, и не самый нуждающийся в этом. Да и без компьютерного зрения с автоматизацией хомяка можно спокойно обойтись. Но с ним, во-первых, интереснее, а во-вторых - это просто хороший пример с минимумом строк кода для демонстрации возможностей библиотеки cv2. Статья, соответственно, предназначена для энтузиастов и начинающих специалистов.",
            "Начнем с того, что мы установим все необходимые зависимости и импортируем их в свой проект. Вот они, слева направо.",
            "С помощью pyautogui наш бот будет управлять мышью. Keyboard пригодится для назначения горячих клавиш, чтобы управлять работой бота. cv2 наградит бота зрением, пусть и компьютерным, с помощью которого тот будет находить совпадения с искомым изображением. А numpy пригодится для работы с большими массивами, но тут он почти для галочки, не бойтесь. Модуль time тоже понадобится, чтобы ставить таймауты в работе программы.",
            "Далее напишем небольшую конструкцию- переключатель.",
            "Функция change при вызове всего лишь меняет значение переменной work, которая будет использована в бесконечном цикле. И если work будет False, работа нашего кода будет останавливаться. И наоборот запускаться, в противоположном случае.",
            "Кстати, забыл упомянуть, что разработчики проекта, над которым мы сейчас проводим эксперимент, большие молодцы, и убрали возможность пользоваться приложением на десктопных устройствах. Поэтому для его запуска понадобится эмулятор Android.",
            "Теперь определим основную логику работы бота:",
            "Он ищет совпадение с изображением полностью заполненной энергии.",
            "Если находит совпадение, ищет изображение монеты и кликает на неё энное количество раз.",
            "И всё это работает в бесконечном цикле.",
            "Значит со скриншота приложения необходимо вырезать две области, которые помечены красным, и разместить их в отдельные файлы, конечно же.",
            "Теперь напишем функцию для кликов по монетке. Она будет принимать путь к исходному изображению, а так же порог чувствительности для компьютерного зрения и интервал (таймаут) в секундах.",
            "Переменной template будет присвоено исходное изображение монетки, но в оттенках серого, так как мы указали в параметрах 0. Это необходимость, так как в оттенках серого компьютер зрит лучше. Сразу вычисляем высоту и ширину исходника, и присваиваем переменным. А далее по ходу исполнения кода он делает скриншот, сравнивает с исходником, получает координаты области с совпадением, и делает 260 даблкликов по ней. Координаты я ищу немного кривовато и в итоге loc содержит большой массив, из которого я использую лишь самые первые координаты, после чего цикл прерываю. Но лучше сделать не смог, извините.",
            "А теперь напишем аналогичную функцию, но с задачей искать совпадение с картинкой полной энергии, после чего вызывать функцию click.",
            "В целом всё аналогично. Добавил лишь ожидание горячей клавиши, чтобы можно было остановить программу в любое время нажатием на тильду (Ё). Ну и для красоты заключил в try-except.",
            "И это всё. Пишем последние строки и запускаем скрипт (не забыв нажать на Ё для запуска логики в цикле).",
            "По сути, этот код многофункционален, и его без труда, с минимальными изменениями, можно переделать под любые другие задачи. На всякий случай оставлю и полную версию кода:",
            "Не судите меня строго по этому скрипту, большую часть жизни я вообще бегал с пистолетиком, и пристрастился к разработке сравнительно недавно, поэтому всего лишь юный падаван в возрасте. Хотя могу писать и большие скучные штуки, но вот писать о них получится дольше, чем сам код. А если будут вопросы- добро пожаловать в телеграм. У меня там небольшой клуб по интересам."
        ]
    },
    {
        "Название статьи": "Сравниваем популярные алгоритмы кластеризации DBSCAN и OPTICS",
        "Дата публикации": "2024-06-03, 13:51",
        "Автор статьи": "evaclick ",
        "Статья целиком": [
            "Привет, Хабр)",
            "Поговорим сегодня о 2 популярных алгоритмах кластеризации — DBSCAN и OPTICS, посмотрим их особенности и сравним",
            "Поехали!",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Ну, DBSCAN в особом в представлении не нуждается, всё-таки один из самых популярных алгоритмов кластеризации. Поэтому по минимуму теории.",
            "DBSCAN (Density-based spatial clustering of applications with noise, плотностной алгоритм пространственной кластеризации с присутствием шума), как следует из названия, оперирует плотностью данных. На вход он просит матрицу близости точек и два параметра — радиус -окрестности и количество соседей .",
            "Эпсилон-окрестность для любого вектора в метрическом признаковом пространстве определяется как множество точек, отстоящих от не более чем на :",
            "где — выбранная метрика (например, евклидовое расстояние).",
            "В общих чертах, алгоритм DBSCAN можно представить как последовательность следующих этапов:",
            "найти новые точки в -окрестности каждой точки и определить основные точки с более чем соседями.",
            "найти связные компоненты основных точек на графе соседей, игнорируя все неосновные точки.",
            "назначить каждую неосновную точку ближайшему кластеру, если кластер является -соседним, в противном случае считаем точку шумом.",
            "Вот так можно использовать DBSCAN из Sci-Kit Learn + с интерактивными ползунками, работает в Colab'е (в Jupyter Notebook какие-то траблы с этим, если кто знает — please, help):",
            "С использованием DBSCAN в Julia и R особых проблем тоже не возникает —",
            "— Julia:",
            "— R:",
            "В идеальном случае DBSCAN может иметь линейную сложность , но не стоит особо на это рассчитывать. Если не пересчитывать каждый раз точек, то ожидаемая сложность — . Худший случай (плохие данные или брутфорс-реализация) — . Наивные реализации DBSCAN любят отъедать памяти под матрицу расстояний — это явно избыточно. Многие версии DBSCAN умеют работать и с более щадящими структурами данных: sklearn и R реализации можно оптимизировать при помощи KD-tree прямо из коробки.",
            "DBSCAN не вычисляет самостоятельно центры кластеров, однако вряд ли это проблема, особенно учитывая произвольную форму кластеров. Зато DBSCAN автоматически определяет выбросы, что довольно здорово.",
            "Соотношение , где — размерность пространства, можно интуитивно рассматривать как пороговую плотность точек данных в области пространства. Ожидаемо, что при одинаковом соотношении , и результаты будут примерно одинаковы. Иногда это действительно так, но есть причина, почему алгоритму нужно задать два параметра, а не один. Во-первых типичное расстояние между точками в разных датасетах разное — явно задавать радиус приходится всегда. Во-вторых, играют роль неоднородности датасета. Чем больше и , тем больше алгоритм склонен «прощать» вариации плотности в кластерах. С одной стороны, это может быть полезно: неприятно увидеть в кластере «дырки», где просто не хватило данных. С другой стороны, это вредно, когда между кластерами нет чёткой границы или шум создаёт «мост» между скоплениями. Тогда DBSCAN запросто соединит две разные группы. В балансе этих параметров и кроется сложность применения DBSCAN: реальные наборы данных содержат кластеры разной плотности с границами разной степени размытости. В условиях, когда плотность некоторых границ между кластерами больше или равна плотности каких-то обособленных кластеров, приходится чем-то жертвовать.",
            "Существуют варианты DBSCAN, способные смягчить эту проблему. Идея состоит в подстраивании в разных областях по ходу работы алгоритма. К сожалению, возрастает количество параметров алгоритма.",
            "Ок, теперь давайте немного поговорим о плюсах и минусах DBSCAN.",
            "Плюсы DBSCAN",
            "• DBSCAN не требует указания числа кластеров в отличие, скажем, от метода k-средних",
            "• DBSCAN может найти кластеры произвольной формы. Он может найти даже кластеры полностью окружённые (но не связанные с) другими кластерами.",
            "• DBSCAN имеет понятие шума и устойчив к выбросам.",
            "• DBSCAN требует лишь двух параметров ( и ) и большей частью нечувствителен к порядку точек в датасете. Однако, точки, находящиеся на границе двух различных кластеров могут оказаться в другом кластере, если изменить порядок точек, а назначение кластеров единственно с точностью до изоморфизма.",
            "Проблемы DBSCAN",
            "• DBSCAN не полностью однозначен — краевые точки, которые могут быть достигнуты из более чем одного кластера, могут принадлежать любому из этих кластеров, что зависит от порядка просмотра точек (тут стоит сказать, что существует DBSCAN❋, который трактует краевые точки как шум и тем самым достигается полностью однозначный результат)",
            "• Качество DBSCAN зависит от способа измерения расстояния. Наиболее часто используемой метрикой расстояний является евклидова метрика. В случае кластеризации данных высокой размерности эта метрика может оказаться почти бесполезной, что делает трудным делом нахождение подходящего значения . Этот эффект, однако, присутствует в любом другом алгоритме, основанном на евклидовом расстоянии.",
            "• DBSCAN не может хорошо разделить на кластеры наборы данных с большой разницей в плотности, поскольку не удается выбрать приемлемую для всех кластеров комбинацию и .",
            "Что ж, теперь давайте теперь переключимся на алгоритм OPTICS (Ordering Points To Identify the Clustering Structure).",
            "Основная идея OPTICS похожа на DBSCAN, но алгоритм предназначен для избавления от одной из главных слабостей алгоритма DBSCAN — проблемы обнаружения кластеров в данных, имеющих различные плотности. Для этого используется граф достижимости, который определяет достижимое расстояние для каждой точки, которая в дальнейшем будет относиться к ближайшему кластеру. Такой подход позволяет ещё лучше определять кластеры разной плотности, особенно если они расположены близко друг к другу, однако это увеличивает время работы алгоритма.",
            "Реализация OPTICS есть в библиотеке Sci-Kit Learn; вот как можно её импортировать и использовать:",
            "С R тоже проблем нет:",
            "Хорошо, давайте немного об особенностях OPTICS",
            "Плюсы OPTICS:",
            "Устойчивость к шуму (впрочем как и у DBSCAN): OPTICS способен обрабатывать данные с шумом и выбросами.",
            "Способность обнаруживать кластеры любой формы",
            "Не требует заранее заданного числа кластеров",
            "Проблемы OPTICS:",
            "Не всегда эффективен для плотных кластеров: OPTICS может иметь проблемы с эффективным обнаружением плотных кластеров, особенно если они имеют сложные формы.",
            "А вот несколько сфер, где регулярно используется OPTICS:",
            "Анализ сетей и обнаружение аномалий: OPTICS используется для анализа социальных сетей, транспортных сетей и других сетевых структур для выявления кластеров и аномалий.",
            "Биоинформатика: OPTICS применяется в биоинформатике для кластеризации геномных данных, выявления генных паттернов и классификации биологических образцов.",
            "Медицинская диагностика: OPTICS может быть применен для кластеризации медицинских данных, таких как результаты тестов, симптомы пациентов и история заболеваний, с целью выявления паттернов заболеваний или групп пациентов схожего профиля. .",
            "Итак, пришло время сравнить DBSCAN и OPTICS",
            "Вот DBSCAN:",
            "...а вот и OPTICS:",
            "И давайте возьмём для начала , , потом поменяем.",
            "Что мы видим? Для данного датасета DBSCAN выделяет кластеры более логичным и понятным способов, но в кластеризации OPTICS тоже есть пара интересных моментов. Как можно увидеть, точки вокруг главных кластеров DBSCAN безнадёжно отмечает как шум, в то время как OPTICS пытается нащупать кластеры и среди этих точек тоже. Это одна из главных фишек OPTICS — метод способен видеть кластеры разной плотности одновременно за счёт того, что он менее чувствителен к параметру .",
            "Вот довольно показательный пример — и тут OPTICS тоже выделил кластер в точках, которые забраковал DBSCAN:",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Описание алгоритма DBSCAN от Sci-Kit Learn",
            "Описание алгоритма OPTICS от Sci-Kit Learn",
            "Наглядная визуализация DBSCAN",
            "Что ж, надеюсь, статья была полезной)",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)"
        ]
    },
    {
        "Название статьи": "Реализация принципа единственной ответственности на Python",
        "Дата публикации": "2024-06-03, 07:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Сегодня мы рассмотрим одну из основополагающих концепций SOLID-принципов — принцип единственной ответственности или сокращенно - SRP. Разберем, что такое SRP и как правильно его применять в Python.",
            "Принцип единственной ответственности гласит, что каждый класс, метод или модуль должен иметь только одну причину для изменения. Проще говоря, каждый компонент вашей системы должен отвечать только за одну функциональность. Т.е если вам нужно внести изменение, связанное с этой функциональностью, вам придется изменить только один компонент.",
            "Когда каждый класс или модуль выполняет одну четко определенную задачу, становится гораздо проще понять его назначение и взаимодействие с другими частями системы.",
            "Что будет, если не соблюдать SRP?",
            "Если класс или модуль берет на себя несколько обязанностей, это приводит к увеличению сложности кода. Такой код сложнее читать, понимать и поддерживать. Также, когда один класс выполняет несколько задач, изменение в одной из них может непредсказуемо повлиять на другие.",
            "Классы, которые нарушают SRP, обычно плохо масштабируются и трудно переиспользуются. Их невозможно легко адаптировать для других целей или проектов.",
            "Для начала рассмотрим класс, который нарушает принцип единственной ответственности. Представим себе класс UserManager, который одновременно отвечает за создание юзера, валидацию данных и сохранение юзера в БД:",
            "Класс нарушает SRP, т.к выполняет несколько задач: валидацию email, создание пользователя и сохранение его в базу данных.",
            "Для исправления нарушения SRP нужно разделить обязанности на отдельные классы: User, UserValidator, UserDatabase, и UserCreator. Каждый класс будет отвечать только за одну задачу:",
            "Теперь каждый класс отвечает за одну конкретную задачу, что соответствует принципу единственной ответственности.",
            "Рассмотрим другой пример, обработку заказов в интернет-магазине. Изначально есть класс, который нарушает SRP, т.к он одновременно обрабатывает заказ, валидирует данные и отправляет уведомления:",
            "Рефакторинг этого класса для соответствия SRP:",
            "Фасадный паттерн помогает упростить взаимодействие между сложными подсистемами, предоставляя простой интерфейс для клиента. С фасадом можно скрыть сложность подсистем и предоставлять единый интерфейс для взаимодействия с ними.",
            "Предположим, есть система обработки заказов, включающая несколько классов для управления заказами, оплатами и уведомлениями. Без фасадного паттерна клиенту пришлось бы взаимодействовать с каждым из этих классов напрямую:",
            "А с использованием фасадного паттерна все будет выглядеть так:",
            "Интерфейсы и абстрактные классы помогают разделить обязанности и четко определить контракт, который должен реализовать класс.",
            "Создание интерфейсов для валидации, сохранения и уведомления:",
            "Разделяем обязанности на интерфейсы, что позволяет каждому классу реализовывать только свои специфические методы, соответствующие SRP.",
            "Для поддержки SRP и других принципов SOLID в Python можно использовать различные библиотеки.",
            "Pylint помогает анализировать код на наличие ошибок и несоответствий стилю, а также выявляет нарушения принципов SOLID, включая SRP.",
            "Mypy - статический анализатор типов для Python, который помогает обнаруживать типовые ошибки и улучшать структуру кода.",
            "Pytest помогает создавать модульные тесты для каждого отдельного компонента.",
            "Dataclasses модуль позволяет создавать классы данных, которые следуют SRP, отделяя логику данных от поведения.",
            "Про другие архитектурные принципы и инструменты коллеги из OTUS рассказывают в рамках практических онлайн-курсов. Также хочу напомнить о том, что в календаре мероприятий вы можете зарегистрироваться на ряд интересных и абсолютно бесплатных вебинаров."
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 12: Дата и время (издание 2024)",
        "Дата публикации": "2024-06-02, 19:47",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это двенадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как работать с датами и временем таким образом, чтобы это работало для всех ваших пользователей, независимо от того, где они проживают.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения",
            "Глава 12: Дата и время (Эта статья)",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Один из аспектов моего приложения для ведения микроблогов, который я долгое время игнорировал, - это отображение дат и времени. До сих пор я просто позволял Python отображать объект datetime в модели User и даже не потрудился отобразить его в модели Post. В этой главе вы узнаете, как работать с этими временными метками.",
            "Ссылки на GitHub для этой главы: Browse, Zip, Diff.",
            "Использование Python на сервере для отображения дат и времени, которые отображаются пользователям в их веб-браузерах, на самом деле не очень хорошая идея, потому что то, что сервер считает своим местным временем, не будет иметь смысла для пользователей, которые живут в другом часовом поясе.",
            "Совершенно ясно, что сервер должен управлять временем, которое является согласованным и независимым от его собственного местоположения и местоположения пользователей. Если это приложение разрастется до такой степени, что потребуется несколько производственных серверов в разных регионах мира, я бы не хотел, чтобы каждый сервер записывал временные метки в базу данных в разных часовых поясах, потому что это сделало бы невозможной работу с этими временами. Поскольку UTC является наиболее используемым единым часовым поясом и поддерживается в классе datetime, именно его я и собираюсь использовать.",
            "В главе 4 вы видели, как создавать временные метки UTC для записей в блоге. В качестве напоминания, вот краткий пример, показывающий, как это было сделано.:",
            "Но с этим подходом связана важная проблема. Пользователям из разных мест будет ужасно сложно определить, когда была сделана публикация, если они будут видеть время в часовом поясе UTC. Им нужно было бы заранее знать, что время указано в UTC, чтобы они могли мысленно подогнать его к своему собственному часовому поясу. Представьте пользователя, скажем, в часовом поясе PDT на Западном побережье США, который публикует что-то в 15: 00 и сразу видит, что сообщение появляется в 10: 00 по времени UTC, или, если быть более точным, в 22: 00. Это будет очень запутанно.",
            "Хотя стандартизация временных меток в соответствии с UTC имеет большой смысл с точки зрения сервера, это создает проблему удобства использования для пользователей. Цель этой главы - представить решение, которое сохраняет все временные метки, управляемые сервером, в часовом поясе UTC, не отталкивая пользователей.",
            "Очевидным решением проблемы является преобразование всех временных меток из сохраненных единиц UTC в местное время каждого пользователя при их отображении. Это позволяет серверу продолжать использовать UTC для обеспечения согласованности, в то время как преобразование \"на лету\", адаптированное к каждому пользователю, решает проблему удобства использования. Сложная часть этого решения - знать местоположение каждого пользователя.",
            "На многих веб-сайтах есть страница конфигурации, где пользователи могут указывать свой часовой пояс. Для этого мне потребуется добавить новую страницу с формой, в которой я представляю пользователям раскрывающийся список часовых поясов. При первом входе на сайт пользователей могут попросить ввести их часовой пояс в рамках регистрации.",
            "Хотя это достойное решение, решающее проблему, немного странно просить пользователей вводить часть информации, которую они уже настроили в своей операционной системе. Кажется, было бы эффективнее, если бы я мог просто получить настройки часового пояса с их компьютеров.",
            "Как выясняется, веб-браузер знает часовой пояс пользователя и предоставляет его через стандартные API JavaScript даты и времени. На самом деле есть два способа воспользоваться информацией о часовом поясе, доступной через JavaScript:",
            "Подход \"старой школы\" заключался бы в том, чтобы веб-браузер каким-то образом отправлял информацию о часовом поясе на сервер, когда пользователь впервые входит в приложение. Это можно было бы сделать с помощью вызова Ajax или гораздо проще с помощью мета-тега обновления. Как только сервер узнает часовой пояс, он может сохранить его в сеансе пользователя или записать в таблицу users в базе данных, и с этого момента корректировать с его помощью все временные метки во время отображения шаблонов.",
            "Подход \"новой школы\" заключается в том, чтобы ничего не менять на сервере и позволить преобразованию UTC в местный часовой пояс происходить в браузере с использованием JavaScript.",
            "Оба варианта допустимы, но второй имеет большое преимущество. Знания часового пояса пользователя не всегда достаточно для представления дат и времени в формате, ожидаемом пользователем. Браузер также имеет доступ к конфигурации языкового стандарта системы, которая определяет такие параметры, как время утра / вечера в сравнении с 24-часовыми часами, формат отображения даты DD / MM / ГГГГ в сравнении с MM / DD / ГГГГ и многие другие культурные или региональные стили.",
            "И если этого недостаточно, у подхода новой школы есть еще одно преимущество. Есть библиотека с открытым исходным кодом, которая выполняет всю эту работу!",
            "Moment.js это небольшая библиотека JavaScript с открытым исходным кодом, которая выводит отображение даты и времени на новый уровень, поскольку предоставляет все мыслимые варианты форматирования, а затем и некоторые другие. Некоторое время назад я создал Flask-Moment, небольшое расширение Flask, которое позволяет очень легко интегрировать moment.js в ваше приложение.",
            "Итак, давайте начнем с установки Flask-Moment:",
            "Это расширение добавляется в приложение Flask обычным способом:",
            "app/__init__.py: Пример Flask-Moment.",
            "В отличие от других расширений, Flask-Moment работает вместе с moment.js, поэтому все шаблоны приложения должны включать эту библиотеку. Чтобы гарантировать, что эта библиотека всегда доступна, я собираюсь добавить ее в базовый шаблон. Это можно сделать двумя способами. Самый прямой способ - явно добавить тег <script>, который импортирует библиотеку, но Flask-Moment упрощает задачу, предоставляя функцию moment.include_moment(), которая генерирует тег <script>:",
            "app/templates/base.html: Включить moment.js в базовый шаблон.",
            "В большинстве случаев библиотеки JavaScript, используемые приложением, включены в конец содержимого <body>, где находится загрузочный JavaScript-код.",
            "Moment.js делает класс moment доступным для браузера. Первым шагом для отображения временной метки является создание объекта этого класса, передающего желаемую временную метку в формате ISO 8601. Вот пример, запущенный в консоли JavaScript браузера:",
            "Если вы не знакомы со стандартным форматом даты и времени ISO 8601, этот формат выглядит следующим образом:",
            "Я уже решил, что буду работать только с часовыми поясами UTC, поэтому последней частью всегда будет +00:00 или в некоторых случаях эквивалент Z, который представляет UTC в стандарте ISO 8601.",
            "В объекте moment предусмотрено несколько методов для различных вариантов рендеринга. Ниже приведены некоторые из наиболее распространенных вариантов:",
            "В этом примере создается объект moment, инициализированный 28 июня 2021 года в 21:45 по Гринвичу. Вы можете видеть, что все параметры, которые я пробовал выше, отображаются в UTC + 1, который является часовым поясом, настроенным на моем компьютере. Вы можете ввести вышеуказанные команды в консоли вашего браузера, убедившись, что в странице, на которой вы открываете консоль, включена moment.js. Вы можете сделать это в микроблоге, при условии, что вы внесли вышеуказанные изменения для включения moment.js, или также на https://momentjs.com/.",
            "Обратите внимание, как разные методы создают разные представления. С помощью метода format() вы управляете форматом выходных данных с помощью строки формата. Метод fromNow() интересен тем, что он отображает временную метку по отношению к текущему времени, поэтому вы получаете выходные данные, такие как \"минуту назад\" или \"через два часа\" и т.д.",
            "Если вы работали непосредственно в JavaScript, приведенные выше вызовы возвращают строку с отображаемой временной меткой. Затем вам предстоит вставить этот текст в нужное место на странице, что, к сожалению, требует работы с DOM. Расширение Flask-Moment значительно упрощает использование moment.js за счет включения в ваши шаблоны объекта moment, аналогичного объекту JavaScript.",
            "Давайте посмотрим на временную метку, которая отображается на странице профиля. Текущий шаблон user.html позволяет Python генерировать строковое представление времени. Теперь я могу отобразить эту временную метку с помощью Flask-Moment следующим образом:",
            "app/templates/user.html: Отрисовка временной метки с помощью moment.js.",
            "Итак, как вы можете видеть, Flask-Moment использует синтаксис, аналогичный синтаксису библиотеки JavaScript, с одним отличием, заключающимся в том, что аргументом для moment() теперь является объект Python datetime, а не строка ISO 8601. Вызов moment(), выполняемый из шаблона, автоматически генерирует необходимый код JavaScript для вставки отображаемой временной метки в нужное место DOM.",
            "Второе место, где я могу воспользоваться преимуществами Flask-Moment, находится во вложенном шаблоне _post.html, который вызывается с главной страницы и страницы пользователя. В текущей версии шаблона каждому сообщению предшествует строка \"username says:\". Теперь я могу добавить временную метку, отображаемую с помощью fromNow():",
            "app/templates/_post.html: Отрисовка временной метки во вложенном шаблоне post.",
            "Ниже вы можете увидеть, как выглядят обе эти временные метки при рендеринге с помощью Flask-Moment и moment.js:"
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 11: Дизайн приложения (издание 2024)",
        "Дата публикации": "2024-06-02, 19:46",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это одиннадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как заменить базовые HTML-шаблоны новым набором, основанным на платформе пользовательского интерфейса Bootstrap.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения (Эта статья)",
            "Глава 12: Даты и время",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Вы уже некоторое время играете с моим приложением для ведения микроблогов, поэтому, я уверен, вы заметили, что я не потратил слишком много времени на то, чтобы оно выглядело хорошо, или, лучше сказать, я вообще не тратил на это времени. Шаблоны, которые я собрал, довольно простые, без какого-либо пользовательского оформления. Мне было полезно сосредоточиться на реальной логике приложения, не отвлекаясь на написание красивых HTML и CSS.",
            "Но я уже долго сосредоточен на серверной части этого приложения. Итак, в этой главе я сделаю перерыв и потрачу некоторое время на то, чтобы показать вам, что можно сделать, чтобы приложение выглядело немного более отточенным и профессиональным.",
            "Эта глава будет немного отличаться от предыдущих, потому что я не собираюсь так подробно, как обычно, описывать сторону Python, которая, в конце концов, является основной темой этого туториала. Создание красивых веб-страниц - обширная тема, которая в значительной степени не связана с веб-разработкой на Python, но я расскажу о некоторых основных рекомендациях и идеях о том, как подойти к этой задаче, и у вас также будет приложение с измененным дизайном, которое можно изучить и перенять опыт.",
            "Ссылки на GitHub для этой главы: Обзор, Zip, Diff.",
            "Хотя мы можем утверждать, что программирование - это сложно, наши усилия ничто по сравнению с усилиями веб-дизайнеров, которым приходится создавать веб-страницы, которые красиво и единообразно выглядят в списке веб-браузеров. За последние годы они стали лучше, но в некоторых браузерах все еще есть непонятные ошибки или причуды, которые сильно усложняют задачу создания веб-страниц, которые везде выглядят красиво. Это еще сложнее, если вам также нужно настроить для браузеров планшетов и смартфонов с ограниченным количеством ресурсов и экранов.",
            "Если вы, как и я, разработчик, который просто хочет создавать прилично выглядящие веб-страницы, но у вас нет времени или интереса изучать низкоуровневые механизмы для эффективного достижения этой цели путем написания необработанного HTML и CSS, то единственным практическим решением является использование CSS фреймворка для упрощения задачи. Выбрав этот путь, вы потеряете некоторую творческую свободу, но, с другой стороны, ваши веб-страницы будут хорошо выглядеть во всех браузерах без особых усилий. Фреймворк CSS предоставляет коллекцию высокоуровневых классов CSS с готовыми стилями для распространенных типов элементов пользовательского интерфейса. Большинство этих фреймворков также предоставляют дополнения JavaScript для вещей, которые нельзя выполнить строго с помощью HTML и CSS.",
            "Одним из самых популярных CSS-фреймворков является Bootstrap. Если вы хотите увидеть, какие страницы можно создавать с помощью этого фреймворка, в документации есть несколько примеров.",
            "Вот некоторые преимущества, которые вы получаете при использовании Bootstrap для оформления ваших веб-страниц:",
            "Аналогично выглядит во всех основных веб-браузерах",
            "Настройка размеров для экранов настольных компьютеров, планшетов и телефонов",
            "Настраиваемые макеты",
            "Красиво оформленные панели навигации, формы, кнопки, оповещения, всплывающие окна и т.д.",
            "Самый простой способ использовать Bootstrap - это просто импортировать файл bootstrap.min.css в ваш базовый шаблон. Вы можете либо загрузить копию этого файла и добавить его в свой проект, либо импортировать его непосредственно из CDN. Затем вы можете начать использовать CSS-классы общего назначения, которые он предоставляет, согласно документации, что довольно неплохо. Возможно, вы также захотите импортировать JavaScript-код фреймворка, чтобы использовать самые продвинутые функции.",
            "Как и большинство проектов с открытым исходным кодом, Bootstrap постоянно развивается. Оригинальная версия мега-учебника Flask была создана для Bootstrap 3. Редакция, которую вы сейчас читаете, создана для Bootstrap 5.3. Текущий подход к интеграции Bootstrap является довольно общим и может быть адаптирован к более новым версиям Bootstrap.",
            "Первым шагом в интеграции Bootstrap с Microblog является добавление его файлов CSS и JavaScript в базовый шаблон. На странице быстрого запуска Bootstrap в качестве примера приведена короткая, но полная HTML-страница, которую я копирую ниже для вашего удобства:",
            "Подход, который я могу применить, чтобы объединить это с моим шаблоном base.html, заключается в том, чтобы использовать приведенный выше в качестве нового базового шаблона, заменив теги <title> и <h1> заголовком и основным содержимым исходного базового шаблона соответственно.",
            "Следующий шаг - заменить базовую панель навигации на более удобную из Bootstrap. На странице документации по панели навигации Bootstrap вверху показан хороший пример. Используя этот пример в качестве руководства, я создал панель навигации со ссылками \"Index\", \"Explore\", \"Profile\", \"Login\" и \"Logout\" из микроблога. Для удобства я настроил профиль, а также ссылки для входа и выхода так, чтобы они отображались в крайнем правом углу.",
            "При использовании Bootstrap полезно знать о некоторых базовых примитивах компоновки. Одним из наиболее важных является контейнер, который определяет область содержимого страницы. Два основных контейнера называются container и container-fluid. В первом случае страница настраивается на использование одной из пяти предопределенных ширин страницы и центрирует содержимое в окне браузера. С другой стороны обтекающий контейнер дает вам доступ ко всей ширине страницы. Для этого приложения я решил использовать контейнер по умолчанию, потому что он предотвращает слишком широкое расширение страницы независимо от размера экрана, поэтому часть содержимого страницы будет заключена в один из этих контейнеров следующим образом:",
            "Последняя часть HTML-разметки в шаблоне base.html, которую необходимо адаптировать, - это раздел, отображающий отображаемые сообщения. Компонент Alert от Bootstrap прекрасно подходит для этой задачи.",
            "Вы можете получить полностью переработанный шаблон base.html из репозитория Github для этой главы. Ниже вы можете увидеть упрощенную структуру, если хотите иметь представление о том, как она выглядит.:",
            "app/templates/base.html: Переработанный базовый шаблон.",
            "Благодаря обновленному базовому шаблону внешний вид приложения уже заметно улучшен без необходимости изменять строки кода Python. Если вы хотите убедиться в этом сами, загрузите копию base.html из репозитория GitHub по ссылкам, приведенным в начале этой главы.",
            "Область, в которой Bootstrap проделывает фантастическую работу, заключается в рендеринге полей формы, которые выглядят намного приятнее и чище, чем поля по умолчанию, предоставляемые браузером. В документации по Bootstrap также есть раздел о формах. В начале этого раздела приведен пример формы входа в систему, который показывает базовую структуру HTML.",
            "HTML-код, необходимый для каждого поля, довольно длинный. Ниже вы можете увидеть одно из текстовых полей из примера формы в документации:",
            "Но это слишком просто для нужд Microblog, который включает проверку полей и, возможно, потребуется показывать пользователю ошибки проверки. На странице документации есть раздел о проверке на стороне сервера, в котором показано, как оформить поля с сообщением об ошибке. Вот пример.:",
            "К сожалению, о необходимости вводить такое количество шаблонов для каждого поля в каждой форме не может быть и речи. Это заняло бы слишком много времени и чревато ошибками. Одним из решений является использование макросов Jinja, которые позволяют вам определять повторно используемые фрагменты HTML, а затем вызывать их из ваших шаблонов, как если бы они были функциями.",
            "Например, макрос Jinja для текстового поля, подобного показанному выше, будет иметь вид:",
            "Обратите внимание, как используются условные обозначения для выборочного добавления стиля ошибки, если поле содержит одно или несколько сообщений об ошибках.",
            "Поскольку макрос определен в файле с именем bootstrap_wtf.html, который расположен в каталоге templates, он может быть вызван, когда потребуется отобразить поле. Например:",
            "Макрос отображения полей можно расширить, чтобы он также поддерживал отображение флажков, раскрывающихся списков выбора, кнопок отправки и других типов полей. Он также может принимать второй аргумент с логическим значением, указывающим, следует ли автоматически переводить поле в фокус страницы, что должно быть сделано для первого поля формы. Для еще большего удобства можно создать другой макрос для рендеринга всей формы, просто перебрав поля формы и вызвав form_field() макрос для каждого из них.",
            "Полный bootstrap_wtf.html файл доступен в репозитории GitHub, ссылка на который приведена в начале этой главы. Он включает в себя более полную версию макроса form_field(), показанного выше, и второй макрос с именем quick_form(), который принимает объект формы и отображает все его поля с помощью первого макроса.",
            "Как это выглядит, когда реализовано в реальной форме? Ниже вы можете увидеть переработанный шаблон register.html в качестве примера:",
            "app/templates/register.html: Шаблон регистрации пользователя.",
            "Разве это не здорово? Оператор import вверху работает аналогично импорту Python на стороне шаблона. Который добавляет макрос wtf.quick_form(), который в одной строке кода отображает полную форму, включая ошибки проверки, и все оформлено в соответствии с фреймворком Bootstrap.",
            "Еще раз, я не собираюсь показывать вам все изменения, которые я сделал для других форм в приложении, но все эти изменения внесены в шаблоны, которые вы можете загрузить или просмотреть на GitHub.",
            "Логика представления, которая отображает отдельные записи в блоге, была абстрагирована в подшаблон под названием _post.html. Все, что мне нужно сделать с этим шаблоном, это внести некоторые незначительные корректировки, чтобы он хорошо выглядел в Bootstrap.",
            "app/templates/_post.html: Переработанный подшаблон публикации.",
            "Ссылки на страницы - это еще одна область, в которой Bootstrap предоставляет поддержку. Для этого я просто еще раз обратился к документации Bootstrap и адаптировал один из их примеров. Вот как это выглядит на странице index.html:",
            "app/templates/index.html: Переработаны ссылки на страницы.",
            "Обратите внимание, что в этой реализации вместо скрытия следующей или предыдущей ссылки, когда в этом направлении больше нет содержимого, я применяю отключенное состояние, из-за которого ссылка будет отображаться серым цветом.",
            "Я не собираюсь показывать это здесь, но аналогичное изменение необходимо применить к шаблону user.html. Пакет для загрузки этой главы включает эти изменения.",
            "Чтобы внести в ваше приложение эти изменения, пожалуйста, загрузите zip-файл для этой главы и соответствующим образом обновите свои шаблоны.",
            "Ниже вы можете увидеть несколько фотографий до и после, чтобы увидеть трансформацию. Имейте в виду, что это изменение было достигнуто не затрагивая ни одной строки кода приложения!",
            "Следующая глава =>"
        ]
    },
    {
        "Название статьи": "Расширяем возможности Keras с помощью кастомных слоев",
        "Дата публикации": "2024-06-02, 18:07",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Keras предоставляет мощные инструменты для создания сложных нейронных сетей. Однако иногда стандартного набора слоев недостаточно для решения некоторых задач. В таких случаях на помощь приходят кастомные слои.",
            "Кастомные слои позволяют адаптировать архитектуру модели под особенности данных, улучшая тем самым производительность и точность моделек.",
            "Каждый кастомный слой начинается с определения нового класса, наследующего от tf.keras.layers.Layer. В __init__ происходит инициализация слоя, где можно задать параметры, необходимые для работы слоя:",
            "Тут units определяет количество нейронов, а activation указывает функцию активации. super(CustomLayer, self).__init__(**kwargs) вызывает конструктор базового класса Layer.",
            "Метод build вызывается Keras при первом использовании слоя. Его юзают для создания параметров слоя, которые зависят от размера входных данных:",
            "В методе создаются веса kernel и bias. Функция add_weight создает и регистрирует переменные слоя, которые будут обновляться во время тренировки.",
            "Метод call содержит основную логику вычислений слоя. Он принимает входные данные и возвращает выходные:",
            "В этом методе выполняется умножение входных данных на веса и добавление смещения. Если определена функция активации, она применяется к выходным данным.",
            "После определения кастомного слоя его можно использовать в моделях Keras как обычный слой:",
            "Другие полезные методы:",
            "add_weight: Добавляет переменную веса в слой.",
            "compute_output_shape: Возвращает форму выходных данных на основе формы входных.",
            "get_config: Возвращает конфигурацию слоя в виде словаря, что полезно для сериализации.",
            "Dense слой выполняет простую линейную операцию: умножение входного вектора на матрицу весов и добавление смещения, а затем применяется функция активации:",
            "Convolutional слои применяют свертку фильтра к входным данным, что позволяет выделять пространственные особенности:",
            "Recurrent слои используются для обработки последовательных данных. Один из наиболее распространнных типов рекуррентных слоев — это LSTM:",
            "Dropout слой используется для регуляризации модели, предотвращая переобучение путем случайного зануления некоторых нейронов во время тренировки:",
            "BatchNormalization слой нормализует активации предыдущего слоя, улучшая скорость обучения модельки:",
            "Больше практических инструментов и кейсов коллеги из OTUS рассматривают в рамках практических онлайн-курсов. Напомню, что с полным каталогом курсов можно ознакомиться по ссылке."
        ]
    },
    {
        "Название статьи": "Enbeddrus — обучение независящей от языка эмбеддинг-модели",
        "Дата публикации": "2024-06-02, 17:31",
        "Автор статьи": "efreelancer ",
        "Статья целиком": [
            "Приветствую, хабровчане!",
            "Сегодня хочу рассказать вам историю о том, как я обучил простую и компактную независящую от языка (language agnostic) модель-эмбеддер, которая умеет работать с техническими текстами о PHP и способна извлекать схожие эмбеддинги для параллельных текстов на английском и русском языках.",
            "Основная причина, по которой я решил заняться этим проектом, заключается в том, что мои заметки, код и документация, накопленные за более чем десять лет практики, представляют собой солянку текстов о разных технологиях, языках программирования, пометки о настройке серверов Linux и т.д. на русском и английском языках. Поэтому мне захотелось сделать Retrieval-Augmented Generation (RAG) помогалку, которая сможет принимать запросы пользователя (меня) и эффективно находить информацию в столь разношерстой базе данных, независимо от того на каком языке я сделал запрос и на каком языке написана документация.",
            "Для достижения этой цели как-раз и необходима независимая от языка модель-эмбеддер, которая будет одинаково хорошо работать с техническими текстами на русском и английском языках.",
            "Ещё одним важным аспектом было то, чтобы модель потребляла как можно меньше ресурсов и, если возможно, чтобы её можно было преобразовать в формат GGUF.",
            "Но прежде чем приступить к созданию своего собственного велосипеда, я решил поискать готовые решения, ведь подобная идея очевидна и, возможно, уже реализована другими.",
            "Спойлер: идея не нова, и подобных решений уже достаточно много.",
            "Для построения системы, которая может извлекать одинаковые эмбеддинги для схожих текстов на русском и английском языках, существует несколько решений, например...",
            "Ссылки: arxiv:1907.04307 , kaggle, github",
            "Это проект разработан инженерами Google и поддерживает 16 языков.",
            "Свойства: ~110m параметров, принимает на вход 128 токенов текста и извлекает из них 512-мерный эмбеддинг.",
            "Плюс: поддерживает русский язык.",
            "Минусы: модель основана на Tensorflow, а так же что с 2019го года не было обновлений.",
            "Ссылки: arxiv:1710.04087, github",
            "Это одна из первых попыток инженеров FB создать модель которая способна выполнять задачи по извлечению независящих от языка эмбеддингов.",
            "Плюс: поддерживает русский язык.",
            "Минусы: в наличии имеются веса для пар языков, навроде en-ru, en-de и т.д., весов нет на HuggingFace, ну и с 2018го года проект не развивается.",
            "Ссылки: arvix:2205.12654, github, pypi",
            "Ещё одна модель разработана инженерами FB и, как сказано в ридми на GitHub, поддерживает более 200 языков (хотя если пройти по ссылочкам и посчитать то получится 147 языков).",
            "Свойства: ~256m параметров, принимает 1024 токенов на вход и извлекает из них 1024-мерный эмбеддинг.",
            "Плюсы: она основана на PyTorch и имеет логику переключения между языками которая явно перекочевала из NLLB (о которой я кстати рассказывал в публикации \"Перевод на разные языки используя модель NLLB\" у себя в блоге на Дзен).",
            "Минусы: весов нет на HuggingFace, а модель несовместима с llama.cpp поэтому её не получится конвертировать в GGUF, чтобы можно было запускать на слабом железе (или же в паре с ollama).",
            "Ссылки: arXiv:1908.10084, сайт",
            "Модели Sentence-BERT представляют собой модифицированную версию предобученной BERT, специально адаптированную для генерации эмбеддингов предложений, multilingual версия позволяет извлекать эмбеддинги из текста на разных языках, а paraphrase модели позволяют извлекать похожие эмбеддинги парафраз на разных языках.",
            "Вот пару примечательных моделей, обученных разными способами:",
            "paraphrase-multilingual-MiniLM-L12-v2 имеет 118m параметров, принимает 256 токенов на вход и возвращает 384-мерный эмбеддинг.",
            "paraphrase-multilingual-mpnet-base-v2 имеет 278m параметров, принимает на вход 512 токенов и возвращает 768-мерный эмбеддинг.",
            "Обе эти модели обучены на комбинации из датасетов:",
            "SNLI о котором говорится в публикации \"A large annotated corpus for learning natural language inference\" (570k примеров)",
            "Multi-Genre NLI, подробнее в работе \"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\" (433k примера)",
            "Плюсы: поддерживает русский язык, можно конвентировать в GGUF.",
            "Минусы: модели не очень хорошо понимают технический текст (особенно русский технический жаргон), нет версии в формате GGUF, и к числу фатальных недостатков могу отнести, что эти модели обучил не я ;)",
            "Пришёл к выводу, что тему обучения подобных модей-эмбеддеров уже достаточно хорошо изучили и что можно без особых сложностей реализовать мою задумку.",
            "В качестве базовой модели решил взять модель google-bert/bert-base-multilingual-uncased, потому что:",
            "У этой крохи всего 168m параметров, что чуть больше чем у paraphrase-multilingual-MiniLM-L12-v2, но меньше чем у paraphrase-multilingual-mpnet-base-v2;",
            "На вход она принимает 512 токенов, а на выходе возвращает 768-мерный эмбеддинг, столько же у paraphrase-multilingual-mpnet-base-v2;",
            "Модель обучена на датасете wikipedia представляющем из себя Text Corpora, а там, сами понимаете, примеров текста больше, чем SNLI и Multi-Genre NLI вместе взятые;",
            "Модель uncased, то есть обучение происходило на регистронезависимых текстах (сиречь всё переводилось в lowercase).",
            "С моделью определились, теперь перейдём к вопросу выбора датасета...",
            "Изначально я хотел собрать больше датасетов, но, собирая датасет по PHP, я понял, какой это трудоёмкий процесс, и решил уменьшить свои амбиции.",
            "Итак, после поиска в интернете я нашёл только один подходящий датасет: OPUS PHP v1 на 2k примеров, содержащий пары текстов на русском и английском языках, по теме PHP.",
            "Из указанного датасеста я использовал только английский корпус (так как русский корпус был очень низкого качества), далее задействовал инстанс LibreTranslate для перевода английских текстов на русский и очистил данные от аномалий и шума (сценарий dataset_php_build.ipynb). Затем вручную перевёл кривые места с помощью Google и Yandex Translate и экспортировал результат в CSV формат. Данные отсортировал и удалил дубликаты (сценарий dataset_php_undup.py) после чего осталось 1.6k примеров.",
            "В финале попросил ChatGPT сгенерировать 100 примеров пар технического текста о PHP на русском и английском языках для сплита eval, а очищенные данные использовал для сплита train.",
            "Результат выгрузил (сценарий dataset_php_publish.ipynb) на HuggingFace: evilfreelancer/opus-php-en-ru-cleaned .",
            "Для создания эффективного эмбеддера, способного работать с техническими текстами о PHP на русском и английском языке, я решил провести обучение модели в два этапа, сначала выполнить Domain Adaptation, чтобы модель могла работать с техническими текстами на английском языке, а после этого обучить её на Parallel Corpora из русских и английских текстов.",
            "Для Domain Adaptation я использовал метод Generative Pseudo Labeling (GPL) (arXiv:2112.07577), данный метод позволяет проводить обучение модели на основе неразмеченных данных, генерируя псевдометки и улучшая качество работы модели для специфических доменов.",
            "Библиотека gpl имеет захардкоженный формат входного датасета и читает данные по определённым путям, поэтому пришлось слегка конвертировать тренировочный датасет и положить результат в директорию datasets (сценарий: dataset_php_convert.py).",
            "Для адаптации модели bert-base-multilingual-uncased к домену английских текстов про PHP я использовал в качестве шаблона скрипт, предложенный авторами проекта GPL на их странице на GitHub, получился следующего вида код:",
            "Полный скрипт тренировки train_domain.py можно найти в репозитории проекта на GitHub.",
            "Процесс обучения включает в себя несколько этапов:",
            "Используется генератор запросов, такой как BeIR/query-gen-msmarco-t5-base-v1, для создания синтетических запросов на основе текстов из корпуса;",
            "С помощью ретриверов, таких как msmarco-distilbert-base-v3 и msmarco-MiniLM-L-6-v3, которые работают с косинусным сходством, извлекаются наиболее релевантные документы для сгенерированных запросов;",
            "Кросс-энкодер, такой как cross-encoder/ms-marco-MiniLM-L-6-v2, используется для создания псевдометок, присваивая оценочные метки соответствия между запросами и документами;",
            "Модель обучается с использованием MarginMSELoss, которая позволяет модели лучше адаптироваться к новому домену.",
            "И так, наша модель обучена работать с новым доменом, поэтому переходим к следующему шагу.",
            "Для обучения модели на параллельных корпусах я использовал метод обучения моделей на разных языках, описанный в примере на сайте Sentence Transformers. Этот метод позволяет обучать мультиязычные модели, используя параллельные тексты на разных языках (заготовка скрипта make_multilingual.py).",
            "Для оценки качества модели я написал юпитер-блокнот, который загружает базовую и дообученную модель, прогоняет пары из eval сплита датасета evilfreelancer/opus-php-en-ru-cleaned и анализирует разницу между эмбеддингами, построенными для текстов на разных языках. Результаты визуализируются в виде графиков. Скрипт можно найти здесь.",
            "На графике видно, что базовая модель bert-base-multilingual-uncased распределяет русские и английские тексты в изолированные кластеры точек, ну а наша задача сделать так, чтобы эти точки были расположены как можно ближе друг к другу.",
            "Подобную задачу позволяет решать MSELoss, так как она минимизирует разницу между эмбеддингом, сгенерированным моделью-учителем (на английском языке) и эмбеддингом, сгенерированным моделью-учеником (на русском языке).",
            "Теперь пару слов про датасеты, решил остановиться на следующем наборе:",
            "evilfreelancer/opus-php-en-ru-cleaned (1.6k) - ранее созданный датасет параллельных текстов на английском и русском языках;",
            "Helsinki-NLP/opus_books (17.5k) - датасет OPUS параллельных текстов из книг.",
            "Выбрал я их потому, что мои первые эксперименты с обучением модели на только PHP датасете показали, что у модели происходит overfitting в результате чего падала общее качество работы модели, поэтому самым логичным решением было добавить ещё один Parallel Corpora общего назначения.",
            "Помимо этого в скрипт обучения я хотел сразу заложить возможность обучать на множестве разных датасетов (имеющих разные форматы данных), в результате чего получилась функция:",
            "В дальнейшем планирую добавить в неё больше датасетов на разные технические темы, но на этапе прототипирования того что есть более чем достаточно.",
            "Двигаемся дальше.",
            "Полный скрипт тренировки train_parallel.py можно найти в репозитории проекта на GitHub, в качестве модели-учителя возьмём google-bert/bert-base-multilingual-uncased, а в качестве модели-ученика ту, что мы обучили ранее на шаге Domain Adaptation.",
            "Обучение происходит в несколько этапов:",
            "Сначала мы загружаем датасеты (функция read_datasets);",
            "Далее выполняем их преобразование в нужный формат, после чего сохраняем на диске (функциия prepare_datasets)",
            "Инициализируем модель-учитель и модель-ученик (тут)",
            "Инициализируем MSELoss, передав ей на вход указатель на модель-ученика (тут)",
            "Запускаем обучение модели-ученика",
            "По завершению обучению давайте попробуем протестировать модель и понять стала ли на лучше извлекать эмбеддинги.",
            "Как видно на графике эмбеддинги извлечённые из русских и английских текстов где-то наложились друг на друга, точность похожести поднялась с 0.83 до 0.94, при этом модель также хорошо разделяет фразы различающиеся по смыслу.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1-domain",
            "Посмотрел я на этот график и пришла в голову мысль, а что если попробовать обучить базовую модель сразу на Parallel Corpora, пропустив шаг с Domain Adaptation?",
            "Правим скрипт тренировки, меняем модель-ученика, получается вот так:",
            "Опять запускаем тренировку и ждём некоторое время, по завершению прогоняем тесты и смотрим что получилось.",
            "Как видно на графиках если обучать сразу на Parallel Corpora модель быстрее, так как не нужно выполнять Domain Adaptation, и лучше обучается извлекать эмбеддинги из параллельных текстов, ведь косинусное расстояние в таком случае между близкими по смыслу фразами на разных языках в среднем в районе 0.97, что выше чем у модели изначально обученной на домене текстов про PHP.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1",
            "Отсюда можно сделать вывод, что дообучение мультиязыковой модели bert-base-multilingual-cased через Domain Adaptation с последующем обучением на Parallel Corpora не имеет особого смысла и проще сразу дообучать её на Parallel Corpora.",
            "Осталось выполнить самую малость, для начала я хочу конвертировать модель в формат GGUF, чтобы можно было использовать обученные модели через llama.cpp, но на этом моменте сильно не будем заострять внимание, сошлюсь на мою публикацию \"Как конвертировать модель BERT в формат GGUF?\" в моём блоге и PR который я создал в проекте llama.cpp.",
            "Но если кратко команды конвертации нужно выполнять и корня проекта llama.cpp и выглядят они следующим образом:",
            "По её завершению в директории models появятся файлы: enbeddrus-v0.1-f16.gguf и enbeddrus-v0.1-domain-f16.gguf.",
            "Полученные модели я выгрузил на серверы Ollama следующим образом:",
            "Выгруженные модели находятся тут и скачать их можно следующей командой:",
            "Содержимое Modelfile'ов можно найти в директории models проекта на GitHub.",
            "https://github.com/EvilFreelancer/enbeddrus",
            "https://huggingface.co/datasets/evilfreelancer/opus-php-en-ru-cleaned",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1-domain",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1",
            "https://ollama.com/evilfreelancer/enbeddrus",
            "Благодаря работе над проектом enbeddrus были достигнуты следующие цели:",
            "Удалось разобрался с тем как подобные модели устроены и как они работают, а так же с тем как их можно обучать;",
            "Был собран датасет с Parallel Corpora тематических текстов о PHP на русском и английском;",
            "Удалось разобраться с методами оценки моделей, а также с тем как эту оценку красиво визуализировать;",
            "Была обучена модель, которая эффективно работает с текстами на двух языках и может быть использована в RAG-системе для поиска и анализа информации.",
            "Полученные результаты подтверждают, что обучение мультиязычных эмбеддеров на основе параллельных корпусов является эффективным подходом для создания моделей, способных работать с текстами на разных языках.",
            "Спасибо за внимание и за что дочитал публикацию до конца! Если у вас есть вопросы или вы хотите связаться со мной, ссылки на мои контакты в социальных сетях можно найти в моём профиле на Хабре."
        ]
    },
    {
        "Название статьи": "Я научу вас неправильно играть в Hearts of iron. Оптимизация довоенной экономики: часть 2",
        "Дата публикации": "2024-06-02, 17:02",
        "Автор статьи": "Glasssparrow ",
        "Статья целиком": [
            "В прошлой части мы создали инструментарий, настало время им воспользоваться.",
            "За долю секунды мы можем провести симуляцию нескольких внутриигровых лет, что позволяет нам применить простейший метод исследования - метод перебора. И, раз уж мы всё равно будем перебирать, стоит также построить графики.",
            "В качестве испытуемой страны мы выберем, конечно, Советский Союз, условия будем выбирать близкие к реальному прохождению.",
            "Во-первых, рассмотрим торговлю. Торговля в игре зависит от многих факторов и не может быть оценена в симуляции, т.к. мы не работаем со всем миром (это потребует больших вычислительных мощностей). Таким образом, торговлю можно взять только из игры, что я и сделал, прокрутив 5 внутриигровых лет и записав количество фабрик получаемых от торговли. При этом закупки брались равными нулю (закупки сильно зависят от того что производит игрок, потому для общего случая я их просто игнорировал).",
            "1 установить_торговлю 18 120 установить_торговлю 10 210 установить_торговлю 7 365 установить_торговлю 5 730 установить_торговлю 9 1100 установить_торговлю 13 1450 установить_торговлю 25 1800 установить_торговлю 18",
            "Во-вторых, рассмотрим технологии. Нам важны технология индустрии и технология строительства. Моменты их развития также были получены из игры следующим образом: технологии исследовались без опережения по времени (с некоторыми погрешностями, конечно), в первую очередь строительство и индустрия, во вторую электроника, всё остальное исследовалось лишь для того чтобы сбить накапливающиеся во время простоя дополнительные 30 дней исследований. Никаких фокусов, решений и политик на исследования использовано не было.",
            "188 construction_tech # технология строительства 1 328 industry_tech # технология индустрии 1 511 construction_tech # технология строительства 2 511 industry_tech # технология индустрии 2 1285 construction_tech # технология строительства 3 1285 industry_tech # технология индустрии 3 1950 construction_tech # технология строительства 4 1950 industry_tech # технология индустрии 4",
            "В-третьих, обратим внимание на фокусы. Рассмотрим 4 варианта: 1) Стандартное быстрое закрытие паранойи с советником на гражданское строительство и частичной мобилизацией2) Оно же, но дополнительно поставим свободную торговлю, когда будет политка3) Также быстро закрываем паранойю, но частичную мобилизацию берем до советника на гражданское строительство (это будет стоить нам 30 политки)4) Вообще никаких фокусов не берем, только советник и частичная мобилизация.",
            "sov # тэг страны.140 добавить_гражданского_советника # если идти по пути Сталина то политки как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранойи 350 добавить_лимит_фабрик 0.1 # фокус ветки паранойи 350 добавить_бонус_строительства 0.05 # фокус ветки паранойи 350 добавить_фабрики 2 # фокус ветки паранойи 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov140 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная #320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная",
            "В общем и целом, получаем достаточно неплохие бонусы на промышленность, но отнюдь не максимальные (есть еще плакаты на -15% товаров народного потребления, есть множество других бонусов на промышленность в фокусах), но для общего случая этого и не нужно, мы (пока) не собираемся ограничивать игрока в свободе выбора фокусов.",
            "Рассмотрим получившиеся графики зависимости максимума количества военных заводов от количества фабрик которые мы строим (это важно, фабрики от фокусов в счетчик не идут) до переключения на военную промышленность.",
            "Видим что 5% от свободной торговли дают нам лишь 4 завода, при том что 2 завода мы можем получить за 30 политки, просто взяв частичную мобилизацию до советника. Скупают ресурсы у СССР или не так активно чтобы свободная торговля была в плюс, или слишком поздно, когда ресурсы уже нужны для производства.",
            "Также видим что бонусы от фокусов в ветке паранойи дают большое преимущество: 116 заводов против 145 (если мы не берем скидку, то нет смысла брать советника до мобилизации экономики). Честно говоря, не ожидал такой разницы, бонусы не выглядели для меня настолько сильными (+5% к скорости строительства, -2% тнп, 10% к лимиту зданий в провинции, 2 фабрики, 2 завода). Товаров народного потребления дают не так много, +5% это как бонус от свободной торговли, места под строительство у СССР и так хватает. Но в сумме разница выходит почти в 30 заводов.",
            "Ну и самое важное: видим малые производные на достаточно широком участке. В сущности, при строительстве от 28 до 50 фабрик, количество военных заводов на 1 января 1941 года остаётся +/- стабильным.",
            "Как было сказано выше, количество заводов относительно постоянно на участке от 28 фабрик до 50 фабрик, рассмотрим это две точки подробнее:",
            "Если начинать строить заводы раньше, то снаряжения к итоговой дате будет больше (см. площадь под графиками военных заводов. Также стоит учесть что эффективность производства будет нарастать со временем, значит, соотношение снаряжение будет больше соотношения площадей), однако я не уверен, насколько следует переходить к максимизации снаряжения. В игру добавили новую систему снабжения (да, для меня она всё еще новая), теперь она требует строительства пунктов снабжения и железных дорог. Таким образом, нельзя считать экономику эффективной если мы можем произвести снаряжение, но не можем доставить его до фронта. Портальные технологии еще более увеличивают важность логистики, т.к. вся произведенная техника сначала телепортируется в столицу и уже после этого отправляется на фронт вместе с пряниками из карамельной страны.",
            "Помимо оптимизации момента перехода с фабрик на военные заводы, можно также оптимизировать количество инфраструктуры. Можно оценить выгодность инфраструктуры при помощи следующего несложного скрипта (нужен только базовый python, никаких сторонних библиотек):",
            "Отдельно отмечу, что приведенный выше код не учитывает что построенные в процессе фабрики тоже будут строить.",
            "Строительство инфраструктуры, это совсем не то же самое что переход с фабрик на военные заводы. Наиболее выгодное количество инфраструктуры разнится в зависимости от количества слотов под строительство. Да и выгода не всегда велика. Скажем так, оптимальное строительство инфраструктуры потребует от игрока определенных усилий.",
            "Наиболее универсальная оптимизация удалась, но что делать с частными случаями пока не понятно. Баланс между логистикой и количеством снаряжения для разных государств будет разным. Понимание того как этот баланс устроен требует большего понимания самой игры (что требует играть в игру правильно, а это не наш случай).",
            "Оптимизация инфраструктуры выглядит многообещающе, но оценка желания игроков применять это оптимизацию - нет. Также сам процесс нахождения оптимального алгоритма строительства с инфраструктурой выглядит достаточно сложно.",
            "В текущей программе упущен такой момент как возможность аннексии других государств. Этот процесс не обязательно сопровождается войной (а тем более серьезной войной), так что под концепцию довоенной экономики вполне подходит. В программе уже реализован алгоритм реализации контроля провинций, остаётся лишь добавить механизм добавления провинций по тэгу страны владельца или по принципу выделяемых стран (например возвращение польских территорий можно реализовать через добавление к СССР всех провинций Белоруссии и Украины, которые еще не входят в состав Советского Союза). Подобное может потребовать расчета строительства для аннексируемых стран (они же тоже развивают свою экономику параллельно вам), что замедлит расчет, но можно упростить его просто до строительства военных заводов, думаю это не должно добавить много погрешности.",
            "С интерфейсом пока всё совсем не здорово. Идеи приходящие мне в голову или звучат очень сложно или звучат еще менее удобно чем редактирование текстовых файлов. Так что gui пока застопорился.",
            "readme.txt будет обновляться по мере внесения изменений в программу. Если вы считаете что в нём чего-то не хватает, можете мне написать в дискорде или прямо в комментарии под этой статьей. В целом, планирую постепенно работать над читаемостью кода и документацией проекта.",
            "Весь код репозитория распространяется по лицензии MIT, которая гласит следующее: \"делайте с кодом что хотите, но не надо из-за него со мной судиться\". В общем, свободное ПО и всё такое, развлекайтесь, если хотите."
        ]
    },
    {
        "Название статьи": "Не только ORM (NoORM)",
        "Дата публикации": "2024-06-02, 14:46",
        "Автор статьи": "maslyaev ",
        "Статья целиком": [
            "Привет, Хабр! Хочу поделиться самодельной питонской библиотекой (ссылка на GitHub в конце статьи), существенно упрощающей взаимодествие с базами данных.",
            "На настоящий момент популярны два основных способа организовать работу с базой данных:",
            "Воспользоваться «низкоуровневым» интерфейсом СУБД. Например, если у нас база данных в SQLite, можем работать через стандартную библиотеку sqlite3.",
            "Задействовать какой-нибудь из популярных ORM-ов, например, SqlAlchemy.",
            "Первый вариант хорош для небольших скриптов, простеньких микросервисов и прочих изделий, объём исходного кода которых не превышает пары тысяч строк. В больших и серьёзных проектах хождение в базу данных через низкоуровневые интерфейсы становится мучительным, и народ предпочитает работать через ORM. Но ORM это тоже не сахар с мёдом. На примерах из туториала любой ORM выглядит как сбывшаяся мечта, но по мере роста функциональности системы, объёма базы данных и нагрузки выясняется, что ORM-ное счастье не было бесплатным. Ранее здесь я уже поворчал о том, что задача \"Object-Relational Mapping\" решения не имеет, и что-то с этим нужно делать. Предложенный тогда вариант просто взять и посадить себя на без-ORM-ную диету был явно непрактичным, поэтому в серьёзных проектах ничего другого не оставалось, как продолжать есть этот кактус, но при этом не прекращать попыток придумать альтернативу.",
            "Постепенно выкристаллизовалась идея NoORM (\"Not only ORM\", по аналогии с \"Not only SQL\"):",
            "Мы не объявляем священную войну ORM-ам, а гармонично с ними сосуществуем, давая альтернатианые решения там, где ORM-ы традиционно приносят головную боль.",
            "Мы не пытаемся сделать плюс ещё один тысяче первый, но на этот раз самый окончательно правильный ORM. Чётко осознаём, что задача \"ORM\" не имеет решения.",
            "Технология должна быть применима не только на маленьких поделках и DIY-проектах, но и на больших и сложных бэкендах.",
            "Фокус на улучшение developer experience. Все эти удобняшки современных IDE – атодополнение, подсветка синтаксиса, переход к объявлению – всё это очень сладко, вносит комфорт и повышает продуктивность.",
            "Технология должна быть подобна истинному дао – делать правильные вещи простыми и естественными, сопротивляясь использованию корявых и неэффективных решений.",
            "Назовём клиентом программу, общающуюся с сервером СУБД. Проигнорируем тот факт, что этот наш клиент сам, возможно, для кого-то является сервером. Взаимодействие с СУБД построено таким образом, что сервер в качестве запроса принимает, по сути, текст программы (SQL это тоже язык программирования, даже не пытайтесь спорить), исполняет её, и отдаёт назад результат. Структура результата зависит от текста запроса, а также от схемы базы данных.",
            "Весьма странный API, не правда ли? Нетипичный. Какой-то текст на вход, какая-то табличка (или число, или просто ничего) на выход – вот и вся схема, вот и весь контракт. Этим, конечно, достигается потрясающая функциональная гибкость, но с точки зрения кода клиента, написанного на «обычном» языке программирования, это сущий кошмар.",
            "Какое-то невнятное \"Any\"... В таком простеньком случае это можно и пережить, но по мере развития проекта код неизбежно превращается в отвратительную смесь языков программирования, в которой код на SQL разбросан по основной логике в виде строковых литералов. В таком сложно разбираться, такое трудно развивать, такое мучительно сопровождать.",
            "ORM-ы, собственно, нужны как раз для того, чтобы сделать взаимодействие с базой данных более естественным для того языка, на котором реализована логика приложения:",
            "Взаимодействие с БД через ORM можно схематично изобразить так:",
            "Примечательно здесь то, что работа с базой данных идёт через персистентные объекты, являющиеся экземплярами «модельных» классов, описывающих структуру БД. Эти персистентные объекты умеют себя прочитать из базы и в неё себя записать. Они живут внутри открытой сессии. И ещё эти объекты умеют «лениво» дотягивать из базы связанные с ними другие персистентные объекты. Эти самые персистентные объекты – корень всех проблем:",
            "По сути, это передача мутабельного объекта в другой процесс. Безобразно тупая затея. Мы запросили сущность «пользователь Вася» из базы данных в процесс своего бэкенда, и теперь где у нас теперь мастер-копия? Как мы их собираемся синхронизировать, в какой момент, и что собираемся делать с возможными коллизиями?",
            "Что случается с живущими в сессии объектами когда сессия закрывается? Что если они продолжают быть нужны в логике приложения? Что если эта логика продолжает считать, что это по-прежнему нормальные объекты, принадлежащие живой сессии?",
            "Невозможно найти единственно правильный баланс между eager- и lazy-загрузкой. Если увлекаемся lazy, получаем проблему N+1, и всё начинает страшно тормозить. Если увлекаемся eager, на каждый невинный чих ORM пытается вычитать полбазы, и тоже всё тормозит. Короче, у нас две педали, но обе они педали тормоза.",
            "Идея персистентных объектов – тяжёлое наследие платоновской концепции «Мира идеальных сущностей». Поначалу нам может показаться соблазнительно один раз на веки вечные и на все случаи жизни реализовать класс Person, но потом внезапно оказывается, что с точки зрения сервиса аутентификации пользователей Person это одно, с точки зрения бухгалтерии другое, а с точки зрения HR третье, и эти точки зрения местами противоречат друг другу. Мы пытаемся создать класс Person, экземпляры которого будут удобны и полезны везде, но в итоге у нас получается корявый, огромный и чрезвычайно капризный программный монстр, жрущий как аппаратные ресурсы, так и рабочее время сотрудников. Даже если база данных одна общая на всех, даже если таблица \"persons\" там тоже одна, всё же для разных целей нам бывает удобно делать совсем разные, порой весьма причудливые SELECT-ы. Одна из ключевых идей NoORM – отказ от использования персистентных объектов. Не модельных классов, заметьте, а именно персистентных объектов.",
            "Создаём в своей программе дополнительный слой, и тем самым рассмотренный ранее «весьма странный API» превращаем в обычный:",
            "Для любого императивного (и тем более функционального) языка программирования самая естественная в мире вещь это функция, которую можно вызвать с известно какими параметрами, и которая вернёт результат известно какого типа. С точки зрения кода-потребителя программный интерфейс БД выглядит как-то так:",
            "Что мы здесь видим:",
            "У нас есть модуль db_api, в котором есть модуль users, в котором есть функция get_users.",
            "Эта функция принимает на вход соединение с базой данных и отдаёт список объектов DbUser, у которых есть атрибуты email, id и username.",
            "Всё это замечательно дружит с удобняшками IDE, линтерами и mypy.",
            "Мы не свинячим SQL-запросами ровным слоем по всей кодовой базе, а собираем их в одном месте – в модуле db_api. В результате код, реализующий основную логику приложения становится более компактным, понятным, гладким, шелковистым и приятным на ощупь как котёнок.",
            "Что касается DbUser, то это никакой не персистентный объект, никакая не платоновская идеальная сущность, а всего лишь dataclass, в который заворачивается результат конкретного запроса. Вот как это выглядит в модуле db_api.users:",
            "У вас может возникнуть резонный вопрос: а не закончится ли это тем, что у нас в \"db_api\" будут сотни и тысячи каких-то маловразумительных датаклассов и функций, и ориентироваться в этом станет совсем невозможно? Честно скажу, у меня самого были такие опасения, когда в порядке эксперимента я взялся переводить с ORM на NoORM один приличного объёма сервис, который много и разнообразно общается с базой данных. Однако обошлось. Более того, стало значительно легче находить ответы на вопросы о том, как, где и для чего используются конкретные таблицы и поля базы данных. Стал проще рефакторинг. Избавившись от персистентных объектов, избавились от необходимости держать открытой сессию на протяжении всей обработки клиентского запроса. Плюс абсолютно предсказуемое поведение коммита – в базу пишется только то, что мы хотим в неё записать здесь и сейчас, и нет никаких персистентных объектов, которые по какой-то неясной причине тоже решили пристроиться к этому коммиту. Ну и, самое сладкое, все \"N+1\" стали видны как на ладони – если мы в потенциально длинном цикле вызываем какую-то функцию, в которую параметром передаём соединение с БД, мы же не просто так его туда передаём, а, очевидно, для того, чтобы сходить в БД столько раз, сколько раз прокрутится цикл.",
            "ORM это не только плохие капризные портящие нам кровь персистентные объекты, но и множество восхитительных удобств, от которых нет смысла отказываться:",
            "Модель структуры базы данных. Можно держать структуру базы данных в голове, можно нарисовать её тушью на ватмане, можно в разбросанных по столу черновиках, можно в заметочках в ноушене или на страничках в конфлюенсе, а можно в питонском коде под гитом. Последний вариант мне кажется самым симпатичным.",
            "Миграции. Они в любом случае боль, но ORM-мы умеют облегчать страдания. Глупо этим пренебрегать.",
            "Я тут сказал много злых слов про персистентные объекты, но если их не пускать в «боевой» код, а использовать только для генерации тестовых данных, то там они чудо как хороши. По сути, едва объявив структуру данных, мы сразу забесплатно «из коробки» получаем для неё реализованный CRUD. Когда нам абсолютно наплевать и на производительность, и на масштабируемость, и на конкурентное исполнение, тогда персистентные объекты – прекрасное решение.",
            "DB-API-функция извлечения данных, работающая через SqlAlchemy выглядит так:",
            "Знаю, некоторым в тягость такой стиль написания SQL, но нельзя не признать, что у него есть свои преимущества, особенно на простых запросах.",
            "По ходу «опытной эксплуатации» этой технологии выработались несколько рекомендаций, которых полезно придерживаться:",
            "Не надо нагружать бизнес-логикой объекты, возвращаемые DB API. Пусть это будут просто датаклассы или даже namedtuple-s. Впрочем, никто не мешает реализовать несколько дополнительных свойств, если их вычисление в SQL-запросе по каким-то причинам затруднительно.",
            "Объявление этих датаклассов – непосредственно перед функциями, которые их будут возвращать. Точно не в отдельном модуле. Между SQL-запросом и тем местом, где определяется структура его результата должно быть не далеко ходить. Идеально, если объявление датакласса вместе с SQL-запросом помещаются одновременно на один экран.",
            "С именами этих датаклассов сильно упариваться не нужно, но удобно, когда они выделяются визуально – когда мы видим, что имя типа переменной начинается с \"Db\", мы сразу понимаем, что значение взялось из базы данных.",
            "Удобно, когда выработано некоторое соглашение об именовании DB-API-функций. Мы используем префиксы \"get_\", \"ins_\", \"upd_\", \"del_\", \"upsert_\". Для функций, в которых принудительно отключается автокоммит, используем суффикс \"_no_commit\".",
            "Когда DB-API-функций мало, их можно держать в одном модуле \"db_api.py\". Если их становится больше, распиливаем этот модуль по функциональным областям, например, \"db_api/users.py\", \"db_api/orders.py\", \"db_api/warehause.py\".",
            "Если в монорепозитории живёт несколько подсистем, есть смысл иметь один общий модуль \"db_api\", но специфические вещи вынести в \"db_api\"-модули подсистем.",
            "Если какая-то часть системы по своей сути является скопищем SQL-запросов (например, коллекция даталоадеров для GraphQL), оставьте эти запросы где они есть. Просто избавьтесь от персистентных объектов, но ни в какое \"db_api\" не выносите.",
            "В принципе, NoORM-стиль можно практиковать и без дополнительной библиотеки, но тогда нам приходится каждый раз писать одинаковый код, вызывающий исполнение запроса и преобразующий результат в датаклассы. Это раздражает и утомляет. Кроме того, выгода от вынесения доступа к базе в отдельные DB-API-функции становится менее очевидной, и в результате всё заканчивается тем, что мы снова начинаем свинячить SQL-код где попало.",
            "Библиотека предельно проста в использовании. Всего лишь пять декораторов – sql_fetch_all для изготовления функции, возвращающей список объектов, а также sql_one_or_none, sql_scalar_or_none, sql_fetch_scalars и sql_execute для сами угадайте чего. Плюс немножко дополнительной функциональности, в частности, реестр функций, автоматически собирающий метрики. Всё это реализовано для:",
            "SQLite – через стандартную библиотеку sqlite3 и async через aiosqlite.",
            "Postgres – sync через psycopg2 и async через asyncpg.",
            "MySQL/MariaDB – sync через PyMySQL и async через aiomysql.",
            "Для всего остального, с чем работает SqlAlchemy, если выбран вариант «NoORM через ORM». Тоже в исполнениях sync и async.",
            "Если нужно что-то ещё, пишите в гитхаб в Issues. Руки чешутся добавить адаптеры для Mongo, но пока удаётся себя сдерживать. С интересом смотрю в сторону PonyORM, и если кому-нибудь это надо, могу добавить. Адаптера для Django нет и не будет, поскольку, к сожалению, работа через персистентные объекты там безальтернативна.",
            "Обещанная ссылка на гитхаб: здесь.",
            "P.S. «НоуОуАрЭм» – язык сломаешь, поэтому прижился вариант произношения /nuːrm/ – «нурм», «нурмализация», «сделаем сразу нурмально»."
        ]
    },
    {
        "Название статьи": "Создание масштабируемых RL систем с Ape-X",
        "Дата публикации": "2024-06-01, 11:56",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Ape-X представляет собой подход к обучению с подкреплением, разработанный для использования в масштабируемых распределенных системах.",
            "Основная идея Ape-X заключается в разделении ролей на акторов, которые взаимодействуют с окружением и собирают данные, и учеников, которые используют эти данные для обучения модели. Такое разделение позволяет ускорить процесс обучения и предотвратить заучивание субоптимальных политик.",
            "Акторы являются агентами, которые взаимодействуют со своими экземплярами среды. Каждый актор выбирает действия, основываясь на общей нейронной сети, которая используется для предсказания действий. Акторы аккумулируют свой опыт в общей памяти повторного воспроизведения. Этот опыт включает состояния, действия, вознаграждения и следующие состояния, полученные в результате взаимодействия со средой.",
            "Ученики занимаются обучением модели. Они извлекают образцы опыта из общей памяти воспроизведения и обновляют параметры нейронной сети на основе этих данных. За счёт того, что обучение происходит вне среды, ученики могут параллельно работать с большим количеством данных.",
            "Приоритетная буферизация опыта (в дальнейшем PER) была предложена для решения проблемы равномерного распределения опыта в стандартных методах опыта с повторным воспроизведением. В классическом опыте с повторным воспроизведением все записи имеют одинаковую вероятность быть выбраны для обучения, что может привести к медленному обучению и субоптимальным результатам. PER решает эту проблему, присваивая приоритеты каждому опыту на основе их важности.",
            "Если коротко, то PER работает так:",
            "Оценка ошибки TD:",
            "Каждому опыту присваивается приоритет на основе ошибки TD.",
            "Опыт с большей ошибкой TD получает более высокий приоритет, т.к он содержит больше информации для обучения модели.",
            "Обновление приоритетов:",
            "При каждом добавлении нового опыта или обновлении модели, приоритеты пересчитываются.",
            "Это позволяет системе адаптироваться к новым данным и изменяющимся условиям среды.",
            "Выбор опыта для обучения:",
            "Для выборки опытов используется стохастический процесс, основанный на приоритетах. Опыты с более высоким приоритетом имеют большую вероятность быть выбраны.",
            "Тем не менее, чтобы избежать переобучения на небольшом наборе данных, в процесс добавляется некоторая степень случайности.",
            "Коррекция смещения:",
            "Использование приоритетной выборки может ввести смещение в процесс обучения. Для его коррекции используется важностное взвешивание, что позволяет уменьшить вероятность ошибки из-за неравномерного выбора данных.",
            "PER позволяет быстрее выявлять и корректировать ошибки модели, ускоряя процесс сходимости. В Ape-X это мастхев, т.к множество параллельных акторов генерируют большие объемы данных, и важно по максимуму использовать эту информацию.",
            "Идем к другим не менее важным механизмам.",
            "Централизованная память воспроизведения:Все акторы отправляют свой опыт в централизованную память воспроизведения. Это позволяет ученикам иметь доступ к большему количеству данных и использовать их для более качественного обучения модели. Коммуникация между акторами и памятью воспроизведения осуществляется пакетами.",
            "Обучение вне политики:Ape-X использует обучение вне политик. Каждый актор может иметь свою стратегию исследования среды, что расширяет разнообразие получаемого опыта.",
            "Будем использовать PyTorch и Ray. PyTorch имееь средства для создания и обучения нейронных сетей, а Ray помогает управлять распределенными вычислениями и координировать работу множества акторов и учеников. Пошагово настроим окружения для Ape-X, используя AWS EC2 или локальные машины.",
            "Будем использовать Python 3.8 и более новые версии библиотек PyTorch и Ray. Для начала, создадим виртуальное окружение:",
            "Далее, установим необходимые библиотеки:",
            "Для масштабирования системы используем AWS EC2. Создаем несколько инстансов, которые будут работать в распределенной среде. Для этого заходим в консоль управления AWS, выбираем EC2 и создаем новый инстанс с нужными характеристиками (например, t2.large для акторов и t2.xlarge для учеников).",
            "После создания инстансов, подключаемся к ним через SSH:",
            "Устанавливаем необходимые пакеты на каждом инстансе:",
            "Окружение готово и мы готовы создавать конфигурационные файлы и запускать Ape-X. Начнем с создания файла config.py, который будет содержать основные параметры:",
            "Затем создадим файл train.py для запуска процесса обучения:",
            "Для запуска на одном узле достаточно выполнить команду:",
            "Для многоузловой конфигурации нужно запустить Ray на каждом узле:",
            "После этого можно запустить train.py, и все узлы будут участвовать в обучении.",
            "С Packer можно автоматизировать создание образов машин. Создадим файл конфигурации packer.json:",
            "Запускаем Packer для создания образа:",
            "Теперь есть готовый образ, который можно использовать для создания новых инстансов EC2 с уже установленными необходимыми библиотеками.",
            "Для запуска Ape-X на одной машине можно юзать этот скрипт:",
            "Для многоузловой конфигурации необходимо настроить несколько экземпляров и настроить взаимодействие между ними.",
            "Теперь мы получили полностью настроенное и работоспособное окружение для реализации Ape-X.",
            "В завершение предлагаю посетить открытые уроки по Reinforcement Learning от OTUS:",
            "13 июня: Применение нейросетевых моделей в обучении с подкреплением. Записаться",
            "17 июня: Построение торгового агента на базе фремворка FinRL. Записаться"
        ]
    }
][
    {
        "Название статьи": "Магия динамического маппинга. Реализация универсальной обработки файлов нефиксированной структуры на Python",
        "Дата публикации": "2024-06-05, 15:58",
        "Автор статьи": "spectr_dev ",
        "Статья целиком": [
            "Привет! На связи Никита Ильин из Spectr, Backend-разработчик с опытом более 5 лет.",
            "Один из проектов, с которым мы работаем, — IBP-платформа для планирования и прогнозирования спроса и продаж в ритейле. В статье поговорим о конкретной реализации для одной из задач в рамках этой платформы на Python и Django. При этом сама концепция может быть реализована абсолютно на любом фреймворке или платформе: Spring, .NET, Laravel.",
            "Мы разрабатываем IBP-платформу для крупной корпорации, где на основе данных, которые поступают из смежных систем, строятся прогнозы и аналитика. И одна из областей — работа с огромным количеством файлов из внешних источников: чтение, обработка, загрузка и запись всех этих данных в БД. При этом существует большое количество различных источников и форматов этих файлов.",
            "Глобально задача состоит в том, чтобы осуществить загрузку из внешних источников в единую внутреннюю систему для последующего анализа и прогноза.",
            "Информация об источниках.",
            "Сейчас в системе около 350 источников. При этом одновременно может поступать до 100 штук новых.",
            "Важное уточнение: один источник — это файл уникальной структуры. Если названия столбцов различаются, то это уже новый файл, новый источник данных.",
            "Информация о файле. Это обычный классический csv-формат. Разделителем может быть либо «;» либо «,». Число колонок — от 5 до 200, но распарсить нужно только количество, которое обозначено в техническом задании. В нашем случае доходило до 40 колонок, все остальное — системные поля. Число строк всегда большое — от 5 млн до 20 млн.",
            "Ниже представлен классический пример файла с условными обозначениями.",
            "Отлично, вся информация о данных у нас на руках! Что же делаем? Тут возможны два пути.",
            "Первая идея, которая у нас возникла, — статическая реализация или, по-другому, «решение в лоб». При такой реализации мы для каждого источника пишем свой парсер и применяем его — эффективно и быстро. Поговорим о преимуществах чуть подробнее.",
            "Быстрая разработка. Раньше уже существовало какое-то количество источников и мы уже знали, как работать по этому пайплайну: без лишних оптимизаций и размышлений. То есть мы приоритезируем источники, отдаем наиболее важные по значимости в первую очередь, а сами занимаемся другими.",
            "Работает надежно, как швейцарские часы. Когда мы пишем парсер на конкретный источник, мы можем написать на него тест. И перед тем как выкатывать, надо посмотреть — а точно ли поведение такое же, как ожидается? Тут мы говорим про стабильность и проверенность того, что наш парсер работает.",
            "Без непредвиденных side-эффектов. Все парсеры строго императивны и изолированы и, благодаря этому, более стабильны. Ведь когда мы говорим про enterprise-разработку, мы точно не хотим, чтобы один наш парсер сломал всю систему.",
            "Сходить на нужный внешний сервер по SFTP.",
            "Именно SFTP, потому что это спецификация нашей задачи, мы так договорились, нам так комфортно общаться между серверами.",
            "Забрать файл.",
            "Применить парсер.",
            "Сохранить в БД.",
            "Работа аналитика. Сверка с локальной структурой БД и проектирование перевода из внешнего имени во внутреннее. Нам нужно узнать, что, где и как хранится, какая между всем этим связь. Данные формируются в виде спецификации, требований и отдаются разработчику.",
            "Работа Backend-разработчика.",
            "Написание кода для валидации файла и его перевода. То есть это сам код, который осуществляет разбор данных в соответствии с ожидаемой структурой.",
            "Добавление конфигурации в общую структуру парсеров: на какой сервер идем, куда и как данные сохраняем. Мы обобщили все на уровне кода, а теперь надо это явно прописать, куда мы идем, чтобы получить данные о конкретном источнике.",
            "Например, когда у нас SFTP-сервер, я не думаю, что вы будете писать одни и те же четыре строчки кода в каждом парсере. Скорее всего, это будет какая-то функция или класс, которому передаем имя файла и доступы для SFTP-сервера, чтобы он пустил в систему.",
            "Работа тестировщика. Отладка, тесты на деве, стейдже на обрезанных данных. На этом этапе мы выявляем и устраняем ошибки в коде парсера. То есть тестировщик должен проверить две вещи:",
            "работает ли код. То есть он берет маленький кусочек файла, например 100–1000 строчек, и смотрит, встали ли данные в БД, в интерфейс, не свалилось ли что-то и работает ли функционал в целом;",
            "какая скорость. Нужно понять, удовлетворяем ли мы скорости загрузки и нет ли каких-то проблем.",
            "Работа тестировщика и DevOps-инженера. Пуско-наладка на реальных данных. Далее мы все выкатываем на прод, делаем первые итерации, проверяем, все ли работает: встали ли данные в интерфейс, ничего ли не потерялось, выдерживаем ли по ресурсам.",
            "Время разработки пропорционально количеству парсеров. Для разработки одного парсера нам нужно 5 дней. Сверка со структурой БД занимает 1 день, написание парсера — 2 дня, на отладку/тесты и пуско-наладку закладываем еще по одномуу дню.",
            "На один парсер — 5 дней, а на 100 — целых 1,5 года. Процесс, конечно, можно оптимизировать и вести параллельно: аналитику не ждать разработчика, а разработчику не ждать тестировщика. Но тем не менее все это в сумме — большой объем очень рутинной работы.",
            "Время доработок пропорционально количеству парсеров. Классический пример: есть 200 источников, при этом появилось требование о том, что столбец — это не целое число, а число с плавающей запятой. И теперь нужно это отвалидировать, чтобы все было в порядке, иначе данные не будут сходиться. А в случае переиспользования кода (DRY) нужно еще приложить усилия к тому, чтобы подумать, как это сделать. К тому же нужно заново пройти по пайплайну. Мы сделали изменения — значит, нам нужно все заново проверить, посмотреть, выкатить и замерить все парсеры.",
            "По итогу формула будет такой: время множественного изменения = время одного изменения * число парсеров. То есть время на расширение числа парсеров или валидируемых ими полей эквивалентно времени разработки. Не получится делать быстрее дублирующуюся работу, придется делать с такой же скоростью.",
            "Расширение числа парсеров или валидируемых ими полей запускает пайплайн заново. Подобная ситуация случилась лично с нами. Изначально было N парсеров, далее убрали 10 и потом добавили еще 15 сверху. А после этого в 20 имеющихся парсерах изменился состав файла и добавился еще один внешний сервер. Приходится начинать все сначала: аналитика –> разработка –> тестирование.",
            "Ключевой вопрос в этой всей ситуации — как преодолеть проблемы «решения в лоб» и сделать результат нашей работы максимально самодостаточным? Мы подумали об этом и пришли к другой идее — динамической реализации.",
            "Давайте вспомним No-code-приложения, например Tilda. Или такой конструктор мобильных приложений, где вы тащите формы, а затем система сама выполняет работу. Код генерируется — вы наслаждаетесь. Примерно то же самое мы сделали в рамках нашего проекта.",
            "В один момент мы подумали: «А что, если разработать пользовательский интерфейс, который позволит самостоятельно решать задачи, минуя аналитика, разработчика и тестировщика?» То есть пользователь сможет сам создать описание для своих действий в админке в виде шаблона. Затем наш магический механизм обработает созданный шаблон. А динамический парсер интерпретирует файлы, соответствующие структуре, описанной в шаблоне, без необходимости дополнительной ручной обработки.",
            "Эту идею мы назвали — шаблон динамического маппинга.",
            "На картинке ниже представлено, как это все можно изобразить с точки зрения пользовательского интерфейса. В этом списке темплейт — наш источник. Мы его назвали шаблоном.",
            "Далее представлено, какие примерно атрибуты могут быть у этого шаблона:",
            "название (name) — на что смотреть в интерфейсе;",
            "внешняя система (external system) — на какой сервер нужно пойти, чтобы достать конкретный файлик (уникальное системное имя шаблона);",
            "имя файлика и директория, в которой он лежит (filepath) — путь к файлу на сервере, с которым связан этот шаблон, и здесь же и имя файла;",
            "системное обозначение для источника (system name) — выбор из внешних систем, куда мы будем подключаться, чтобы туда идти по пути выше.",
            "Дальше мы уже говорим о том, что внутри этого источника. Это находится в отдельной сущности — attributes. Эта сущность включает в себя такие элементы, как:",
            "name — чтобы человеку было на что смотреть в интерфейсе;",
            "system_name — уникальное системное имя поля шаблона, которое мы должны искать в файле;",
            "type — тип данных поля, такие как float, str, str_alpha_numeric, date, int, bool;",
            "field_representations (представления поля) — JSON-структура, представляющая отображение поля на БД;",
            "template (шаблон, Foreign Key) — связь шаблона с общей инфой, к которому относится данное поле.",
            "На каждом этапе присутствует валидация, которая проверяет, например, наличие файлика, полей в файлике, соответствие типу. И, в конечном итоге, это все может записаться в БД.",
            "Сейчас мы построили чисто концептуальное решение. Давайте разберемся, какой результат нам бы принес этот подход — поговорим о его преимуществах.",
            "Элемент продукта закончен",
            "Такой формат реализации сокращает все возможные согласования. Вместо написания отдельного парсера для каждого нового файла и его источника создаем шаблоны, описывающие структуру файла и определяющие соответствие полей самостоятельно. Это существенно сокращает процесс работы, к тому же позволяет сразу же проверить результат. И это будет работать уже завтра. Сегодня написал — завтра это уже готово, сегодня придумал — завтра уже на проде валидируешь новый файл.",
            "Настройка и поддержка, которая оптимизирует время",
            "В случае со статической реализацией каждый день на протяжении полутора лет придется заниматься переводом спецификаций в код. Естественно, это не творческая и скучная задача. Мы все-таки хотим закрыть эту задачу и, конечно, сделать это наиболее интересным для нас способом.",
            "При динамической реализации настройка и поддержка будут намного интереснее, чем просто сконструировать «решение в лоб» и сидеть полтора года переводить спецификации.",
            "Возникает ряд вопросов. А что если где-то что-то отвалится? А почему данные не загружаются? А как вообще это все сохранить в табличку и как это все будет выглядеть? А как эти 20 млн строчек обработать? Явно придется над всем этим поразмыслить. 5 часов подумать — 1 час написать код.",
            "Масштабируемость",
            "Появляются новые требования:",
            "добавить в N-количестве источников проверку на дубликаты и действие, которое надо совершать, если они есть;",
            "а еще в M-источниках добавить проверку на отрицательность.",
            "При этом непонятно, будут ли все эти поля в файле.",
            "Так у нас появились дополнительные атрибуты у поля шаблона:",
            "required (обязательное) — флаг, указывающий, является ли поле обязательным для заполнения;",
            "can_be_negative (может быть отрицательным) — флаг, указывающий, может ли поле содержать отрицательные значения;",
            "contained_in_md_table (содержится в таблице md) — имя таблицы md, в которой содержится это поле (если применимо);",
            "contained_in_md_attribute (содержится в атрибуте md) — имя атрибута md, в котором содержится это поле (если применимо);",
            "duplicates_in_table (дубликаты в таблице) — имя таблицы, в которой разрешены дубликаты этого поля (если применимо);",
            "duplicates_attribute (атрибут дубликатов) — атрибут, определяющий дубликаты этого поля (если применимо);",
            "duplicates_action (действие с дубликатами) — выбор из действий по обработке дубликатов: обновление или пропуск.",
            "И на все это есть 5 дней — вспоминаем сроки разработки одного парсера. В такой системе мы на этапе валидации прописываем новые условия один раз, а дальше клиент уже сам работает с шаблонами и сам отвечает за выбранные им параметры.",
            "admin API (CRUD — в админке) — могут вносить изменения и создавать новые записи;",
            "user API (Read — для всех пользователей) — есть возможность только читать.",
            "Python и Django — это решение удобно для нас, к тому же мы используем его с начала работы над проектом.",
            "Blazingly-Fast Polars — о том, почему мы выбрали именно этот инструмент, рассказывал наш тимлид в статье Битва медведей: Pandas против Polars. Если кратко: этот вариант для наших вариантов использования работает быстрее, чем Pandas.",
            "Paramiko — библиотека для подключения по SFTP. Очень красиво и надежно.",
            "SQLAlchemy — в качестве дополнительной ОRМ. Удобный интерфейс, быстро и красиво.",
            "Здесь мы перевели сущность модели в сущность шаблона, которая была до этого. Из интересного — здесь есть функция temporary_table_name, в ней мы получаем temporary-имя. Это название временной таблицы. О том, для чего нам нужна временная таблица, поговорим чуть позже.",
            "Далее то же самое делаем для TemplateField. Можно просто взять, перевести на другой язык — и все готово.",
            "Как я и говорил, у админов будет полный набор CRUD-операций: чтение, создание, обновление. Но следующий момент более интересный.",
            "Вот эти два поинта ниже нам нужны для того, чтобы у клиента в интерфейсе была возможность посмотреть, какие у нас есть таблички и атрибуты и поля этой таблички.",
            "Классические представления — обычные классы доступа. Здесь идет пагинация по страницам. Далее, когда мы предоставляем список шаблонов, мы передаем только основную информацию о них — название. А когда отдаем только один шаблон, мы его отдаем вместе с атрибутивным составом, чтобы пользователь мог убедиться, что у него все правильно загружается.",
            "На этом этапе мы получаем какой-то путь файла у этого темплейта, то есть не пишем явно — идти на такой-то сервер. А просто говорим — взять у темплейта название файла. И на выходе получается путь файла.",
            "Далее через библиотеку мы подключаемся по SFTP, забираем файл.",
            "У темплейта есть филды — template.fields. Мы их забираем — это и будут наши правила валидации.",
            "django_file = InMemoryUploadedFile",
            "validate_file(django_file, list(template.fields.all()))",
            "BasicFileHeadersValidator. В процессе валидации проверяем наличие данных, дублирование колонок, наличие требуемых колонок. Если какой-то из этих пунктов не проходит, мы отправляем пользователю ошибку, так как нам незачем загружать файл, у которого нет требуемых нам колонок. Мы знаем, что он заведомо сохранит его туда, куда нам не надо.",
            "BasicFileReaderValidator / GeneralValuesFieldsValidator. Базовое чтение (проверка на кодировку, соответствие строк размеру), перевод названий колонок согласно шаблону, нормализация данных и проверка их на соответствие типам, генерация дата-фрейма.",
            "Даже если взять 10 млн строк, с учетом того, что параллельно работает 100 источников, cкорее всего, они упадут по памяти.",
            "Что с этим делать? Классический вариант — разбиение на чанки. В результате этого получается кусок файла, c которым можно продолжать работать. Далее с ним проводим соответствующие этапы валидации и формируем соответствующий дата-фрейм. Но в такой структуре важно, чтобы мы не захотели сделать дополнительную логику — например, агрегацию, дезагрегацию, суммирование, фильтрацию, категоризацию. Для всех этих работ мы используем сохранение во временную таблицу, а потом при необходимости все это сохраняем в основную БД.",
            "Если требование состоит в том, что решение нужно кастомное, то это делает уже другой разработчик в рамках другой задачи. Он идет во временную таблицу, забирает все нужные данные и, соответственно, с ними делает то, что нужно ему. При этом временную таблицу нужно каждый раз чистить перед загрузкой, иначе вы упадете по памяти в БД или будете работать со старыми данными.",
            "Что будем использовать дальше? Во-первых, SQLAlchemy. Собираем из дата-фрейма название колонок, делаем сущности колонок, берем название таблицы и с помощью контекстного менеджера вставляем какое-то количество записей. В нашем случае — 1 млн.",
            "Что такое контекстный менеджер",
            "В Python можно использовать удобный декоратор для этого паттерна. Перед началом мы напишем такой connection_url, где мы вставляем доступ к БД. Здесь конструкция try-finally говорит о том, что мы пытаемся отдать наш движок подключения к БД. Но в любом случае, какие бы ошибки ни были, этот движок будет в конечном итоге закрываться. Это нужно для того, чтобы в БД не висело открытое подключение.",
            "Берем нужную нам модель представления данных, написанную на Django.",
            "Здесь есть специфичный для Django код. Но я почти уверен, что таким образом можно получить табличку из основной БД на любом другом языке.",
            "Дальше мы смотрим поля у этой таблицы и пытаемся найти ее первичный ключ. Для нашего проекта специфично то, что может быть три разных первичных ключа, в зависимости от таблички: обычный ID, External ID либо Name ID.",
            "Итак, мы получили название первичного ключа. Далее мы идем в БД и проверяем, какие записи с этими ID уже есть.",
            "Мы берем их, кладем в оперативную память.",
            "После этого мы идем по дата-фрейму и проверяем, есть ли у нас такая запись.",
            "Если не смогли найти с этим ID объект среди тех, что мы вытащили из БД, то сохраняем ее как новый объект, кладем в какую-то структуру (в нашем случае список) и затем позже создадим их в БД.",
            "Но если мы все-таки смогли найти этот ID, то берем и меняем у найденного объекта все атрибуты.",
            "Условно, в файле одно значение, в БД другое значение для этой строчки — поменяли, сохранили в структуру. И здесь происходит множественное сохранение/обновление. Важно использовать batch_size. Потому что создавать запрос на миллион строчек — невыгодно. Делаем batch_size, разбиваем структуру на множество запросов и делаем с ним.",
            "batch_size=1000 — число объектов в каждом запросе.",
            "Заклинание освоено! А теперь поговорим о недостатках такого решения.",
            "Трудности в отладке и тестировании. Это интересно, но приходится думать, тратить время, пытаться понять, где ошибка, почему не можем распарсить — затратно дебажить.",
            "Строгость в соблюдении принципов. Важно соблюдать всю последовательность действий, про которую мы говорили ранее (этапы реализации динамического решения). Мы не должны запихнуть весь файл в систему, а на выходе говорить, что сделаем полностью кастомную работу.",
            "Ограничение в функциональности. Здесь как раз идет речь об отсутствии некоторых кастомных полей, все динамическое. И если вы тронете хотя бы одну строчку, то для всех остальных это изменение автоматически применится. Один файл, но его тяжело поддерживать.",
            "Дополнительное время на валидацию. Многие знают, что функции в Python вызывать довольно дорого, поэтому вы можете столкнуться с тем, что валидация может занимать много времени. Ни в коем случае не пытайтесь запустить код, который кажется примерно рабочим. Посмотрите, какие есть практики использования функции, почему нельзя использовать лямбду-функцию в цикле и т. д.",
            "Дополнительное время на сохранение во временную БД и/или основную БД. Мы говорим, что у нас есть временная таблица, и после нее вы можете делать некоторые кастомные операции, которые вы хотите. Но в то же время мы тратим минуту-две на то, чтобы сохранить эти данные, и, плюс ко всему, они занимают какую-то дополнительную память на диске.",
            "Как мы можем обойти эти минусы:",
            "Расширение поддерживаемых форматов файлов. Здесь имеются в виду Excel и все подобные структуры. Нам не потребуется писать отдельный парсер для каждого такого формата. Можно просто добавить новый вариант ридера, добавить его в общую структуру и смотреть на расширение файла перед загрузкой.",
            "Оптимизация производительности. Пытаемся ускорить валидацию, оптимизировав все возможные куски, меняя пайплайн вызова валидаторов.",
            "Развитие интерфейсов и возможных конфигураций. Админу нужно дать возможность делать batch_size для источника. То есть он знает, что, например, у него будет 100 валидированных колонок, и здесь batch_size в 100 тыс. — это слишком много, а 1,5 тыс. — уже нормально. Пусть у него будет возможность заменить на любое значение, которое он сам посчитает нужным. Дальше, если говорить о других возможных интерфейсах, у нас есть проверка на отрицательность на случай дублирования (мы меняем эту строчку или пропускаем ее и говорим, что нам все равно, есть ли она).",
            "И на этом все! В статье я поделился своим опытом, основанным на решении конкретной задачи. Надеюсь, что материал поможет вам выбрать правильное решение в аналогичной ситуации и покажет, как можно творчески подходить к задачам. А если у вас появятся вопросы — буду рад на них ответить в комментариях к этой статье!",
            "Статья подготовлена по мотивам доклада Никиты Ильина, Backend-разработчика в Spectr, на митапе #DevTalks. Ссылка на запись доклада:"
        ]
    },
    {
        "Название статьи": "Получение списка людей посещающих определенные места",
        "Дата публикации": "2024-06-05, 15:12",
        "Автор статьи": "fire64 ",
        "Статья целиком": [
            "Представьте: вы ведете Telegram-канал о животных и хотите пригласить в него посетителей зоопарка. Или вам нужно собрать контакты потенциальных клиентов, посещающих определенный торговый центр. Как это сделать?",
            "Полиция может легко получить такую информацию от мобильных операторов, но что делать обычному человеку?",
            "Ответ – использовать Telegram и его функцию \"Люди рядом\" в сочетании с Python-скриптом.",
            "\"Люди рядом\": эта функция Telegram показывает контакты пользователей, находящихся поблизости, с примерным расстоянием до них (500 м, 1 км, 2 км и 3 км). Отображаются первые 100 ближайших контактов.",
            "Python-скрипт: с помощью библиотеки telethon можно получить доступ к этой информации и автоматизировать процесс сбора контактов.",
            "Установка:",
            "Скачайте и установите Python с официального сайта: https://www.python.org/downloads/",
            "Установите необходимые модули:",
            "Регистрация приложения Telegram:",
            "Зарегистрируйте свое приложение на сайте Telegram: https://core.telegram.org/api/obtaining_api_id",
            "Важно: используйте свой реальный номер телефона, привязанный к Telegram-аккаунту, а не бота.",
            "Создание скрипта:",
            "Создайте файл с расширением .py и вставьте код скрипта (https://pastebin.com/pYPA8PF0).",
            "Замените следующие значения:",
            "api_id = (ваш API ID)",
            "api_hash = (ваш API Hash)",
            "phone_number = '' (ваш номер телефона)",
            "Запустите скрипт.",
            "Выберите на карте нужное местоположение.",
            "Укажите радиус поиска (500, 1000, 2000 или 3000 метров).",
            "Нажмите кнопку \"Начать поиск\".",
            "Скрипт автоматически получит список пользователей Telegram, находящихся в заданном радиусе, и добавит их в ваши контакты.",
            "Данные обновляются Telegram каждые 30 минут.",
            "Отображаются только первые 100 пользователей.",
            "Поиск работает только в регионе, к которому привязан ваш номер телефона.",
            "Важно использовать этот метод этично и уважительно по отношению к другим пользователям. Не рассылайте спам и не используйте полученную информацию в незаконных целях.",
            "Помните: эта информация предназначена только для ознакомления. Перед использованием подобных скриптов убедитесь, что вы не нарушаете правила Telegram и законодательство вашей страны."
        ]
    },
    {
        "Название статьи": "Автоматизация Juniper на Python",
        "Дата публикации": "2024-06-05, 15:05",
        "Автор статьи": "svg30 ",
        "Статья целиком": [
            "Добрый день, Habr. Меня зовут Сергей, я старший эксперт в компании Ростелеком. В зоне моей ответственности эксплуатация сетевого оборудования компании (в основном маршрутизаторы и коммутаторы). Когда счет устройств, с которыми необходимо работать, идет на тысячи, обойтись без автоматизации решительно невозможно. И значительная часть моей деятельности - автоматизация работы с оборудованием различных моделей и производителей, для чего, как правило, я использую скрипты на Python. В нашей компании используется оборудование различных производителей, но значительная доля оборудования - Juniper. Поэтому, в данной статье я хотел бы описать возможные подходы к автоматизации сбора информации и обслуживанию оборудования именно данного производителя. Отличительной чертой оборудования Juniper является то, что и коммутаторы, и маршрутизаторы работают под управлением одной и той же операционной системы JunOS. Конечно, доступные функции и команды отличаются в зависимости от модели, но общая операционная система позволяет использовать один и тот же подход в работе со всем спектром оборудования вендора. Так, вне зависимости от модели применяется единый подход внесения изменений в конфигурацию, когда изменения применяются только после их фиксации командой commit. У некоторых других производителей это поведение различается от модели к модели, что усложняет процесс автоматизации.",
            "В качестве подопытного для демонстрации практических действий будет выступать коммутатор Juniper EX2200.",
            "Стандартный алгоритм работы с оборудованием Juniper (да и не только с ним) без автоматизации выглядит очень просто: подключаемся к устройству по протоколу SSH (вряд ли кто-то сейчас будет использовать для этого Telnet, если только это не единственный возможный вариант), вводим в консоли команды и туда же получаем ответ устройства. Поэтому логично, что самый простой способ автоматизации работы - автоматизация подключения к устройству и отправки на него команд. И для этого в Python есть отличная библиотека - netmiko. Она не только позволяет работать с оборудованием через различные протоколы (SSH, telnet, serial), но и для большого количества известного оборудования берет на себя заботу о поиске приглашений командной строки, отключению постраничного вывода, переходу в режим конфигурации и обратно и многом другом. Да, для некоторых вендоров различные модели ведут себя несколько по-разному и требуется \"обработка напильником\", но для Juniper все работает отлично.",
            "Установить библиотеку можно, например, так: pip install netmiko Чтобы сразу перейти к практике, вот пример подключения к устройству и вывод результата работы команды show version",
            "Указываем необходимые данные для подключения в виде IP, логина и пароля (можно указать ключ для подключения как в закомментированных строках), а так же тип устройства, к которому подключаемся. Указание на тип устройства позволит библиотеке правильно обрабатывать вывод (например, отсекать от информации баннер в виде {master:0}, который добавляет juniper при ответе) и выполнить подготовительные команды:",
            "Отдельно хочу обратить внимание на вторую команду. Стандартное поведение junos - при вводе части команды и пробела, система автоматически дополняет команду до полного названия. А в ответе от устройства в первой строке библиотека ожидает увидеть ту команду, которую она отправила на сервер. Если бы мы не отключили данную опцию, при вводе сокращенной команды, например sho ver вместо show version, отклик от устройства не совпадал бы с переданным запросом и выскочило бы исключение по таймауту. Но, важно помнить, что наше поведение спасает от сокращенного ввода команд, но не может спасти, например, от нескольких пробелов. Если в пример выше мы вызовем dev.send_command('show version'), то получим сообщение:",
            "Если писать команду целиком, то такое вряд ли случится, но при автоматическом формировании команды из нескольких частей вполне можно и пропустить два подряд пробела. А потом долго искать, что же не так.",
            "На этом, в части получения информации, можно было бы закончить. Смотрим на устройстве вывод конкретной команды, определяем, где в выводе находится интересующая нас информация, и выполняем команду с помощью send_command. А дальше в дело вступает самый обычный Python со всеми его возможностями по обработке текста, которые мы используем для разбора ответа от устройства. Но иногда удобнее воспользоваться дополнительными возможностями, которые предоставляет нам JunOS. К примеру, требуется получить значение какого-то конкретного параметра, а выбирать его из большой простыни вывода не очень удобно. Тогда мы можем попросить JunOS выдать ответ уже в структурированном виде, который и будем в дальнейшем разбирать. Можно получить вывод в формате XML, для чего просто добавить после команды | display xml. К примеру, если мы хотим получить список всех маршрутизаторов из протокола IS-IS, то можем сделать этот так (очевидно, что тут уже нам нужен будет маршрутизатор):",
            "А начиная с 15 версии JunOS так же есть возможность вывода в JSON добавив | display json.",
            "Для изменения конфигурации в библиотеке netmiko у объекта BaseConnection имеется метод send_config_set. Метод принимает одну или список команд конфигурации, проверяет, активен ли режим конфигурирования, если нет, заходит в него и выполняет переданные команды. Попробуем изменить, к примеру, описание одного из портов.",
            "На выходе получим следующее:",
            "Если теперь посмотреть описание этого порта на устройстве, то мы увидим что оно совершенно не изменилось",
            "В этом нет ничего удивительного, если мы посмотрим на сообщение, которое было выведено после вызова метода send_config_set. Там нам сразу напоминают, что The configuration has been changed but not committed. То есть изменения то мы в базу конфигурации внесли, но не выполнили commit и изменения не применились. Поэтому, при работе с juniper мы должны в метод send_config_set передать не только список команд, но и параметр exit_config_mode = False, чтобы автоматически не выходить из режима конфигурации и можно было бы применить изменения. Дополнительно можно через вызов команды show | compare посмотреть, какие же изменения мы собираемся применить. Вот работающий код",
            "На выходе получим",
            "Первая часть вывода показывает, какие изменения мы будем применять, вторая - результат выполнения команды commit. Сообщение commit complete показывает, что изменения успешно применились.",
            "В принципе, тут можно было бы и закончить с данной библиотекой, но хотелось бы чуть глубже копнуть особенности JunOS. Ранее я писал, а потом показывал на примере, что в JunOS (впрочем, как и у многих других вендоров) изменения в конфигурации не вступают в силу сразу, а требуют явного указания на их применение через команду commit. То есть в процессе конфигурирования можно даже удалить интерфейс, через который вы в данный момент подключены, если это повышает удобство конфигурирования (например, для выполнения команды copy interface). Пока вы не применили изменения, можно не волноваться. Но есть нюанс. Когда вы заходите в режим конфигурирования с помощью команды configure и начинаете вносить изменения, вы правите общую конфигурационную базу, а значит все изменения, которые вы внесли, доступны и для других пользователей.",
            "Например, кто-то из консоли решил поправить интерфейс ge-0/0/11 и ввел команду set interfaces ge-0/0/11 description TEST1. После чего на некоторое время задумался, что делать дальше. А в этот момент вы запускаете написанный ранее скрипт по изменению интерфейса ge-0/0/10. В выводе вы можете с удивление увидеть, что почему-то show | compare показывает, что вы меняете два интерфейса вместо одного.",
            "Хорошо, что команды тут безобидные, а если бы кто-то действительно удалил интерфейс, на котором наше устройство подключено - седых волос бы сильно прибавилось у многих. Чтобы избежать подобных неприятных моментов у JunOS есть специальные режимы конфигурирования - exclusive и private (есть еще, но это основные, на мой взгляд). В режиме exclusive никто, кроме нас, не может изменять базу настроек. Для использования данного режима изменим наш скрипт, добавив в вызов send_config_set параметр config_mode_command='configure exclusive'",
            "Второй рассматриваемый режим - private. В этом режиме вы работает с собственной копией конфигурационной базы, поэтому правки могут вносить несколько пользователей параллельно, не мешая друг другу. Код для этого режима не сильно отличается от предыдущего",
            "Для обоих режимов работы, в отличие от просто configure, все изменения будут сброшены после выхода. Кроме того, если кто-то параллельно уже внес какие-то изменения в общую базу (в режиме configure), то, что configure private, что configure exclusive, не смогут войти в режим конфигурирования, выдав в консоли error: shared configuration database modified, а в случае netmiko - выдав по таймауту исключение Pattern not detected: '(?s:Entering configuration mode.*\\\\].*#)' in output Еще одна интересная возможность JunOS - автоматический откат последних изменений. Если вы опасаетесь незапланированных последствий изменений, можете использовать commit confirmed. Данная команда применяет изменения и ожидает заданное количество времени (задается в минутах). Если в течение этого времени не совершен повторный commit, изменения откатываются до предыдущего состояния. Netmiko так же поддерживает данную команду. Просто добавьте параметры в вызов dev.commit(confirm = True, confirm_delay = 5) и через 5 минут, если вы повторно не вызовете commit(), все изменения будут отменены.",
            "В связи с огромной популярностью Python в части автоматизации работы с сетевым оборудованием, многие компании выпускают библиотеки для работы с их оборудованием. Не исключение и Juniper, который выпустил библиотеку PyEZ. Данная библиотека работает по протоколу NETCONF, который надо не забыть включить перед использованием. Для этого выполним команду set system services netconf ssh port 830 Перед использованием библиотеку необходимо установить pip install junos-eznc",
            "Ниже - простейший код подключения к коммутатору и вывод информации о нем.",
            "В результате будет выведена базовая информация об устройстве",
            "Основной класс для подключения - Device. При создании туда передается вся нужная информация, такая как адрес устройства, логин и пароль или ключ для входа. Кроме того, можно передавать дополнительные параметры, например gather_facts=False, если нам не нужно собирать информацию об устройстве при подключении.",
            "После того, как мы подключились к устройству можно начинать работать с ним. Метод работы \"в лоб\" - вызов метода dev.cli() с передачей в него команды, которую необходимо выполнить. Однако, данный метод не является рекомендованным, о чем нам будет сообщено в возвращаемом значении. Например, при выполнении print(dev.cli('show version')) мы получим следующее:",
            "Библиотека сама подсказывает, что основной метод работы с устройством - вызов rpc процедур. Узнать имя процедуры и название параметров можно двумя способами: вызвать команду прямо на коробке, указав | display xml rpc",
            "вызвать функцию cli_to_rpc_string которая и вернет правильное название функции и параметров",
            "Теперь, зная название функции и параметров, можем ее вызвать. Однако, вывод будет в формате xml, поэтому необходимо будет несколько дополнительных действий, чтобы отобразить результат",
            "Тут мы уже можем разобрать этот xml, ну или сразу получать только нужные нам значения. Вот, для примера, получение значения оптической мощности входящего сигнала из полученного ответа",
            "Примерно того же можно добиться, если включить фильтрацию при отправке запроса. Для этого надо передать параметр use_filter=True при создании подключения.",
            "Чтобы закрыть тему с вызовом методов надо еще добавить, что информацию можно получать не только в формате xml, но и в json, и в виде текста (правда тут тоже будет xml, но один общий тег output внутри которого будет текстовый вывод команды). Для этого при вызове методе передать нужный формат",
            "Небольшое замечание по поводу метода cli. Да, данный метод не рекомендуемый. Но иногда бывает, что другое решение найти трудно. К примеру, однажды мне надо было собирать информацию с резервного RE маршрутизатора. Обычно для этого заходишь на резервный RE командой request routing-engine login other-routing-engine и дальше смотришь, что нужно. Библиотека подсказала, что для этого есть метод request_login_to_other_routing_engine, который вроде как даже работал, но дальше новые вызовы выводили информацию все так же с основного RE. Сходу победить не получилось, поэтому пришлось решить задачу в лоб через cli.",
            "Но все же, чаще всего результатом запроса информации является некая таблица. Например, данных по интерфейсам или маршрутам. Основная полезность данной библиотеки - механизм получения таких данных. Можно, конечно, вызвать rpc.get_interface_information(media=True) и обрабатывать полученный вывод. Но есть метод проще.",
            "Импортируем нужную нам таблицу и запрашиваем данные. Данные приходят уже в готовом для дальнейшей работы виде.",
            "Вот пример простейшего вывода всех интерфейсов в виде таблицы",
            "В составе библиотеки уже имеются готовые определения для наиболее востребованных запросов. Вот тут содержится их полный перечень. Но это еще не все. Поскольку подготовленных определений много, но охватывают они все же далеко не все, мы можем самостоятельно определять нужные для нас таблицы и получать данные. Для примера, создадим таблицу для получения данных с оптических модулей. Описание таблиц производится с использованием YAML синтаксиса. Вот часть вывода команды show interfaces diagnostics optics | display xml, которая нам будет нужна для создания yml файла",
            "Создадим файл optic.yml с определением того, как мы будем получать данные из нашего вывода ```",
            "В файле мы описываем две сущности Table и View. Если попробовать объяснить по-простому, то Table описывает откуда мы вообще берем данные, какая именно часть этих данных будет использоваться для наполнения таблицы и что будет ключом. А View показывает, как мы будем из исходных данных формировать элементы таблицы. Возможно объяснение немного путанное, но, надеюсь, при разборе конкретного примера станет понятнее. Итак, начала мы описываем таблицу - OpticTable rpc - указываем имя процедуры, которая и является главным источником данных для заполнения таблицы. Имя этой процедуры мы можем получить разными способами, например вызвав команду и указав display xml rpc",
            "item - название элемента, из которого мы будем формировать строки нашей таблицы. В нашем случае данные для строки нашей таблицы содержатся в элементе <physical-interface>. Поскольку этот элемент находится прямо внутри основного элемента ответа <interface-information>, то мы указываем только его имя. Но если элемент является вложенным, надо будет указать весь путь. key - каждая строка нашей таблицы состоит из двух частей - ключа и данных. То, как будет сформирован ключ, указываем после ключевого слова key. Тут может быть как одно поле, так и несколько. В рассматриваемом случае это имя интерфейса, которое указано сразу в дочернем элементе нашего <physical-interface> view - то, что из полученных в результате вызова процедуры данных будет использовано для заполнения строки таблицы, указывается в отдельной сущности View. Название этой сущности тут мы и укажем.",
            "OpticView Тут в списке fields мы просто указываем, какие элементы необходимо выбрать, чтобы заполнить таблицу. Обратим внимание, что необходимые нам данные содержатся не напрямую в элементе <physical-interface>, который был указан в качестве item при описании таблицы, поэтому необходимо указать полный пусть до данного элемента. Так же мы можем брать не только значение внутри элемента, но и из его свойств и привести его к указанному типу данных, как продемонстрировано в строке temp: {optics-diagnostics/module-temperature/@celsius: float} Создав файл с описанием нужной нам таблицы, мы можем загрузить его встроенными средствами библиотеки и создать нужные нами классы:",
            "После этого можно будет использовать новые классы точно так же, как и встроенные:",
            "В результате получим требуемую информацию:",
            "Для настройки нашего устройства в библиотеке PyEZ имеется специальный класс Config",
            "Далее мы можем приступать к внесению изменений в конфигурацию устройства. Здесь работает стандартная схема Juniper: сначала изменения вносятся в базу конфигурации, но не применяются, для применения необходимо вызвать commit. Библиотека позволяет подходить к процессу очень гибко. Так, новые настройки можно вносить в 3 форматах:",
            "txt или conf - стандартный формат на базе фигурных скобок, который мы видим в консоли оборудования, если вывести show configuration",
            "xml - настройки в формате xml, пример такой конфигурации можно посмотреть с помощью show configuration | display xml",
            "set - форматирование в формате set. Фактически стандартный способ конфигурирования через консоль. Пример можно посмотреть с помощью show configuration | display set Изменения можно вносить как напрямую передав строку с настройками, так и загрузив данные из файла. Для примера создадим 3 файла в разных форматах и поменяем description на 3 разных интерфейсах config.txt",
            "config.xml",
            "config.set",
            "Далее, загружаем все три конфигурации. Для этого используется метод load. Если мы загружаем изменения из файла, формат определяется по расширению файла. Если же передаем в виде строки, то формат надо будет указать явно.",
            "В последней строке мы с помощью функции diff выводим все отличия новой конфигурации от текущей. В результате вывод будет такой:",
            "Обратим внимание на два момента. Мы пока не применили наши изменения и рабочая конфигурация осталась такой же, что и была. Но если мы закомментируем в нашем скрипте все строки с cfg.load, оставив только вызов diff, то, при повторном запуске, вывод будет точно таким же, как и выше. То есть, несмотря на то, что наш скрипт завершился, мы меняли общую базу конфигурации и изменения там так и остались. Чтобы начать вносить изменения заново, нужно откатить уже внесенные изменения. Это можно сделать через rollback()",
            "Мы откатили изменения к исходному состоянию, загрузили только одно изменение и вывели разницу. Результат, как и ожидалось",
            "Как я и писал ранее, в части про Netmiko, править общую базу бывает весьма опасно и есть два альтернативных подхода: режим private, где мы работаем с собственной копией базы конфигурации, и режим exclusive, когда мы блокируем общую базу и правим ее единолично. Для примера работы в режиме private запустим следующий код",
            "Мы дважды входим в режим конфигурирования и выводим разницу между рабочей конфигураций и новой версией Ожидаемо получим",
            "После выхода из режима конфигурирования и последующего входа изменения, внесенные в общую базу, остались. Теперь добавим mode='private'",
            "При первом запуске нас поджидает разочарование в виде вылетевшего исключения:",
            "Изменения в общей базе остались и зайти в режим private не получается. Откатим изменения вручную и повторно выполним код.",
            "Тут тоже все, как и ожидалось. После выхода из режима private все изменения обнулились. С режимом exlusive тоже все просто. Вот код",
            "Вывод",
            "Мы блокируем базу конфигурации с помощью lock(), вносим изменения. Но как только мы делаем unlock(), все изменения обнуляются.",
            "Еще одна приятная возможность загрузки файлов конфигураций - использование их как шаблонов, которые могут формироваться динамически. Для их формирования можно использовать синтаксис Jinja2. Для примера переделаем файл xml для правки сразу нескольких интерфейсов config.xml",
            "А сам скрипт",
            "В методе load() мы вместо параметра path указываем template_path, кроме того, в template_vars мы передаем все переменные, которые будут использоваться при обработке шаблона. В результате вывод будет следующий",
            "После того, как мы загрузили новые изменения в конфигурацию, нам остается сделать две вещи:",
            "Проверить, что изменения применимы (в консоли это была бы команда commit check)",
            "Применить изменения Первое мы выполняем при помощи метода commit_check, который возвращает True, если изменения могут быть применены, либо вызывает исключение, из которого можно получить информацию, что же именно в новом конфиге не понравилось. Ну и применение изменений производится при помощи метода commit(). Данный метод может принимать следующие параметры:",
            "Параметр",
            "Описание",
            "comment",
            "Текстовое описание коммита.",
            "confirm",
            "Возможность автоматического отката изменений. Указывается число минут до отката",
            "sync",
            "Булевское значение. Если True будет выполнена команда commit synchronize, которая синхронизирует изменения на обоих RE в конфигурациях, где есть два RE",
            "detail",
            "Булевское значение. Если True, the commit() выведет дополнительную информацию по выполнению. Используется для отладки",
            "force_sync",
            "Булевское значение. Если True, выполняет commit synchronize force.",
            "full",
            "Булевское значение. Если True все процессы будут принудительно перечитывать конфигурацию, даже если изменения их не затрагивают.",
            "Проверим на практике. Выведем описанием интерфейсов до изменений, загрузим и применим изменения и повторно посмотрим описания.",
            "Результат работы",
            "На этом я бы хотел завершить данную статью. Конечно, показанные тут подходы не единственные возможные для автоматизации Juniper, есть еще множество других возможностей, от простого вызова команды ssh и работы с вводом/выводом до использования еще более высокоуровневых библиотек, вроде Napalm, и систем управления конфигурациями, вроде Ansible. Если будет интерес, про это можно написать отдельные статьи. Буду рад комментариям и с удовольствием отвечу на вопросы читателей."
        ]
    },
    {
        "Название статьи": "Как в Tele2 автоматизировали тестирование SAP ERP с помощью Python",
        "Дата публикации": "2024-06-05, 13:34",
        "Автор статьи": "a_valeeva ",
        "Статья целиком": [
            "Привет, Хабр! Меня зовут Анастасия Валеева, я – руководитель группы обеспечения качества в Tele2. Наша команда работает в большинстве своём с SAP ERP, и мы не понаслышке знаем, что автоматизация данной платформы — дело далеко не тривиальное. В этой статье я хочу поделиться с вами, как и зачем мы автоматизировали тестирование с помощью Python.",
            "Зачем мы это придумали",
            "SAP ERP – гибкий инструмент в руках нашей команды. Мы дорабатываем функциональность системы под потребности конкретного бизнес сегмента. Эти изменения производятся по запросу бизнес-пользователей. Объём и влияние доработок могут быть различными, но одно остаётся неизменным – каждая доработка является уникальной. Таким образом, это не простое устранение багов и улучшения текущего функционала, не изменение версионности продукта после оптимизации, а, как правило, абсолютно новый «продукт» в системе. В случае автоматизации функционального тестирования нам потребуется писать автотест на каждую доработку/разработку, что занимает больше времени, чем ручное тестирование (написание автотеста, отладка, оптимизация) + данный автотест с каждой новой разработкой будет уже неактуален, и нужно будет создавать новые и новые из раза в раз. Делаем выводы, что автоматизировать функциональные тексты для нас нерелевантно. А вот регрессионные тесты, которые мы проводим после каждого изменения системы, представляют собой более шаблонные варианты, шаги повторяются, и от их автоматизации есть профит.",
            "Сейчас мы работаем с SAP ERP и интегрированными продуктами (FileNet, BW, Fiori), однако, импортозамещение идёт полным ходом, и мы проводим пилотный проект по миграции на новую платформу. Так или иначе, созданный нами инструмент для автотестов универсален и может быть применён в работе с новой системой.",
            "Как выбирали инструмент автоматизации",
            "Из множества инструментов автоматизации мы выбрали для ознакомления четыре наиболее совместимых с SAP ERP:",
            "SAP Scripting;",
            "Tricentis Tosca;",
            "eCatt;",
            "CBTA.",
            "Анализируя, мы исходили из трёх основных для нас факторов: скорость освоения, простота и гибкость, а также бюджет. По каждому из инструментов мы отметили свои плюсы и минусы, собрали информацию в единую таблицу. И вот что у нас получилось.",
            "По количеству зелёных блоков мы увидели, что нашим критериям в большей степени соответствует SAP Scripting.",
            "Принцип работы данного инструмента состоит в том, что он записывает все действия пользователя в системе, на выходе формирует файл в формате .vbs, который в последующем можно запускать в SAP. Соответственно, при запуске этого файла система будет повторять ваши предварительно записанные шаги. Кроме того, данный файл можно корректировать: удалять лишнее, дописывать недостающее или даже полностью переписать. Для этого необходимо открыть файл либо в блокноте, либо в любом другом редакторе, работающем с кодом.",
            "В процессе пилотирования SAP Scripting помимо технических вопросов мы решали несколько административных задач: удобство использования, гибкость, кастомизация, универсальность, прозрачность.",
            "Мы хотели внедрить такой инструмент, который будет полезен не только группе тестирования, но и другим смежным группам нашего подразделения. И поскольку мы говорим об автоматизации, одним из основополагающих факторов для нас было минимальное участие человека в этом процессе. Согласитесь, часто хочется просто нажать на волшебную кнопку \"РАБОТАТЬ\", чтобы оно всё само заработало :)",
            "Добиться данного магического эффекта «работает само» нам помог Python. За это отвечала коллега из моей команды — она написала скрипт для робота, который сейчас работает буквально по одному клику.",
            "Что касается прозрачности, то мы пошли по пути, доступному для любого пользователя. Для этого «прикрутили» Python к файлу Excel. Это означает, что сейчас провести регресс может любой сотрудник — достаточно зайти в файл автотеста и нажать кнопку «СТАРТ».",
            "Бизнес-процесс состоит из набора бизнес-операций. Например, создание логистического заказа состоит из заведения заказа, смены статуса подписания договора, деблокирования заказа и создания счёта-фактуры. Для обеспечения полного регрессионного тестирования мы автоматизируем всю цепочку шагов. На выходе получаем Excel-документ со скриншотами и подробной информацией по каждому шагу тестирования. Причём регресс может запустить любой пользователь, не только тестировщик, это доступно в том числе для менеджеров со стороны бизнеса. А полученные данные (скрипты) можно использовать также для генерации тестовых данных.",
            "Существует несколько способов выполнения автотестов.",
            "1. Отдельно по каждому бизнес-процессу. По каждому модулю финансовой системы SAP ERP создан файл Excel, в котором есть кнопка вызова макроса. По вызову этой кнопки запускается Visual Basic for Applications. VBA обращается к системе SAP и вызывает на выполнение ранее записанный скрипт vbs.Таким образом, мы можем выполнять тестирование по отдельному модулю или бизнес-операции.",
            "2. По всему модулю или нескольким модулям.Для этих нужд как раз используется Python. Наш робот обращается к SAP, открывая рабочее окно. Далее вызывает необходимые файлы Excel, которые работают по описанному принципу макросов на VBA. Таким образом, мы получаем следующую цепочку:",
            "При этом пользователю необходимо только единожды нажать кнопку ВЫПОЛНИТЬ.",
            "Запуск SAP GUI",
            "Заведение функции для чтения файла Excel",
            "Подключение к Excel",
            "На каждом листе в Excel есть подробная входная и выходная информация, при этом входную информацию можно корректировать. Большая часть листов связана между собой, чтобы можно было провести всю цепочку на одних данных, а последующие шаги не зависели от дополнительных действий пользователя.",
            "Все скриншоты, которые создаются в процессе регресса, генерируются вместе с документами и проводками. Лишние скриншоты можно удалить прямо на странице в Excel. При необходимости сотрудник может по номеру документа найти нужную проводку или операцию в SAP. Это является прозрачным и удобным способом анализа логов тестирования.",
            "Рядом с каждым шагом в файле появляется текстовое описание, статус «успешно» или «не успешно» пройден шаг и цветовой индикатор — зеленый означает успешно пройденный этап, красный сигнализирует об ошибках.",
            "Если ошибка является блокирующей для системы и дальнейшее прохождение шагов невозможно, то скрипт остановится, выдаст информационное сообщение и сохранит изменения в файл. Если ошибка не влияет на последующие шаги, то скрипт продолжит работу, а в конце выдаст лог в Excel с отображением корректных и некорректных шагов. При таком раскладе у нас появляется возможность увидеть проблему в моменте и исправить её.",
            "Также, завершение работы скрипта сопровождается звуковым оповещением.",
            "Дополнительно мы настроили автоматическое удаление листов из общей папки через три дня после их создания.",
            "Что в итоге",
            "Мы посчитали, сколько рабочего времени ручных тестировщиков мы экономим при использовании инструмента автоматизации. Получилось, что на один кейс при использовании SAP Scripting мы тратим 31 секунду против 148 секунд при ручном тестировании. Таким образом, 80% времени инженеров высвободилось на другие задачи, и мы смогли повысить эффективность тестирования.",
            "Данный вариант автоматизации является гибким к изменениям. В случае переезда на другую финансовую систему мы перенаправим нашего робота на Python на вызов нужной нам программы. Сейчас одна из наших основных задач – обеспечить качество работы текущего функционала и уже на этой надёжной основе реализовывать улучшения и внедрять новые фичи. Для нашей команды автоматизация тестирования SAP ERP стала интересным и полезным опытом, а бизнесу предоставила доступную, понятную и безотказную систему проверки рабочих процессов."
        ]
    },
    {
        "Название статьи": "Быстрый интерфейс, быстрый деплой",
        "Дата публикации": "2024-06-05, 11:01",
        "Автор статьи": "funtastick ",
        "Статья целиком": [
            "Салют! Не так давно создатели знаменитого pydantic выпустили новый фреймворк — FastUI, который позволяет создавать пользовательские интерфейсы с помощью декларативного кода на Python. В этой статье рассмотрим создание простого приложения и деплой его в Cloud Apps. ❯ Обзор По заявлению авторов фреймворка, фронтенду не нужно (и не следует) знать ничего о приложении, которое вы создаете, вместо этого он должен просто предоставить все компоненты, необходимые для создания интерфейса, а затем бэкенд может предоставить необходимые данные и параметры компонентов. Реализовано это таким образом, что FastUI инкапсулирует описание компонентов интерфейса и в виде классов, затем запускается простое React приложение, которое обращается к эндпоинтам за данными и компонентами. ❯ Пример Для примера давайте напишем простое приложение, предоставляющее информацию о городах из списка с возможностью пагинации. Данные для экспериментов любезно предоставили создатели фреймворка. Для начала опишем pydantic модель и функцию для чтения данных. from pydantic import BaseModel, Field, TypeAdapter import json from pathlib import Path class City(BaseModel): id: int city: str = Field(title=\"Name\") city_ascii: str = Field(title=\"City Ascii\") lat: float = Field(title=\"Latitude\") lng: float = Field(title=\"Longitude\") country: str = Field(title=\"Country\") iso2: str = Field(title=\"ISO2\") iso3: str = Field(title=\"ISO3\") admin_name: str = Field(title=\"Admin Name\") capital: str = Field(title=\"Capital\") population: float = Field(title=\"Population\") def cities_list() -> list[City]: cities_file = Path(__file__).parent / \"cities.json\" with open(cities_file, \"r\", encoding=\"utf-8\") as f: data = json.load(f) cities = [City(**city) for city in data] return cities Далее напишем каркас для нашего примера, с помощью FastAPI. Опишем два роута, первый возвращает необходимые компоненты и данные, а второй — простое React приложение, которое отвечает за запрос и отображение компонентов, полученных из предыдущего. from fastapi import FastAPI from fastapi.responses import HTMLResponse from fastui import AnyComponent, FastUI from fastui import components as c, prebuilt_html from fastui.components.display import DisplayLookup, DisplayMode from fastui.events import BackEvent, GoToEvent app = FastAPI() @app.get(\"/api/cities\", response_model=FastUI, response_model_exclude_none=True) def cities_view(page: int = 1, country=None): cities = cities_list() page_size = 10 # Количество записей в таблице, отображаемых на странице filter_form_initial = {} return c.Page( # Page - базовый контейнер для остальных компонентов components=[ c.Table( # Table - базовая разметка таблицы data=cities[(page - 1) * page_size : page * page_size], #Создаём срез данных для заполнения таблицы data_model=City, #Передаём модель данных columns=[ # Описываем столбцы таблицы DisplayLookup( #Указываем содержимое и размер столбца в процентах field=\"city\", table_width_percent=33 ), DisplayLookup(field=\"country\", table_width_percent=33), DisplayLookup(field=\"population\", table_width_percent=33), ], ), c.Pagination(page=page, page_size=page_size, total=len(cities)), #Кнопки для пагинации ] ) @app.get(\"/{path:path}\") async def html_landing() -> HTMLResponse: \"\"\"Простое React приложение, идёт последним, т.к. соответствует всем маршрутам\"\"\" return HTMLResponse(prebuilt_html(title=\"Большие города\")) Результат работы представлен на рисунке ниже: ❯ Деплой Для деплоя приложений на FastUI можно воспользоваться сервисом Apps, к сожалению рассмотренный фреймворк только набирает популярность, поэтому мы воспользуемся опцией: «деплой из Dockerfile». Для этого достаточно создать Dockerfile и разместить его в корне репозитория. FROM python:3.11 COPY . /app WORKDIR /app RUN pip install -r requirements.txt CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] EXPOSE 8000 Обратите внимание, что при отсутствии в Dockerfile параметра EXPOSE, APPS будет слушать порт 8080 контейнера. Далее достаточно предоставить сервису доступ к аккаунту на github. Затем остаётся следить за логами деплоя: В случае успешного развёртывания появиться удобный дашборд с графиками нагрузки на виртуальную машину: ❯ Заключение В данной статье мы рассмотрели лишь малую часть возможностей фреймворка, однако можно отметить, что FastUI предоставляет новый подход к созданию веб-приложений и позволяет существенно ускорить разработку. Возможно, захочется почитать и это: ➤ Timeweb Cloud CLI ➤ Бесплатный прокси к Docker Hub ➤ Фантастически быстрый деплой веб-приложения ➤ Учимся летать: симуляция эволюции на Rust ➤ Age of Empires – культовая попытка сделать Цивилизацию в реал-тайме Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩"
        ]
    },
    {
        "Название статьи": "Как я создавал аудиоплеер на python с FFmpeg",
        "Дата публикации": "2024-06-04, 18:10",
        "Автор статьи": "Niamorro ",
        "Статья целиком": [
            "Всех приветствую. Сегодня хочу поделиться опытом создания своего первого проекта на Python. Мой проект — это простой аудиоплеер, и я хочу рассказать, как я его создавал, с какими сложностями столкнулся и что из этого вышло.",
            "Выбор языка для первого проекта — это всегда непросто. Я выбрал Python по нескольким причинам:",
            "Простота синтаксиса. Python очень читабельный и понятный, что идеально подходит для новичков.",
            "Богатая стандартная библиотека и сообщество. Множество готовых решений и библиотек, которые можно использовать в своих проектах.",
            "Популярность в разработке. Python — один из самых популярных языков программирования, и навыки работы с ним будут полезны в будущем.",
            "Моя цель была написать простой аудиоплеер, который мог бы играть основные аудиоформаты. Я хотел, чтобы пользователь мог выбирать треки, ставить их на паузу и останавливать, так же изменять скорость проигрывания.",
            "Выбор библиотек занял действительно много времени, так как нужно было выбрать библиотеки которые обновляются, и имеют необходимый мне функционал. Я использовал несколько библиотек:",
            "PySide6: библиотека для создания интерфейсов созданная разработчиками Qt, имеет хорошую поддержку сообщества и регулярные обновления, в дополнение к ней использовал qdarktheme для стилизации интерфейса.",
            "FFmpeg: Универсальный инструмент для обработки видео и аудио.",
            "Sounddevice: Библиотека для воспроизведения и записи звука в Python.",
            "Mutagen: Библиотека для извлечения данных из аудиофайлов.",
            "Выбор файла:",
            "Пользователь выбирает аудиофайл из меню \"Файл\". Поддерживаемые форматы включают MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Выбранный файл передаётся в FFmpeg через подпроцесс для извлечения необработанных аудиоданных. Используемая команда:",
            "Чтение аудиоданных:Аудиоданные считываются блоками и сохраняются в массив NumPy для эффективной обработки.",
            "Регулировка громкости:Регулировка громкости осуществляется путём умножения аудиомассива на коэффициент громкости.",
            "Регулировка скорости воспроизведения:Скорость воспроизведения (например, 2x) управляется через библиотеку sounddevice путём изменения частоты дискретизации.",
            "Поток вывода:Обработанные аудиоданные передаются на аудиовыход через библиотеку sounddevice.",
            "Управление воспроизведением:Элементы управления, такие как воспроизведение/пауза, следующий/предыдущий трек и перемотка, обрабатываются через класс AudioTrigger.",
            "Воспроизведение/Пауза:Использует класс AudioTrigger для начала/остановки аудиопотока.",
            "Следующий/Предыдущий трек:Обновляет текущий индекс трека и загружает следующий/предыдущий трек в плейлисте.",
            "Перемотка:Регулирует позицию воспроизведения, пересчитывая индекс позиции на основе значения ползунка.",
            "Виджет очереди треков:Отображает добавленные ранее папки.",
            "Виджет плейлиста:Отображает содержимое папки.",
            "Виджет информации о треке:Показывает метаданные и обложку для воспроизводимого трека.",
            "Если хотите ознакомиться с исходным кодом или внести свой вклад в проект,",
            "приглашаю вас посетить страницу GitHub проекта. Там вы найдёте весь исходный код аудиоплеера.",
            "Также у проекта есть веб-сайт, где вы можете скачать готовые .exe и .deb пакеты для Windows и Linux. Здесь же доступна подробная документация по установке и использованию программы.",
            "Работа с FFmpeg требовала правильной организации буферизации аудиоданных, чтобы избежать прерываний и задержек при воспроизведении.",
            "Решение: Буферизация данных в массив NumPy.",
            "Треки воспроизводились с неправильной скоростью из-за некорректной частоты дискретизации.",
            "Решение: Я считываю частоту дискретизации трека и открываю аудиопоток с настройками именно для того трека, который в данный момент должен воспроизводиться.",
            "В результате я создал аудиоплеер с основными функциональными возможностями:",
            "Проигрывание аудиофайлов: Поддерживаются популярные форматы MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Управление воспроизведением: Воспроизведение, пауза, остановка, перемотка, следующий/предыдущий трек.",
            "Регулировка скорости воспроизведения: Возможность воспроизводить треки быстрее или медленнее.",
            "Плейлист: Добавление папок с треками.",
            "Информация о треках: Отображение метаданных и обложек альбомов.",
            "Тёмная тема: Благодаря qdarktheme, аудиоплеер имеет современный и стильный интерфейс.",
            "Добавить поддержку потокового аудио: Возможность воспроизводить музыку из интернет-радиостанций и стриминговых сервисов.",
            "Расширить функциональность плейлиста: Добавить возможность создания и сохранения пользовательских плейлистов.",
            "Поддержка эквалайзера: Добавить эквалайзер для настройки звука.",
            "Создание аудиоплеера на Python оказалось полезным опытом. Я научился работать с аудио на низком уровне, обрабатывать потоки и создавать пользовательский интерфейс.",
            "Буду рад любым отзывам и предложениям по улучшению плеера. Спасибо за внимание!",
            "Ссылки:",
            "Исходный код на GitHub",
            "Сайт проекта с загрузкой пакетов"
        ]
    },
    {
        "Название статьи": "Как мониторинг связан с тестированием. Преимущества мониторинга для бизнеса: как экономить время и деньги",
        "Дата публикации": "2024-06-04, 15:59",
        "Автор статьи": "luffity ",
        "Статья целиком": [
            "Привет! Проходя множество собеседований, я не раз слышал вопросы по типу: «Что такое мониторинг?», «Как это связано с тестированием?», «Зачем это нужно?». Для меня, волей случая ставшего специалистом по мониторингу чуть больше года назад, это тривиальные вопросы, однако многие компании либо не знают, что это такое, либо не видят в этом пользы. На одном из последних интервью я услышал интересное мнение от QA Lead о том, что assert должен быть в каждом тесте. Смелое заявление, подумал я. Поэтому, собственно, вы и читаете эту статью.Разберёмся, что такое мониторинг и с чем его едят. А главное, зачем он нужен вообще.",
            "Думаю, начать с небольшого введения обо мне будет наиболее верным для погружения в тему. Сейчас я занимаю должность middle SDET в ООО «МИТ» (если проще, то DIXY Group). Попал я туда как AQA, причём единственным. Я находился в группе по мониторингу корпоративных сервисов, соответственно, кроме меня там были только специалист по мониторингу (мой Lead), DevOps и системный админ. Похоже на стандартное начало анекдота…",
            "Моим заданием на испытательный срок стало написание сервиса по мониторингу интернет-соединения на торговых точках компании. И тут я подумал: какой, блин, мониторинг? С другой стороны, работа на дороге не валяется, тем более настолько приятная. Дело было вечером, делать было нечего…",
            "По итогу этого задания ко мне пришло осознание того, как тесты могут трансформироваться в нечто большее, чем проверки, прогоняемые раз в релиз. Они могут быть целой экосистемой, даже можно сказать: «Глазом Бога» (кто понял отсылку к Форсажу, спасибо).",
            "Начнём мы, конечно же, с поверхности и определим для себя, что такое мониторинг и как он связан с тестированием. В самом распространённом смысле под тестированием понимают сверку ожидаемого и фактического результатов на конечном наборе тестов. Если мы берём стандартные парадигмы тестирования, то такие проверки могут выполняться при добавлении новой фичи, раз в спринт, при релизе и много когда ещё. Однако все эти тестовые прогоны не преследуют цель отслеживать состояние продукта (программного или бизнесового) постоянно.",
            "Как раз здесь и вступает в игру мониторинг. Если о-о-очень грубо сказать, то это, по сути, те же наборы тестов, только несущие цель постоянного наблюдения за состоянием продукта. Но тут стоит уточнить, что это не тестовые кейсы в их привычном понимании.",
            "Думаю, будет проще понять наглядно.",
            "Это всем понятный простой тест. Мы переходим на страницу и ожидаем заголовок. Как можно заметить, для того чтобы исполнить свою главную функцию – сопоставить ожидаемый и фактический результаты, тест содержит в себе assert. Это несомненно верный подход к написанию тестов, так как это позволит более точно валидировать ошибку, а также правильно отобразить её в отчётах, например Allure.",
            "А теперь взглянем на код скрипта мониторинга, который проверяет доступность ресурса.",
            "Сразу бросается в глаза отсутствие assert. Но в таком случае, как такой скрипт вообще можно считать информативным, если он не выводит ошибки? Именно поэтому мы добавим дополнительное действие. Например, найдём какую-то кнопку и нажмём на неё. Теперь, если ресурс не прогрузился или сломался, мы получим TimeoutException и сообщение о том, что именно скрипт не смог сделать.",
            "Возникает вопрос: почему бы тогда точно так же не поставить assert и не ждать лишнее время для выпадения TimeoutException ? Справедливо! Однако возьмём во внимание, что данный скрипт не нацелен на то, чтобы просто проверить доступность ресурса и отследить ошибку в отчёте. Если мы предполагаем, что скрипт гоняется бесконечно, пока смерть сервера не разлучит вас, то отчётом в данном случае будет не Allure, например (хотя я и его прикрутил к скриптам для Project Manager’а), а сервисы для графического отображения типа Grafana или сервисы мониторинга типа Prometheus. Да и сам скрипт, помимо успеха или провала теста, должен собирать ещё кучу полезных данных. В данном примере это может быть время прохождения скрипта, что может дать нам представление о том, в каком состоянии находится сервис. Особенно если учесть, что всегда можно настроить параметры интернет-соединения или любые другие моменты, имитирующие пользователя. И тут мы плавно перейдём к другому вопросу.",
            "Теперь стоит сказать и о том, что мониторинг может быть как на микро-, так и на макроуровне. Под микроуровнем обычно понимают низкоуровневый мониторинг, например, физического оборудования, элемента большого сервиса или что-то подобное. На макроуровне мониторинг предстает как UX-тестирование или тестирование пользовательских путей.",
            "Немного про микроуровень. Вернёмся к проекту по мониторингу интернет-соединения на торговых точках компании. По сути, скрипт достаёт из базы данных ID магазинов, конвертирует их в IP-адреса маршрутизаторов в магазинах и пингует их несколько раз. Помимо того, что в таблице в Grafana этот скрипт отображает «Up» или «Down» в зависимости от доступности каналов, он также собирает время отклика, хранит историю падений и содержит в себе данные об операторе SIM-карты, номере телефона и многое другое. Не очень-то похоже на тест.",
            "Теперь про макроуровень. Высокоуровневый мониторинг уже больше похож на UI/UX-тестирование. В его основе лежит постоянное отслеживание пользовательского пути через UI. Например, для сайта доставки продуктов — от захода пользователя на сайт и выбора товаров до оплаты. Помимо прочего, такой скрипт также собирает множество данных.",
            "В чём, собственно, разница? Основными критериями, отличающими мониторинг от тестирования, являются:",
            "Цель в постоянном наблюдении. Мониторинг — большой брат для ваших сервисов, который безустанно следит за ними;",
            "Сбор данных. Помимо отчётов о тестировании, мониторинг собирает ещё кучу данных;",
            "Быстрое реагирование. Думаю, тут и пояснять не надо. Если у вас есть тестовый сервер или синтетика, то критической баге будет сложно пролезть в прод;",
            "Имитация пользователя. Хоть UX-тесты и тесты пользовательских путей позволяют имитировать действия пользователя, но пишут их далеко не в первую очередь (информация со 100+ собеседований. Всем API подавай, а на пользователей мы кладём...).",
            "Что в итоге польза? Ну, тут я расскажу лучше пару «До и После» примеров.",
            "До разработки мною сервиса мониторинга интернет-соединения на валидацию проблем и выезд оперативной группы на точку уходило два-три рабочих дня. Более того, часто это были ложные вызовы, так как при неработающем основном канале включался резервный. Мониторинг позволил проходить весь процесс за два часа. Процент ложных вызовов за год его работы составляет не более 0,2%. А уж сколько денег это экономит компании, говорить не приходится, если учитывать, что к этому мониторингу подключена вся первая линия поддержки. Во всех магазинах Дикси. По всей России. Даже не думал, что час простоя торговой точки может стоить так много…",
            "А как вам такая новость в ленте: «Основной сайт и сайт доставки магазина Дикси не работают!»? Именно такую новость прочло руководство компании, заваривая утренний кофе. Да, узнавать о падении основных сервисов компании из новостей — это, видимо, не весело. Мне кажется, кофе точно не полезет после такого. Стоит ли говорить, что после этого случая мониторинг был внедрён на все сервисы?",
            "Забавно, правда?",
            "Но остался ещё один вопрос. Специалист по мониторингу и специалист по тестированию — это один и тот же профессионал? Мне кажется, специалист по мониторингу ближе к SDET, чем к AQA. Всё-таки я считаю, что автоматизатор тестирования должен знать и уметь меньше. AQA как бы и должен иметь представление о контейнеризации, но как бы и просто собрать контейнер в Docker достаточно. Специалист по мониторингу должен бы и под каждый свой мониторинг собрать контейнер, и доставить его, и обслужить если что, и k8s знать бы по-хорошему, ноды и воркеры – лучшие друзья. И опять-таки, ты же не знаешь, что может быть важно для бизнеса. Возможно, придётся выйти за рамки PyTest, Selenium и Appium. Уметь разобраться в различных библиотеках, знать асинхронные подходы, парадигмы проектирования, сильные и слабые стороны рабочего языка программирования — всё это важные навыки специалиста по мониторингу. Так что да, SDET более подходящее описание для его деятельности.",
            "Ссылочка на телегу"
        ]
    },
    {
        "Название статьи": "Кратко про Seq2Seq-модели",
        "Дата публикации": "2024-06-04, 09:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Seq2Seq модели — это архитектуры ML, предназначенные для задач, связанных с последовательными данными, типо машинного перевода, суммирования текста, создания описаний к пикчам и прочие задачи, где требуется преобразование одной последовательности в другую.",
            "В этой статье в общих деталях рассмотрим то, как реализуются Seq2Seq модели.",
            "Seq2Seq модели состоят из двух основных частей: энкодера и декодера.",
            "Энкодер преобразует входную последовательность в контекстный вектор, содержащий обобщённое представление всей входной информации. Этот вектор затем используется декодером для генерации выходной последовательности, о декодере чуть ниже.",
            "Перед тем, как подать данные в энкодер, текстовые данные преобразуются в числовые представления с помощью эмбеддинга. Это делается с помощью слоя Embedding, который преобразует каждый токен во входной последовательности в вектор фиксированной размерности. Например, слово milk может быть представлено как вектор размерности 300.",
            "Основу энкодера RNN, обычно реализованные с использованием LSTM или GRU. Эти сети обрабатывают входную последовательность пошагово:",
            "На каждом шаге RNN принимает эмбеддинговое представление текущего токена и скрытое состояние от предыдущего шага.",
            "Выход каждого шага включает новое скрытое состояние, которое передаётся на следующий шаг вместе со следующим токеном.",
            "В конце последовательности RNN генерирует контекстный вектор, который является финальным скрытым состоянием. Этот вектор обобщает всю информацию из входной последовательности и передаётся в декодер для дальнейшей генерации выходной последовательности. Контекстный вектор — это своего рода сжатая версия входной последовательности, включающая в себя её смысл.",
            "Для улучшения качества представления входной последовательности часто используются двунаправленные RNN. В этом случае два RNN работают параллельно: один — слева направо, другой — справа налево. Их состояния объединяются на каждом шаге, что позволяет учитывать как предшествующие, так и последующие слова для каждого токена в последовательности.",
            "Пример реализации энкодера на Keras с LSTM:",
            "Здесь входные данные сначала проходят через эмбеддинговый слой, который преобразует их в векторы фиксированной размерности. Затем эти векторы подаются в LSTM, который на выходе даёт финальные скрытые состояния, использующиеся в качестве контекстного вектора.",
            "В отличие от энкодера, декодер генерирует данные на основе предыдущих предсказаний и контекстного вектора, предоставленного энкодером.",
            "Подобно энкодеру, декодер принимает токены, которые сначала преобразуются в числовые представления с помощью эмбеддинга. Однако, в случае декодера на вход подаются не только реальные данные, но и предсказанные токены на предыдущих шагах.",
            "Декодер также реализован с использованием RNN, как LSTM или GRU. На каждом шаге декодер принимает:",
            "Контекстный вектор от энкодера.",
            "Предыдущий предсказанный токен (или начальный токен для первого шага).",
            "Скрытое состояние от предыдущего шага декодера. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности.",
            "На каждом шаге RNN декодера производит новое скрытое состояние и выходной вектор. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности",
            "Для улучшения качества генерации используется механизм внимания, который позволяет декодеру фокусироваться на различных частях входной последовательности на каждом шаге генерации выходной последовательности. Механизм внимания вычисляет веса для каждого состояния энкодера, определяя важность каждого токена входной последовательности в текущий момент времени.",
            "Пример реализации декодера на Keras с LSTM и механизмом внимания:",
            "Декодер принимает начальные состояния от энкодера и генерирует выходную последовательность.",
            "Машинный перевод — это одна из наиболее базовых задач для Seq2Seq моделей. Реализуем Seq2Seq модельку для перевода с английского на французский язык с использованием Keras.",
            "Для этой задачи будем использовать датасеты французских и английских предложений (они есть на kaggle):",
            "Следующий пример - текстовое суммирование. Это задача генерации краткого представления текста. Реализуем Seq2Seq модель с использованием механизма внимания.",
            "Для этой задачи будем использовать датасет новостей, где заголовок является суммарным представлением статьи:",
            "Реализуем генерацию описаний к изображениям — это задача, где Seq2Seq модели используются для генерации текста, описывающего содержание изображения. Будем использовать предобученную модель InceptionV3 для экстракции признаков изображения и Seq2Seq модельку для генерации текста:",
            "Seq2Seq модели - это очень мощный инструмент для решения задач, связанных с последовательными данными. Они позволяют преобразовывать входные последовательности в выходные с высокой точностью, в особенности при использовании механизмов внимания (об этом не забываем).",
            "В завершение хочу порекомендовать бесплатные вебинары курса ML Advanced:",
            "Современные модели прогнозирования типа TimesNet и TimeGPT",
            "H2O, TPOT, Autokeras - а вы что, за меня и модели строить будете?"
        ]
    },
    {
        "Название статьи": "Как подружить Llama-3 и YouTube имея всего 40 строк кода",
        "Дата публикации": "2024-06-03, 21:57",
        "Автор статьи": "evg_dc ",
        "Статья целиком": [
            "Сделаем Телеграм бота которому можно кинуть ссылку на YouTube видео и поговорить с ним о содержимом этого видео.",
            "За основу возьмем бота работающего на Llama 3-70b из моей прошлой статьи. Можно использовать и любую другую языковую модель включая ChatGPT или локальную запущенную на Ollama.",
            "Создать Телеграм бота и получить его токен (как это сделать, смотрите инструкции на просторах интернета, например здесь).",
            "Зарегистрироваться на Groq и получить api key (нужен VPN).Заходим по этой ссылке, регистрируем аккаунт, генерим ключ. Платежная карта не требуется.",
            "Замените в коде GROQ_API_KEY на api ключ полученный в Groq и TELEGRAM_BOT_TOKEN на токен телеграм бота, все должно быть в кавычках.",
            "После получения сообщения от пользователя ищем в тексте сообщения ссылку на YouTube видео. Делаем это перебирая все слова и проверяя их на наличие URL. Если ссылка на видео найдена, используя библиотеку youtube-transcript-api забираем транскрипцию.",
            "Далее, полученную транскрипцию подставляем языковой модели в виде сообщения от функции. Здесь мы немного обманываем модель, потому что такой функции нет, но лучше делать так чем ставить транскрипцию в системное сообщение. Модель заточена под работу с функциями и все правильно поймет.",
            "Как и в предыдущей версии, бот будет запоминать последние 6 сообщений и поддерживать диалог.",
            "Запускаем скрипт и в Телеграм боте задаем вопрос со ссылкой на видео:",
            "Посмотреть как это работает можно в YouTubeGPT.",
            "Еще есть НашGPT - это как ChatGPT только модель Llama 3-70b."
        ]
    },
    {
        "Название статьи": "Python в Excel жив?",
        "Дата публикации": "2024-06-03, 17:24",
        "Автор статьи": "Gonchar_POTT ",
        "Статья целиком": [
            "Уже больше месяца экспериментирую, исследую, как разные схемы (паттерны) осознанного дыхания влияют на вариабельность сердечного ритма (Heart Rate Variability, HRV на чуждом языке). В скромной, но не совсем уж крошечной Excel-таблице со столбцами “Паттерн”, “HRV”, “Пульс” набралось 258 записей и мне понадобилось выбрать победителя -- дыхательный паттерн, дающий на выходе максимальное значение HRV. Не вручную же сортировать эти записи!",
            "Да, я знаю: есть Pivot Table & Power Query. Но Pivot Table мне не по душе необходимостью после каждого изменения таблицы-источника делать REFRESH, во-первых, избыточной сложностью создания, во-вторых. И просто так не нравятся Pivot Table, что главное. Что же касается Power Query, то сочетание слов вызывает у меня трепет и учащенное сердцебиение: не понимаю, что это за зверь такой и насколько он страшный или полезный.",
            "Поэтому для подсчета результатов -- и выбора победителя -- применил относительно недавно появившуюся в Excel функцию GROUPBY в связке с XLOOKUP. И, раз уж пришлось группировать данные, решил сравнить нативные функции Excel с GROUPBY от Pandas (мы ведь помним, что сейчас Python можно запустить внутри Excel).",
            "Написал простой код:",
            "Поместил код через =PY( в ячейку А1 Excel",
            "И он прекрасно справился с задачей и выдал таблицу с результатами:",
            "breathing_pattern",
            "HRV",
            "HR",
            "8",
            "physiological sighs moderate",
            "59",
            "65",
            "7",
            "physiological sighs light",
            "57",
            "62",
            "1",
            "4.4-6.6",
            "56",
            "59",
            "6",
            "following pulse",
            "55",
            "61",
            "0",
            "4.2-0-6.4-0",
            "53",
            "62",
            "3",
            "6-6",
            "53",
            "61",
            "4",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "61",
            "2",
            "5-5",
            "52",
            "63",
            "5",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "Комбинация функций GROUPBY и XLOOKUP тоже отработала без изъянов:",
            "breathing_pattern",
            "HRV",
            "HR",
            "physiological sighs moderate",
            "59",
            "65",
            "physiological sighs light",
            "57",
            "63",
            "4.4-6.6",
            "56",
            "60",
            "following pulse",
            "55",
            "61",
            "4.2-0-6.4-0",
            "54",
            "63",
            "6-6",
            "54",
            "61",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "62",
            "5-5",
            "53",
            "63",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "* Для внимательных: разница в данных между двумя таблицами -- плод Python-овского округления до целых чисел.",
            "* В “нативном” подходе нет отсечки паттернов с количеством замеров менее шести.",
            "Выводы и наблюдения по теме:",
            "В Python итоговая таблица сама автоматически изменяет размеры при добавлении новых паттернов. GROUPBY by Excel ведет себя так же, а вот связка GROUPBY&XLOOKUP уже потребует редактирования формул: нужно изменять адреса диапазонов ячеек, к которым обращается XLOOKUP.",
            "Вопреки большему размеру Python-код мне кажется проще и для написания, и для чтения-понимания. Хотя писать код в ячейке Excel -- весьма извращенное удовольствие.",
            "Исполнение кода Python требует интернет-соединения.",
            "Выводы и наблюдения не совсем по теме:",
            "Для меня лично схема дыхания “physiological sighs light” (легкие физиогические вздохи) -- оптимальный выбор.",
            "Более шести месяцев я придумывал, зачем мне может понадобиться живущий в Excel Python и наконец нашел.",
            "Буду благодарен за советы и критику. Постараюсь ответить на вопросы."
        ]
    },
    {
        "Название статьи": "Майним крипто-коины с помощью Python и компьютерного зрения",
        "Дата публикации": "2024-06-03, 13:58",
        "Автор статьи": "temabed ",
        "Статья целиком": [
            "После внезапного обогащения энтузиастов, которые поиграли в начале года в приложение Notcoin в телеграм, подобные проекты стали расти как грибы. Да и грибников заметно поприбавилось. Но в данной статье мы не будем касаться тем блокчейна или финансов, а рассмотрим простой пример применения компьютерного зрения для фарма поинтов в самом популярном, после Notcoin, проекте - хомяке комбате. Название явно на что-то намекает, но да ладно.",
            "Это не первый проект, который я автоматизирую, и не самый нуждающийся в этом. Да и без компьютерного зрения с автоматизацией хомяка можно спокойно обойтись. Но с ним, во-первых, интереснее, а во-вторых - это просто хороший пример с минимумом строк кода для демонстрации возможностей библиотеки cv2. Статья, соответственно, предназначена для энтузиастов и начинающих специалистов.",
            "Начнем с того, что мы установим все необходимые зависимости и импортируем их в свой проект. Вот они, слева направо.",
            "С помощью pyautogui наш бот будет управлять мышью. Keyboard пригодится для назначения горячих клавиш, чтобы управлять работой бота. cv2 наградит бота зрением, пусть и компьютерным, с помощью которого тот будет находить совпадения с искомым изображением. А numpy пригодится для работы с большими массивами, но тут он почти для галочки, не бойтесь. Модуль time тоже понадобится, чтобы ставить таймауты в работе программы.",
            "Далее напишем небольшую конструкцию- переключатель.",
            "Функция change при вызове всего лишь меняет значение переменной work, которая будет использована в бесконечном цикле. И если work будет False, работа нашего кода будет останавливаться. И наоборот запускаться, в противоположном случае.",
            "Кстати, забыл упомянуть, что разработчики проекта, над которым мы сейчас проводим эксперимент, большие молодцы, и убрали возможность пользоваться приложением на десктопных устройствах. Поэтому для его запуска понадобится эмулятор Android.",
            "Теперь определим основную логику работы бота:",
            "Он ищет совпадение с изображением полностью заполненной энергии.",
            "Если находит совпадение, ищет изображение монеты и кликает на неё энное количество раз.",
            "И всё это работает в бесконечном цикле.",
            "Значит со скриншота приложения необходимо вырезать две области, которые помечены красным, и разместить их в отдельные файлы, конечно же.",
            "Теперь напишем функцию для кликов по монетке. Она будет принимать путь к исходному изображению, а так же порог чувствительности для компьютерного зрения и интервал (таймаут) в секундах.",
            "Переменной template будет присвоено исходное изображение монетки, но в оттенках серого, так как мы указали в параметрах 0. Это необходимость, так как в оттенках серого компьютер зрит лучше. Сразу вычисляем высоту и ширину исходника, и присваиваем переменным. А далее по ходу исполнения кода он делает скриншот, сравнивает с исходником, получает координаты области с совпадением, и делает 260 даблкликов по ней. Координаты я ищу немного кривовато и в итоге loc содержит большой массив, из которого я использую лишь самые первые координаты, после чего цикл прерываю. Но лучше сделать не смог, извините.",
            "А теперь напишем аналогичную функцию, но с задачей искать совпадение с картинкой полной энергии, после чего вызывать функцию click.",
            "В целом всё аналогично. Добавил лишь ожидание горячей клавиши, чтобы можно было остановить программу в любое время нажатием на тильду (Ё). Ну и для красоты заключил в try-except.",
            "И это всё. Пишем последние строки и запускаем скрипт (не забыв нажать на Ё для запуска логики в цикле).",
            "По сути, этот код многофункционален, и его без труда, с минимальными изменениями, можно переделать под любые другие задачи. На всякий случай оставлю и полную версию кода:",
            "Не судите меня строго по этому скрипту, большую часть жизни я вообще бегал с пистолетиком, и пристрастился к разработке сравнительно недавно, поэтому всего лишь юный падаван в возрасте. Хотя могу писать и большие скучные штуки, но вот писать о них получится дольше, чем сам код. А если будут вопросы- добро пожаловать в телеграм. У меня там небольшой клуб по интересам."
        ]
    },
    {
        "Название статьи": "Сравниваем популярные алгоритмы кластеризации DBSCAN и OPTICS",
        "Дата публикации": "2024-06-03, 13:51",
        "Автор статьи": "evaclick ",
        "Статья целиком": [
            "Привет, Хабр)",
            "Поговорим сегодня о 2 популярных алгоритмах кластеризации — DBSCAN и OPTICS, посмотрим их особенности и сравним",
            "Поехали!",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Ну, DBSCAN в особом в представлении не нуждается, всё-таки один из самых популярных алгоритмов кластеризации. Поэтому по минимуму теории.",
            "DBSCAN (Density-based spatial clustering of applications with noise, плотностной алгоритм пространственной кластеризации с присутствием шума), как следует из названия, оперирует плотностью данных. На вход он просит матрицу близости точек и два параметра — радиус -окрестности и количество соседей .",
            "Эпсилон-окрестность для любого вектора в метрическом признаковом пространстве определяется как множество точек, отстоящих от не более чем на :",
            "где — выбранная метрика (например, евклидовое расстояние).",
            "В общих чертах, алгоритм DBSCAN можно представить как последовательность следующих этапов:",
            "найти новые точки в -окрестности каждой точки и определить основные точки с более чем соседями.",
            "найти связные компоненты основных точек на графе соседей, игнорируя все неосновные точки.",
            "назначить каждую неосновную точку ближайшему кластеру, если кластер является -соседним, в противном случае считаем точку шумом.",
            "Вот так можно использовать DBSCAN из Sci-Kit Learn + с интерактивными ползунками, работает в Colab'е (в Jupyter Notebook какие-то траблы с этим, если кто знает — please, help):",
            "С использованием DBSCAN в Julia и R особых проблем тоже не возникает —",
            "— Julia:",
            "— R:",
            "В идеальном случае DBSCAN может иметь линейную сложность , но не стоит особо на это рассчитывать. Если не пересчитывать каждый раз точек, то ожидаемая сложность — . Худший случай (плохие данные или брутфорс-реализация) — . Наивные реализации DBSCAN любят отъедать памяти под матрицу расстояний — это явно избыточно. Многие версии DBSCAN умеют работать и с более щадящими структурами данных: sklearn и R реализации можно оптимизировать при помощи KD-tree прямо из коробки.",
            "DBSCAN не вычисляет самостоятельно центры кластеров, однако вряд ли это проблема, особенно учитывая произвольную форму кластеров. Зато DBSCAN автоматически определяет выбросы, что довольно здорово.",
            "Соотношение , где — размерность пространства, можно интуитивно рассматривать как пороговую плотность точек данных в области пространства. Ожидаемо, что при одинаковом соотношении , и результаты будут примерно одинаковы. Иногда это действительно так, но есть причина, почему алгоритму нужно задать два параметра, а не один. Во-первых типичное расстояние между точками в разных датасетах разное — явно задавать радиус приходится всегда. Во-вторых, играют роль неоднородности датасета. Чем больше и , тем больше алгоритм склонен «прощать» вариации плотности в кластерах. С одной стороны, это может быть полезно: неприятно увидеть в кластере «дырки», где просто не хватило данных. С другой стороны, это вредно, когда между кластерами нет чёткой границы или шум создаёт «мост» между скоплениями. Тогда DBSCAN запросто соединит две разные группы. В балансе этих параметров и кроется сложность применения DBSCAN: реальные наборы данных содержат кластеры разной плотности с границами разной степени размытости. В условиях, когда плотность некоторых границ между кластерами больше или равна плотности каких-то обособленных кластеров, приходится чем-то жертвовать.",
            "Существуют варианты DBSCAN, способные смягчить эту проблему. Идея состоит в подстраивании в разных областях по ходу работы алгоритма. К сожалению, возрастает количество параметров алгоритма.",
            "Ок, теперь давайте немного поговорим о плюсах и минусах DBSCAN.",
            "Плюсы DBSCAN",
            "• DBSCAN не требует указания числа кластеров в отличие, скажем, от метода k-средних",
            "• DBSCAN может найти кластеры произвольной формы. Он может найти даже кластеры полностью окружённые (но не связанные с) другими кластерами.",
            "• DBSCAN имеет понятие шума и устойчив к выбросам.",
            "• DBSCAN требует лишь двух параметров ( и ) и большей частью нечувствителен к порядку точек в датасете. Однако, точки, находящиеся на границе двух различных кластеров могут оказаться в другом кластере, если изменить порядок точек, а назначение кластеров единственно с точностью до изоморфизма.",
            "Проблемы DBSCAN",
            "• DBSCAN не полностью однозначен — краевые точки, которые могут быть достигнуты из более чем одного кластера, могут принадлежать любому из этих кластеров, что зависит от порядка просмотра точек (тут стоит сказать, что существует DBSCAN❋, который трактует краевые точки как шум и тем самым достигается полностью однозначный результат)",
            "• Качество DBSCAN зависит от способа измерения расстояния. Наиболее часто используемой метрикой расстояний является евклидова метрика. В случае кластеризации данных высокой размерности эта метрика может оказаться почти бесполезной, что делает трудным делом нахождение подходящего значения . Этот эффект, однако, присутствует в любом другом алгоритме, основанном на евклидовом расстоянии.",
            "• DBSCAN не может хорошо разделить на кластеры наборы данных с большой разницей в плотности, поскольку не удается выбрать приемлемую для всех кластеров комбинацию и .",
            "Что ж, теперь давайте теперь переключимся на алгоритм OPTICS (Ordering Points To Identify the Clustering Structure).",
            "Основная идея OPTICS похожа на DBSCAN, но алгоритм предназначен для избавления от одной из главных слабостей алгоритма DBSCAN — проблемы обнаружения кластеров в данных, имеющих различные плотности. Для этого используется граф достижимости, который определяет достижимое расстояние для каждой точки, которая в дальнейшем будет относиться к ближайшему кластеру. Такой подход позволяет ещё лучше определять кластеры разной плотности, особенно если они расположены близко друг к другу, однако это увеличивает время работы алгоритма.",
            "Реализация OPTICS есть в библиотеке Sci-Kit Learn; вот как можно её импортировать и использовать:",
            "С R тоже проблем нет:",
            "Хорошо, давайте немного об особенностях OPTICS",
            "Плюсы OPTICS:",
            "Устойчивость к шуму (впрочем как и у DBSCAN): OPTICS способен обрабатывать данные с шумом и выбросами.",
            "Способность обнаруживать кластеры любой формы",
            "Не требует заранее заданного числа кластеров",
            "Проблемы OPTICS:",
            "Не всегда эффективен для плотных кластеров: OPTICS может иметь проблемы с эффективным обнаружением плотных кластеров, особенно если они имеют сложные формы.",
            "А вот несколько сфер, где регулярно используется OPTICS:",
            "Анализ сетей и обнаружение аномалий: OPTICS используется для анализа социальных сетей, транспортных сетей и других сетевых структур для выявления кластеров и аномалий.",
            "Биоинформатика: OPTICS применяется в биоинформатике для кластеризации геномных данных, выявления генных паттернов и классификации биологических образцов.",
            "Медицинская диагностика: OPTICS может быть применен для кластеризации медицинских данных, таких как результаты тестов, симптомы пациентов и история заболеваний, с целью выявления паттернов заболеваний или групп пациентов схожего профиля. .",
            "Итак, пришло время сравнить DBSCAN и OPTICS",
            "Вот DBSCAN:",
            "...а вот и OPTICS:",
            "И давайте возьмём для начала , , потом поменяем.",
            "Что мы видим? Для данного датасета DBSCAN выделяет кластеры более логичным и понятным способов, но в кластеризации OPTICS тоже есть пара интересных моментов. Как можно увидеть, точки вокруг главных кластеров DBSCAN безнадёжно отмечает как шум, в то время как OPTICS пытается нащупать кластеры и среди этих точек тоже. Это одна из главных фишек OPTICS — метод способен видеть кластеры разной плотности одновременно за счёт того, что он менее чувствителен к параметру .",
            "Вот довольно показательный пример — и тут OPTICS тоже выделил кластер в точках, которые забраковал DBSCAN:",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Описание алгоритма DBSCAN от Sci-Kit Learn",
            "Описание алгоритма OPTICS от Sci-Kit Learn",
            "Наглядная визуализация DBSCAN",
            "Что ж, надеюсь, статья была полезной)",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)"
        ]
    },
    {
        "Название статьи": "Реализация принципа единственной ответственности на Python",
        "Дата публикации": "2024-06-03, 07:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Сегодня мы рассмотрим одну из основополагающих концепций SOLID-принципов — принцип единственной ответственности или сокращенно - SRP. Разберем, что такое SRP и как правильно его применять в Python.",
            "Принцип единственной ответственности гласит, что каждый класс, метод или модуль должен иметь только одну причину для изменения. Проще говоря, каждый компонент вашей системы должен отвечать только за одну функциональность. Т.е если вам нужно внести изменение, связанное с этой функциональностью, вам придется изменить только один компонент.",
            "Когда каждый класс или модуль выполняет одну четко определенную задачу, становится гораздо проще понять его назначение и взаимодействие с другими частями системы.",
            "Что будет, если не соблюдать SRP?",
            "Если класс или модуль берет на себя несколько обязанностей, это приводит к увеличению сложности кода. Такой код сложнее читать, понимать и поддерживать. Также, когда один класс выполняет несколько задач, изменение в одной из них может непредсказуемо повлиять на другие.",
            "Классы, которые нарушают SRP, обычно плохо масштабируются и трудно переиспользуются. Их невозможно легко адаптировать для других целей или проектов.",
            "Для начала рассмотрим класс, который нарушает принцип единственной ответственности. Представим себе класс UserManager, который одновременно отвечает за создание юзера, валидацию данных и сохранение юзера в БД:",
            "Класс нарушает SRP, т.к выполняет несколько задач: валидацию email, создание пользователя и сохранение его в базу данных.",
            "Для исправления нарушения SRP нужно разделить обязанности на отдельные классы: User, UserValidator, UserDatabase, и UserCreator. Каждый класс будет отвечать только за одну задачу:",
            "Теперь каждый класс отвечает за одну конкретную задачу, что соответствует принципу единственной ответственности.",
            "Рассмотрим другой пример, обработку заказов в интернет-магазине. Изначально есть класс, который нарушает SRP, т.к он одновременно обрабатывает заказ, валидирует данные и отправляет уведомления:",
            "Рефакторинг этого класса для соответствия SRP:",
            "Фасадный паттерн помогает упростить взаимодействие между сложными подсистемами, предоставляя простой интерфейс для клиента. С фасадом можно скрыть сложность подсистем и предоставлять единый интерфейс для взаимодействия с ними.",
            "Предположим, есть система обработки заказов, включающая несколько классов для управления заказами, оплатами и уведомлениями. Без фасадного паттерна клиенту пришлось бы взаимодействовать с каждым из этих классов напрямую:",
            "А с использованием фасадного паттерна все будет выглядеть так:",
            "Интерфейсы и абстрактные классы помогают разделить обязанности и четко определить контракт, который должен реализовать класс.",
            "Создание интерфейсов для валидации, сохранения и уведомления:",
            "Разделяем обязанности на интерфейсы, что позволяет каждому классу реализовывать только свои специфические методы, соответствующие SRP.",
            "Для поддержки SRP и других принципов SOLID в Python можно использовать различные библиотеки.",
            "Pylint помогает анализировать код на наличие ошибок и несоответствий стилю, а также выявляет нарушения принципов SOLID, включая SRP.",
            "Mypy - статический анализатор типов для Python, который помогает обнаруживать типовые ошибки и улучшать структуру кода.",
            "Pytest помогает создавать модульные тесты для каждого отдельного компонента.",
            "Dataclasses модуль позволяет создавать классы данных, которые следуют SRP, отделяя логику данных от поведения.",
            "Про другие архитектурные принципы и инструменты коллеги из OTUS рассказывают в рамках практических онлайн-курсов. Также хочу напомнить о том, что в календаре мероприятий вы можете зарегистрироваться на ряд интересных и абсолютно бесплатных вебинаров."
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 12: Дата и время (издание 2024)",
        "Дата публикации": "2024-06-02, 19:47",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это двенадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как работать с датами и временем таким образом, чтобы это работало для всех ваших пользователей, независимо от того, где они проживают.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения",
            "Глава 12: Дата и время (Эта статья)",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Один из аспектов моего приложения для ведения микроблогов, который я долгое время игнорировал, - это отображение дат и времени. До сих пор я просто позволял Python отображать объект datetime в модели User и даже не потрудился отобразить его в модели Post. В этой главе вы узнаете, как работать с этими временными метками.",
            "Ссылки на GitHub для этой главы: Browse, Zip, Diff.",
            "Использование Python на сервере для отображения дат и времени, которые отображаются пользователям в их веб-браузерах, на самом деле не очень хорошая идея, потому что то, что сервер считает своим местным временем, не будет иметь смысла для пользователей, которые живут в другом часовом поясе.",
            "Совершенно ясно, что сервер должен управлять временем, которое является согласованным и независимым от его собственного местоположения и местоположения пользователей. Если это приложение разрастется до такой степени, что потребуется несколько производственных серверов в разных регионах мира, я бы не хотел, чтобы каждый сервер записывал временные метки в базу данных в разных часовых поясах, потому что это сделало бы невозможной работу с этими временами. Поскольку UTC является наиболее используемым единым часовым поясом и поддерживается в классе datetime, именно его я и собираюсь использовать.",
            "В главе 4 вы видели, как создавать временные метки UTC для записей в блоге. В качестве напоминания, вот краткий пример, показывающий, как это было сделано.:",
            "Но с этим подходом связана важная проблема. Пользователям из разных мест будет ужасно сложно определить, когда была сделана публикация, если они будут видеть время в часовом поясе UTC. Им нужно было бы заранее знать, что время указано в UTC, чтобы они могли мысленно подогнать его к своему собственному часовому поясу. Представьте пользователя, скажем, в часовом поясе PDT на Западном побережье США, который публикует что-то в 15: 00 и сразу видит, что сообщение появляется в 10: 00 по времени UTC, или, если быть более точным, в 22: 00. Это будет очень запутанно.",
            "Хотя стандартизация временных меток в соответствии с UTC имеет большой смысл с точки зрения сервера, это создает проблему удобства использования для пользователей. Цель этой главы - представить решение, которое сохраняет все временные метки, управляемые сервером, в часовом поясе UTC, не отталкивая пользователей.",
            "Очевидным решением проблемы является преобразование всех временных меток из сохраненных единиц UTC в местное время каждого пользователя при их отображении. Это позволяет серверу продолжать использовать UTC для обеспечения согласованности, в то время как преобразование \"на лету\", адаптированное к каждому пользователю, решает проблему удобства использования. Сложная часть этого решения - знать местоположение каждого пользователя.",
            "На многих веб-сайтах есть страница конфигурации, где пользователи могут указывать свой часовой пояс. Для этого мне потребуется добавить новую страницу с формой, в которой я представляю пользователям раскрывающийся список часовых поясов. При первом входе на сайт пользователей могут попросить ввести их часовой пояс в рамках регистрации.",
            "Хотя это достойное решение, решающее проблему, немного странно просить пользователей вводить часть информации, которую они уже настроили в своей операционной системе. Кажется, было бы эффективнее, если бы я мог просто получить настройки часового пояса с их компьютеров.",
            "Как выясняется, веб-браузер знает часовой пояс пользователя и предоставляет его через стандартные API JavaScript даты и времени. На самом деле есть два способа воспользоваться информацией о часовом поясе, доступной через JavaScript:",
            "Подход \"старой школы\" заключался бы в том, чтобы веб-браузер каким-то образом отправлял информацию о часовом поясе на сервер, когда пользователь впервые входит в приложение. Это можно было бы сделать с помощью вызова Ajax или гораздо проще с помощью мета-тега обновления. Как только сервер узнает часовой пояс, он может сохранить его в сеансе пользователя или записать в таблицу users в базе данных, и с этого момента корректировать с его помощью все временные метки во время отображения шаблонов.",
            "Подход \"новой школы\" заключается в том, чтобы ничего не менять на сервере и позволить преобразованию UTC в местный часовой пояс происходить в браузере с использованием JavaScript.",
            "Оба варианта допустимы, но второй имеет большое преимущество. Знания часового пояса пользователя не всегда достаточно для представления дат и времени в формате, ожидаемом пользователем. Браузер также имеет доступ к конфигурации языкового стандарта системы, которая определяет такие параметры, как время утра / вечера в сравнении с 24-часовыми часами, формат отображения даты DD / MM / ГГГГ в сравнении с MM / DD / ГГГГ и многие другие культурные или региональные стили.",
            "И если этого недостаточно, у подхода новой школы есть еще одно преимущество. Есть библиотека с открытым исходным кодом, которая выполняет всю эту работу!",
            "Moment.js это небольшая библиотека JavaScript с открытым исходным кодом, которая выводит отображение даты и времени на новый уровень, поскольку предоставляет все мыслимые варианты форматирования, а затем и некоторые другие. Некоторое время назад я создал Flask-Moment, небольшое расширение Flask, которое позволяет очень легко интегрировать moment.js в ваше приложение.",
            "Итак, давайте начнем с установки Flask-Moment:",
            "Это расширение добавляется в приложение Flask обычным способом:",
            "app/__init__.py: Пример Flask-Moment.",
            "В отличие от других расширений, Flask-Moment работает вместе с moment.js, поэтому все шаблоны приложения должны включать эту библиотеку. Чтобы гарантировать, что эта библиотека всегда доступна, я собираюсь добавить ее в базовый шаблон. Это можно сделать двумя способами. Самый прямой способ - явно добавить тег <script>, который импортирует библиотеку, но Flask-Moment упрощает задачу, предоставляя функцию moment.include_moment(), которая генерирует тег <script>:",
            "app/templates/base.html: Включить moment.js в базовый шаблон.",
            "В большинстве случаев библиотеки JavaScript, используемые приложением, включены в конец содержимого <body>, где находится загрузочный JavaScript-код.",
            "Moment.js делает класс moment доступным для браузера. Первым шагом для отображения временной метки является создание объекта этого класса, передающего желаемую временную метку в формате ISO 8601. Вот пример, запущенный в консоли JavaScript браузера:",
            "Если вы не знакомы со стандартным форматом даты и времени ISO 8601, этот формат выглядит следующим образом:",
            "Я уже решил, что буду работать только с часовыми поясами UTC, поэтому последней частью всегда будет +00:00 или в некоторых случаях эквивалент Z, который представляет UTC в стандарте ISO 8601.",
            "В объекте moment предусмотрено несколько методов для различных вариантов рендеринга. Ниже приведены некоторые из наиболее распространенных вариантов:",
            "В этом примере создается объект moment, инициализированный 28 июня 2021 года в 21:45 по Гринвичу. Вы можете видеть, что все параметры, которые я пробовал выше, отображаются в UTC + 1, который является часовым поясом, настроенным на моем компьютере. Вы можете ввести вышеуказанные команды в консоли вашего браузера, убедившись, что в странице, на которой вы открываете консоль, включена moment.js. Вы можете сделать это в микроблоге, при условии, что вы внесли вышеуказанные изменения для включения moment.js, или также на https://momentjs.com/.",
            "Обратите внимание, как разные методы создают разные представления. С помощью метода format() вы управляете форматом выходных данных с помощью строки формата. Метод fromNow() интересен тем, что он отображает временную метку по отношению к текущему времени, поэтому вы получаете выходные данные, такие как \"минуту назад\" или \"через два часа\" и т.д.",
            "Если вы работали непосредственно в JavaScript, приведенные выше вызовы возвращают строку с отображаемой временной меткой. Затем вам предстоит вставить этот текст в нужное место на странице, что, к сожалению, требует работы с DOM. Расширение Flask-Moment значительно упрощает использование moment.js за счет включения в ваши шаблоны объекта moment, аналогичного объекту JavaScript.",
            "Давайте посмотрим на временную метку, которая отображается на странице профиля. Текущий шаблон user.html позволяет Python генерировать строковое представление времени. Теперь я могу отобразить эту временную метку с помощью Flask-Moment следующим образом:",
            "app/templates/user.html: Отрисовка временной метки с помощью moment.js.",
            "Итак, как вы можете видеть, Flask-Moment использует синтаксис, аналогичный синтаксису библиотеки JavaScript, с одним отличием, заключающимся в том, что аргументом для moment() теперь является объект Python datetime, а не строка ISO 8601. Вызов moment(), выполняемый из шаблона, автоматически генерирует необходимый код JavaScript для вставки отображаемой временной метки в нужное место DOM.",
            "Второе место, где я могу воспользоваться преимуществами Flask-Moment, находится во вложенном шаблоне _post.html, который вызывается с главной страницы и страницы пользователя. В текущей версии шаблона каждому сообщению предшествует строка \"username says:\". Теперь я могу добавить временную метку, отображаемую с помощью fromNow():",
            "app/templates/_post.html: Отрисовка временной метки во вложенном шаблоне post.",
            "Ниже вы можете увидеть, как выглядят обе эти временные метки при рендеринге с помощью Flask-Moment и moment.js:"
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 11: Дизайн приложения (издание 2024)",
        "Дата публикации": "2024-06-02, 19:46",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это одиннадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как заменить базовые HTML-шаблоны новым набором, основанным на платформе пользовательского интерфейса Bootstrap.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения (Эта статья)",
            "Глава 12: Даты и время",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Вы уже некоторое время играете с моим приложением для ведения микроблогов, поэтому, я уверен, вы заметили, что я не потратил слишком много времени на то, чтобы оно выглядело хорошо, или, лучше сказать, я вообще не тратил на это времени. Шаблоны, которые я собрал, довольно простые, без какого-либо пользовательского оформления. Мне было полезно сосредоточиться на реальной логике приложения, не отвлекаясь на написание красивых HTML и CSS.",
            "Но я уже долго сосредоточен на серверной части этого приложения. Итак, в этой главе я сделаю перерыв и потрачу некоторое время на то, чтобы показать вам, что можно сделать, чтобы приложение выглядело немного более отточенным и профессиональным.",
            "Эта глава будет немного отличаться от предыдущих, потому что я не собираюсь так подробно, как обычно, описывать сторону Python, которая, в конце концов, является основной темой этого туториала. Создание красивых веб-страниц - обширная тема, которая в значительной степени не связана с веб-разработкой на Python, но я расскажу о некоторых основных рекомендациях и идеях о том, как подойти к этой задаче, и у вас также будет приложение с измененным дизайном, которое можно изучить и перенять опыт.",
            "Ссылки на GitHub для этой главы: Обзор, Zip, Diff.",
            "Хотя мы можем утверждать, что программирование - это сложно, наши усилия ничто по сравнению с усилиями веб-дизайнеров, которым приходится создавать веб-страницы, которые красиво и единообразно выглядят в списке веб-браузеров. За последние годы они стали лучше, но в некоторых браузерах все еще есть непонятные ошибки или причуды, которые сильно усложняют задачу создания веб-страниц, которые везде выглядят красиво. Это еще сложнее, если вам также нужно настроить для браузеров планшетов и смартфонов с ограниченным количеством ресурсов и экранов.",
            "Если вы, как и я, разработчик, который просто хочет создавать прилично выглядящие веб-страницы, но у вас нет времени или интереса изучать низкоуровневые механизмы для эффективного достижения этой цели путем написания необработанного HTML и CSS, то единственным практическим решением является использование CSS фреймворка для упрощения задачи. Выбрав этот путь, вы потеряете некоторую творческую свободу, но, с другой стороны, ваши веб-страницы будут хорошо выглядеть во всех браузерах без особых усилий. Фреймворк CSS предоставляет коллекцию высокоуровневых классов CSS с готовыми стилями для распространенных типов элементов пользовательского интерфейса. Большинство этих фреймворков также предоставляют дополнения JavaScript для вещей, которые нельзя выполнить строго с помощью HTML и CSS.",
            "Одним из самых популярных CSS-фреймворков является Bootstrap. Если вы хотите увидеть, какие страницы можно создавать с помощью этого фреймворка, в документации есть несколько примеров.",
            "Вот некоторые преимущества, которые вы получаете при использовании Bootstrap для оформления ваших веб-страниц:",
            "Аналогично выглядит во всех основных веб-браузерах",
            "Настройка размеров для экранов настольных компьютеров, планшетов и телефонов",
            "Настраиваемые макеты",
            "Красиво оформленные панели навигации, формы, кнопки, оповещения, всплывающие окна и т.д.",
            "Самый простой способ использовать Bootstrap - это просто импортировать файл bootstrap.min.css в ваш базовый шаблон. Вы можете либо загрузить копию этого файла и добавить его в свой проект, либо импортировать его непосредственно из CDN. Затем вы можете начать использовать CSS-классы общего назначения, которые он предоставляет, согласно документации, что довольно неплохо. Возможно, вы также захотите импортировать JavaScript-код фреймворка, чтобы использовать самые продвинутые функции.",
            "Как и большинство проектов с открытым исходным кодом, Bootstrap постоянно развивается. Оригинальная версия мега-учебника Flask была создана для Bootstrap 3. Редакция, которую вы сейчас читаете, создана для Bootstrap 5.3. Текущий подход к интеграции Bootstrap является довольно общим и может быть адаптирован к более новым версиям Bootstrap.",
            "Первым шагом в интеграции Bootstrap с Microblog является добавление его файлов CSS и JavaScript в базовый шаблон. На странице быстрого запуска Bootstrap в качестве примера приведена короткая, но полная HTML-страница, которую я копирую ниже для вашего удобства:",
            "Подход, который я могу применить, чтобы объединить это с моим шаблоном base.html, заключается в том, чтобы использовать приведенный выше в качестве нового базового шаблона, заменив теги <title> и <h1> заголовком и основным содержимым исходного базового шаблона соответственно.",
            "Следующий шаг - заменить базовую панель навигации на более удобную из Bootstrap. На странице документации по панели навигации Bootstrap вверху показан хороший пример. Используя этот пример в качестве руководства, я создал панель навигации со ссылками \"Index\", \"Explore\", \"Profile\", \"Login\" и \"Logout\" из микроблога. Для удобства я настроил профиль, а также ссылки для входа и выхода так, чтобы они отображались в крайнем правом углу.",
            "При использовании Bootstrap полезно знать о некоторых базовых примитивах компоновки. Одним из наиболее важных является контейнер, который определяет область содержимого страницы. Два основных контейнера называются container и container-fluid. В первом случае страница настраивается на использование одной из пяти предопределенных ширин страницы и центрирует содержимое в окне браузера. С другой стороны обтекающий контейнер дает вам доступ ко всей ширине страницы. Для этого приложения я решил использовать контейнер по умолчанию, потому что он предотвращает слишком широкое расширение страницы независимо от размера экрана, поэтому часть содержимого страницы будет заключена в один из этих контейнеров следующим образом:",
            "Последняя часть HTML-разметки в шаблоне base.html, которую необходимо адаптировать, - это раздел, отображающий отображаемые сообщения. Компонент Alert от Bootstrap прекрасно подходит для этой задачи.",
            "Вы можете получить полностью переработанный шаблон base.html из репозитория Github для этой главы. Ниже вы можете увидеть упрощенную структуру, если хотите иметь представление о том, как она выглядит.:",
            "app/templates/base.html: Переработанный базовый шаблон.",
            "Благодаря обновленному базовому шаблону внешний вид приложения уже заметно улучшен без необходимости изменять строки кода Python. Если вы хотите убедиться в этом сами, загрузите копию base.html из репозитория GitHub по ссылкам, приведенным в начале этой главы.",
            "Область, в которой Bootstrap проделывает фантастическую работу, заключается в рендеринге полей формы, которые выглядят намного приятнее и чище, чем поля по умолчанию, предоставляемые браузером. В документации по Bootstrap также есть раздел о формах. В начале этого раздела приведен пример формы входа в систему, который показывает базовую структуру HTML.",
            "HTML-код, необходимый для каждого поля, довольно длинный. Ниже вы можете увидеть одно из текстовых полей из примера формы в документации:",
            "Но это слишком просто для нужд Microblog, который включает проверку полей и, возможно, потребуется показывать пользователю ошибки проверки. На странице документации есть раздел о проверке на стороне сервера, в котором показано, как оформить поля с сообщением об ошибке. Вот пример.:",
            "К сожалению, о необходимости вводить такое количество шаблонов для каждого поля в каждой форме не может быть и речи. Это заняло бы слишком много времени и чревато ошибками. Одним из решений является использование макросов Jinja, которые позволяют вам определять повторно используемые фрагменты HTML, а затем вызывать их из ваших шаблонов, как если бы они были функциями.",
            "Например, макрос Jinja для текстового поля, подобного показанному выше, будет иметь вид:",
            "Обратите внимание, как используются условные обозначения для выборочного добавления стиля ошибки, если поле содержит одно или несколько сообщений об ошибках.",
            "Поскольку макрос определен в файле с именем bootstrap_wtf.html, который расположен в каталоге templates, он может быть вызван, когда потребуется отобразить поле. Например:",
            "Макрос отображения полей можно расширить, чтобы он также поддерживал отображение флажков, раскрывающихся списков выбора, кнопок отправки и других типов полей. Он также может принимать второй аргумент с логическим значением, указывающим, следует ли автоматически переводить поле в фокус страницы, что должно быть сделано для первого поля формы. Для еще большего удобства можно создать другой макрос для рендеринга всей формы, просто перебрав поля формы и вызвав form_field() макрос для каждого из них.",
            "Полный bootstrap_wtf.html файл доступен в репозитории GitHub, ссылка на который приведена в начале этой главы. Он включает в себя более полную версию макроса form_field(), показанного выше, и второй макрос с именем quick_form(), который принимает объект формы и отображает все его поля с помощью первого макроса.",
            "Как это выглядит, когда реализовано в реальной форме? Ниже вы можете увидеть переработанный шаблон register.html в качестве примера:",
            "app/templates/register.html: Шаблон регистрации пользователя.",
            "Разве это не здорово? Оператор import вверху работает аналогично импорту Python на стороне шаблона. Который добавляет макрос wtf.quick_form(), который в одной строке кода отображает полную форму, включая ошибки проверки, и все оформлено в соответствии с фреймворком Bootstrap.",
            "Еще раз, я не собираюсь показывать вам все изменения, которые я сделал для других форм в приложении, но все эти изменения внесены в шаблоны, которые вы можете загрузить или просмотреть на GitHub.",
            "Логика представления, которая отображает отдельные записи в блоге, была абстрагирована в подшаблон под названием _post.html. Все, что мне нужно сделать с этим шаблоном, это внести некоторые незначительные корректировки, чтобы он хорошо выглядел в Bootstrap.",
            "app/templates/_post.html: Переработанный подшаблон публикации.",
            "Ссылки на страницы - это еще одна область, в которой Bootstrap предоставляет поддержку. Для этого я просто еще раз обратился к документации Bootstrap и адаптировал один из их примеров. Вот как это выглядит на странице index.html:",
            "app/templates/index.html: Переработаны ссылки на страницы.",
            "Обратите внимание, что в этой реализации вместо скрытия следующей или предыдущей ссылки, когда в этом направлении больше нет содержимого, я применяю отключенное состояние, из-за которого ссылка будет отображаться серым цветом.",
            "Я не собираюсь показывать это здесь, но аналогичное изменение необходимо применить к шаблону user.html. Пакет для загрузки этой главы включает эти изменения.",
            "Чтобы внести в ваше приложение эти изменения, пожалуйста, загрузите zip-файл для этой главы и соответствующим образом обновите свои шаблоны.",
            "Ниже вы можете увидеть несколько фотографий до и после, чтобы увидеть трансформацию. Имейте в виду, что это изменение было достигнуто не затрагивая ни одной строки кода приложения!",
            "Следующая глава =>"
        ]
    },
    {
        "Название статьи": "Расширяем возможности Keras с помощью кастомных слоев",
        "Дата публикации": "2024-06-02, 18:07",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Keras предоставляет мощные инструменты для создания сложных нейронных сетей. Однако иногда стандартного набора слоев недостаточно для решения некоторых задач. В таких случаях на помощь приходят кастомные слои.",
            "Кастомные слои позволяют адаптировать архитектуру модели под особенности данных, улучшая тем самым производительность и точность моделек.",
            "Каждый кастомный слой начинается с определения нового класса, наследующего от tf.keras.layers.Layer. В __init__ происходит инициализация слоя, где можно задать параметры, необходимые для работы слоя:",
            "Тут units определяет количество нейронов, а activation указывает функцию активации. super(CustomLayer, self).__init__(**kwargs) вызывает конструктор базового класса Layer.",
            "Метод build вызывается Keras при первом использовании слоя. Его юзают для создания параметров слоя, которые зависят от размера входных данных:",
            "В методе создаются веса kernel и bias. Функция add_weight создает и регистрирует переменные слоя, которые будут обновляться во время тренировки.",
            "Метод call содержит основную логику вычислений слоя. Он принимает входные данные и возвращает выходные:",
            "В этом методе выполняется умножение входных данных на веса и добавление смещения. Если определена функция активации, она применяется к выходным данным.",
            "После определения кастомного слоя его можно использовать в моделях Keras как обычный слой:",
            "Другие полезные методы:",
            "add_weight: Добавляет переменную веса в слой.",
            "compute_output_shape: Возвращает форму выходных данных на основе формы входных.",
            "get_config: Возвращает конфигурацию слоя в виде словаря, что полезно для сериализации.",
            "Dense слой выполняет простую линейную операцию: умножение входного вектора на матрицу весов и добавление смещения, а затем применяется функция активации:",
            "Convolutional слои применяют свертку фильтра к входным данным, что позволяет выделять пространственные особенности:",
            "Recurrent слои используются для обработки последовательных данных. Один из наиболее распространнных типов рекуррентных слоев — это LSTM:",
            "Dropout слой используется для регуляризации модели, предотвращая переобучение путем случайного зануления некоторых нейронов во время тренировки:",
            "BatchNormalization слой нормализует активации предыдущего слоя, улучшая скорость обучения модельки:",
            "Больше практических инструментов и кейсов коллеги из OTUS рассматривают в рамках практических онлайн-курсов. Напомню, что с полным каталогом курсов можно ознакомиться по ссылке."
        ]
    },
    {
        "Название статьи": "Enbeddrus — обучение независящей от языка эмбеддинг-модели",
        "Дата публикации": "2024-06-02, 17:31",
        "Автор статьи": "efreelancer ",
        "Статья целиком": [
            "Приветствую, хабровчане!",
            "Сегодня хочу рассказать вам историю о том, как я обучил простую и компактную независящую от языка (language agnostic) модель-эмбеддер, которая умеет работать с техническими текстами о PHP и способна извлекать схожие эмбеддинги для параллельных текстов на английском и русском языках.",
            "Основная причина, по которой я решил заняться этим проектом, заключается в том, что мои заметки, код и документация, накопленные за более чем десять лет практики, представляют собой солянку текстов о разных технологиях, языках программирования, пометки о настройке серверов Linux и т.д. на русском и английском языках. Поэтому мне захотелось сделать Retrieval-Augmented Generation (RAG) помогалку, которая сможет принимать запросы пользователя (меня) и эффективно находить информацию в столь разношерстой базе данных, независимо от того на каком языке я сделал запрос и на каком языке написана документация.",
            "Для достижения этой цели как-раз и необходима независимая от языка модель-эмбеддер, которая будет одинаково хорошо работать с техническими текстами на русском и английском языках.",
            "Ещё одним важным аспектом было то, чтобы модель потребляла как можно меньше ресурсов и, если возможно, чтобы её можно было преобразовать в формат GGUF.",
            "Но прежде чем приступить к созданию своего собственного велосипеда, я решил поискать готовые решения, ведь подобная идея очевидна и, возможно, уже реализована другими.",
            "Спойлер: идея не нова, и подобных решений уже достаточно много.",
            "Для построения системы, которая может извлекать одинаковые эмбеддинги для схожих текстов на русском и английском языках, существует несколько решений, например...",
            "Ссылки: arxiv:1907.04307 , kaggle, github",
            "Это проект разработан инженерами Google и поддерживает 16 языков.",
            "Свойства: ~110m параметров, принимает на вход 128 токенов текста и извлекает из них 512-мерный эмбеддинг.",
            "Плюс: поддерживает русский язык.",
            "Минусы: модель основана на Tensorflow, а так же что с 2019го года не было обновлений.",
            "Ссылки: arxiv:1710.04087, github",
            "Это одна из первых попыток инженеров FB создать модель которая способна выполнять задачи по извлечению независящих от языка эмбеддингов.",
            "Плюс: поддерживает русский язык.",
            "Минусы: в наличии имеются веса для пар языков, навроде en-ru, en-de и т.д., весов нет на HuggingFace, ну и с 2018го года проект не развивается.",
            "Ссылки: arvix:2205.12654, github, pypi",
            "Ещё одна модель разработана инженерами FB и, как сказано в ридми на GitHub, поддерживает более 200 языков (хотя если пройти по ссылочкам и посчитать то получится 147 языков).",
            "Свойства: ~256m параметров, принимает 1024 токенов на вход и извлекает из них 1024-мерный эмбеддинг.",
            "Плюсы: она основана на PyTorch и имеет логику переключения между языками которая явно перекочевала из NLLB (о которой я кстати рассказывал в публикации \"Перевод на разные языки используя модель NLLB\" у себя в блоге на Дзен).",
            "Минусы: весов нет на HuggingFace, а модель несовместима с llama.cpp поэтому её не получится конвертировать в GGUF, чтобы можно было запускать на слабом железе (или же в паре с ollama).",
            "Ссылки: arXiv:1908.10084, сайт",
            "Модели Sentence-BERT представляют собой модифицированную версию предобученной BERT, специально адаптированную для генерации эмбеддингов предложений, multilingual версия позволяет извлекать эмбеддинги из текста на разных языках, а paraphrase модели позволяют извлекать похожие эмбеддинги парафраз на разных языках.",
            "Вот пару примечательных моделей, обученных разными способами:",
            "paraphrase-multilingual-MiniLM-L12-v2 имеет 118m параметров, принимает 256 токенов на вход и возвращает 384-мерный эмбеддинг.",
            "paraphrase-multilingual-mpnet-base-v2 имеет 278m параметров, принимает на вход 512 токенов и возвращает 768-мерный эмбеддинг.",
            "Обе эти модели обучены на комбинации из датасетов:",
            "SNLI о котором говорится в публикации \"A large annotated corpus for learning natural language inference\" (570k примеров)",
            "Multi-Genre NLI, подробнее в работе \"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\" (433k примера)",
            "Плюсы: поддерживает русский язык, можно конвентировать в GGUF.",
            "Минусы: модели не очень хорошо понимают технический текст (особенно русский технический жаргон), нет версии в формате GGUF, и к числу фатальных недостатков могу отнести, что эти модели обучил не я ;)",
            "Пришёл к выводу, что тему обучения подобных модей-эмбеддеров уже достаточно хорошо изучили и что можно без особых сложностей реализовать мою задумку.",
            "В качестве базовой модели решил взять модель google-bert/bert-base-multilingual-uncased, потому что:",
            "У этой крохи всего 168m параметров, что чуть больше чем у paraphrase-multilingual-MiniLM-L12-v2, но меньше чем у paraphrase-multilingual-mpnet-base-v2;",
            "На вход она принимает 512 токенов, а на выходе возвращает 768-мерный эмбеддинг, столько же у paraphrase-multilingual-mpnet-base-v2;",
            "Модель обучена на датасете wikipedia представляющем из себя Text Corpora, а там, сами понимаете, примеров текста больше, чем SNLI и Multi-Genre NLI вместе взятые;",
            "Модель uncased, то есть обучение происходило на регистронезависимых текстах (сиречь всё переводилось в lowercase).",
            "С моделью определились, теперь перейдём к вопросу выбора датасета...",
            "Изначально я хотел собрать больше датасетов, но, собирая датасет по PHP, я понял, какой это трудоёмкий процесс, и решил уменьшить свои амбиции.",
            "Итак, после поиска в интернете я нашёл только один подходящий датасет: OPUS PHP v1 на 2k примеров, содержащий пары текстов на русском и английском языках, по теме PHP.",
            "Из указанного датасеста я использовал только английский корпус (так как русский корпус был очень низкого качества), далее задействовал инстанс LibreTranslate для перевода английских текстов на русский и очистил данные от аномалий и шума (сценарий dataset_php_build.ipynb). Затем вручную перевёл кривые места с помощью Google и Yandex Translate и экспортировал результат в CSV формат. Данные отсортировал и удалил дубликаты (сценарий dataset_php_undup.py) после чего осталось 1.6k примеров.",
            "В финале попросил ChatGPT сгенерировать 100 примеров пар технического текста о PHP на русском и английском языках для сплита eval, а очищенные данные использовал для сплита train.",
            "Результат выгрузил (сценарий dataset_php_publish.ipynb) на HuggingFace: evilfreelancer/opus-php-en-ru-cleaned .",
            "Для создания эффективного эмбеддера, способного работать с техническими текстами о PHP на русском и английском языке, я решил провести обучение модели в два этапа, сначала выполнить Domain Adaptation, чтобы модель могла работать с техническими текстами на английском языке, а после этого обучить её на Parallel Corpora из русских и английских текстов.",
            "Для Domain Adaptation я использовал метод Generative Pseudo Labeling (GPL) (arXiv:2112.07577), данный метод позволяет проводить обучение модели на основе неразмеченных данных, генерируя псевдометки и улучшая качество работы модели для специфических доменов.",
            "Библиотека gpl имеет захардкоженный формат входного датасета и читает данные по определённым путям, поэтому пришлось слегка конвертировать тренировочный датасет и положить результат в директорию datasets (сценарий: dataset_php_convert.py).",
            "Для адаптации модели bert-base-multilingual-uncased к домену английских текстов про PHP я использовал в качестве шаблона скрипт, предложенный авторами проекта GPL на их странице на GitHub, получился следующего вида код:",
            "Полный скрипт тренировки train_domain.py можно найти в репозитории проекта на GitHub.",
            "Процесс обучения включает в себя несколько этапов:",
            "Используется генератор запросов, такой как BeIR/query-gen-msmarco-t5-base-v1, для создания синтетических запросов на основе текстов из корпуса;",
            "С помощью ретриверов, таких как msmarco-distilbert-base-v3 и msmarco-MiniLM-L-6-v3, которые работают с косинусным сходством, извлекаются наиболее релевантные документы для сгенерированных запросов;",
            "Кросс-энкодер, такой как cross-encoder/ms-marco-MiniLM-L-6-v2, используется для создания псевдометок, присваивая оценочные метки соответствия между запросами и документами;",
            "Модель обучается с использованием MarginMSELoss, которая позволяет модели лучше адаптироваться к новому домену.",
            "И так, наша модель обучена работать с новым доменом, поэтому переходим к следующему шагу.",
            "Для обучения модели на параллельных корпусах я использовал метод обучения моделей на разных языках, описанный в примере на сайте Sentence Transformers. Этот метод позволяет обучать мультиязычные модели, используя параллельные тексты на разных языках (заготовка скрипта make_multilingual.py).",
            "Для оценки качества модели я написал юпитер-блокнот, который загружает базовую и дообученную модель, прогоняет пары из eval сплита датасета evilfreelancer/opus-php-en-ru-cleaned и анализирует разницу между эмбеддингами, построенными для текстов на разных языках. Результаты визуализируются в виде графиков. Скрипт можно найти здесь.",
            "На графике видно, что базовая модель bert-base-multilingual-uncased распределяет русские и английские тексты в изолированные кластеры точек, ну а наша задача сделать так, чтобы эти точки были расположены как можно ближе друг к другу.",
            "Подобную задачу позволяет решать MSELoss, так как она минимизирует разницу между эмбеддингом, сгенерированным моделью-учителем (на английском языке) и эмбеддингом, сгенерированным моделью-учеником (на русском языке).",
            "Теперь пару слов про датасеты, решил остановиться на следующем наборе:",
            "evilfreelancer/opus-php-en-ru-cleaned (1.6k) - ранее созданный датасет параллельных текстов на английском и русском языках;",
            "Helsinki-NLP/opus_books (17.5k) - датасет OPUS параллельных текстов из книг.",
            "Выбрал я их потому, что мои первые эксперименты с обучением модели на только PHP датасете показали, что у модели происходит overfitting в результате чего падала общее качество работы модели, поэтому самым логичным решением было добавить ещё один Parallel Corpora общего назначения.",
            "Помимо этого в скрипт обучения я хотел сразу заложить возможность обучать на множестве разных датасетов (имеющих разные форматы данных), в результате чего получилась функция:",
            "В дальнейшем планирую добавить в неё больше датасетов на разные технические темы, но на этапе прототипирования того что есть более чем достаточно.",
            "Двигаемся дальше.",
            "Полный скрипт тренировки train_parallel.py можно найти в репозитории проекта на GitHub, в качестве модели-учителя возьмём google-bert/bert-base-multilingual-uncased, а в качестве модели-ученика ту, что мы обучили ранее на шаге Domain Adaptation.",
            "Обучение происходит в несколько этапов:",
            "Сначала мы загружаем датасеты (функция read_datasets);",
            "Далее выполняем их преобразование в нужный формат, после чего сохраняем на диске (функциия prepare_datasets)",
            "Инициализируем модель-учитель и модель-ученик (тут)",
            "Инициализируем MSELoss, передав ей на вход указатель на модель-ученика (тут)",
            "Запускаем обучение модели-ученика",
            "По завершению обучению давайте попробуем протестировать модель и понять стала ли на лучше извлекать эмбеддинги.",
            "Как видно на графике эмбеддинги извлечённые из русских и английских текстов где-то наложились друг на друга, точность похожести поднялась с 0.83 до 0.94, при этом модель также хорошо разделяет фразы различающиеся по смыслу.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1-domain",
            "Посмотрел я на этот график и пришла в голову мысль, а что если попробовать обучить базовую модель сразу на Parallel Corpora, пропустив шаг с Domain Adaptation?",
            "Правим скрипт тренировки, меняем модель-ученика, получается вот так:",
            "Опять запускаем тренировку и ждём некоторое время, по завершению прогоняем тесты и смотрим что получилось.",
            "Как видно на графиках если обучать сразу на Parallel Corpora модель быстрее, так как не нужно выполнять Domain Adaptation, и лучше обучается извлекать эмбеддинги из параллельных текстов, ведь косинусное расстояние в таком случае между близкими по смыслу фразами на разных языках в среднем в районе 0.97, что выше чем у модели изначально обученной на домене текстов про PHP.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1",
            "Отсюда можно сделать вывод, что дообучение мультиязыковой модели bert-base-multilingual-cased через Domain Adaptation с последующем обучением на Parallel Corpora не имеет особого смысла и проще сразу дообучать её на Parallel Corpora.",
            "Осталось выполнить самую малость, для начала я хочу конвертировать модель в формат GGUF, чтобы можно было использовать обученные модели через llama.cpp, но на этом моменте сильно не будем заострять внимание, сошлюсь на мою публикацию \"Как конвертировать модель BERT в формат GGUF?\" в моём блоге и PR который я создал в проекте llama.cpp.",
            "Но если кратко команды конвертации нужно выполнять и корня проекта llama.cpp и выглядят они следующим образом:",
            "По её завершению в директории models появятся файлы: enbeddrus-v0.1-f16.gguf и enbeddrus-v0.1-domain-f16.gguf.",
            "Полученные модели я выгрузил на серверы Ollama следующим образом:",
            "Выгруженные модели находятся тут и скачать их можно следующей командой:",
            "Содержимое Modelfile'ов можно найти в директории models проекта на GitHub.",
            "https://github.com/EvilFreelancer/enbeddrus",
            "https://huggingface.co/datasets/evilfreelancer/opus-php-en-ru-cleaned",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1-domain",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1",
            "https://ollama.com/evilfreelancer/enbeddrus",
            "Благодаря работе над проектом enbeddrus были достигнуты следующие цели:",
            "Удалось разобрался с тем как подобные модели устроены и как они работают, а так же с тем как их можно обучать;",
            "Был собран датасет с Parallel Corpora тематических текстов о PHP на русском и английском;",
            "Удалось разобраться с методами оценки моделей, а также с тем как эту оценку красиво визуализировать;",
            "Была обучена модель, которая эффективно работает с текстами на двух языках и может быть использована в RAG-системе для поиска и анализа информации.",
            "Полученные результаты подтверждают, что обучение мультиязычных эмбеддеров на основе параллельных корпусов является эффективным подходом для создания моделей, способных работать с текстами на разных языках.",
            "Спасибо за внимание и за что дочитал публикацию до конца! Если у вас есть вопросы или вы хотите связаться со мной, ссылки на мои контакты в социальных сетях можно найти в моём профиле на Хабре."
        ]
    },
    {
        "Название статьи": "Я научу вас неправильно играть в Hearts of iron. Оптимизация довоенной экономики: часть 2",
        "Дата публикации": "2024-06-02, 17:02",
        "Автор статьи": "Glasssparrow ",
        "Статья целиком": [
            "В прошлой части мы создали инструментарий, настало время им воспользоваться.",
            "За долю секунды мы можем провести симуляцию нескольких внутриигровых лет, что позволяет нам применить простейший метод исследования - метод перебора. И, раз уж мы всё равно будем перебирать, стоит также построить графики.",
            "В качестве испытуемой страны мы выберем, конечно, Советский Союз, условия будем выбирать близкие к реальному прохождению.",
            "Во-первых, рассмотрим торговлю. Торговля в игре зависит от многих факторов и не может быть оценена в симуляции, т.к. мы не работаем со всем миром (это потребует больших вычислительных мощностей). Таким образом, торговлю можно взять только из игры, что я и сделал, прокрутив 5 внутриигровых лет и записав количество фабрик получаемых от торговли. При этом закупки брались равными нулю (закупки сильно зависят от того что производит игрок, потому для общего случая я их просто игнорировал).",
            "1 установить_торговлю 18 120 установить_торговлю 10 210 установить_торговлю 7 365 установить_торговлю 5 730 установить_торговлю 9 1100 установить_торговлю 13 1450 установить_торговлю 25 1800 установить_торговлю 18",
            "Во-вторых, рассмотрим технологии. Нам важны технология индустрии и технология строительства. Моменты их развития также были получены из игры следующим образом: технологии исследовались без опережения по времени (с некоторыми погрешностями, конечно), в первую очередь строительство и индустрия, во вторую электроника, всё остальное исследовалось лишь для того чтобы сбить накапливающиеся во время простоя дополнительные 30 дней исследований. Никаких фокусов, решений и политик на исследования использовано не было.",
            "188 construction_tech # технология строительства 1 328 industry_tech # технология индустрии 1 511 construction_tech # технология строительства 2 511 industry_tech # технология индустрии 2 1285 construction_tech # технология строительства 3 1285 industry_tech # технология индустрии 3 1950 construction_tech # технология строительства 4 1950 industry_tech # технология индустрии 4",
            "В-третьих, обратим внимание на фокусы. Рассмотрим 4 варианта: 1) Стандартное быстрое закрытие паранойи с советником на гражданское строительство и частичной мобилизацией2) Оно же, но дополнительно поставим свободную торговлю, когда будет политка3) Также быстро закрываем паранойю, но частичную мобилизацию берем до советника на гражданское строительство (это будет стоить нам 30 политки)4) Вообще никаких фокусов не берем, только советник и частичная мобилизация.",
            "sov # тэг страны.140 добавить_гражданского_советника # если идти по пути Сталина то политки как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранойи 350 добавить_лимит_фабрик 0.1 # фокус ветки паранойи 350 добавить_бонус_строительства 0.05 # фокус ветки паранойи 350 добавить_фабрики 2 # фокус ветки паранойи 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov140 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная #320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная",
            "В общем и целом, получаем достаточно неплохие бонусы на промышленность, но отнюдь не максимальные (есть еще плакаты на -15% товаров народного потребления, есть множество других бонусов на промышленность в фокусах), но для общего случая этого и не нужно, мы (пока) не собираемся ограничивать игрока в свободе выбора фокусов.",
            "Рассмотрим получившиеся графики зависимости максимума количества военных заводов от количества фабрик которые мы строим (это важно, фабрики от фокусов в счетчик не идут) до переключения на военную промышленность.",
            "Видим что 5% от свободной торговли дают нам лишь 4 завода, при том что 2 завода мы можем получить за 30 политки, просто взяв частичную мобилизацию до советника. Скупают ресурсы у СССР или не так активно чтобы свободная торговля была в плюс, или слишком поздно, когда ресурсы уже нужны для производства.",
            "Также видим что бонусы от фокусов в ветке паранойи дают большое преимущество: 116 заводов против 145 (если мы не берем скидку, то нет смысла брать советника до мобилизации экономики). Честно говоря, не ожидал такой разницы, бонусы не выглядели для меня настолько сильными (+5% к скорости строительства, -2% тнп, 10% к лимиту зданий в провинции, 2 фабрики, 2 завода). Товаров народного потребления дают не так много, +5% это как бонус от свободной торговли, места под строительство у СССР и так хватает. Но в сумме разница выходит почти в 30 заводов.",
            "Ну и самое важное: видим малые производные на достаточно широком участке. В сущности, при строительстве от 28 до 50 фабрик, количество военных заводов на 1 января 1941 года остаётся +/- стабильным.",
            "Как было сказано выше, количество заводов относительно постоянно на участке от 28 фабрик до 50 фабрик, рассмотрим это две точки подробнее:",
            "Если начинать строить заводы раньше, то снаряжения к итоговой дате будет больше (см. площадь под графиками военных заводов. Также стоит учесть что эффективность производства будет нарастать со временем, значит, соотношение снаряжение будет больше соотношения площадей), однако я не уверен, насколько следует переходить к максимизации снаряжения. В игру добавили новую систему снабжения (да, для меня она всё еще новая), теперь она требует строительства пунктов снабжения и железных дорог. Таким образом, нельзя считать экономику эффективной если мы можем произвести снаряжение, но не можем доставить его до фронта. Портальные технологии еще более увеличивают важность логистики, т.к. вся произведенная техника сначала телепортируется в столицу и уже после этого отправляется на фронт вместе с пряниками из карамельной страны.",
            "Помимо оптимизации момента перехода с фабрик на военные заводы, можно также оптимизировать количество инфраструктуры. Можно оценить выгодность инфраструктуры при помощи следующего несложного скрипта (нужен только базовый python, никаких сторонних библиотек):",
            "Отдельно отмечу, что приведенный выше код не учитывает что построенные в процессе фабрики тоже будут строить.",
            "Строительство инфраструктуры, это совсем не то же самое что переход с фабрик на военные заводы. Наиболее выгодное количество инфраструктуры разнится в зависимости от количества слотов под строительство. Да и выгода не всегда велика. Скажем так, оптимальное строительство инфраструктуры потребует от игрока определенных усилий.",
            "Наиболее универсальная оптимизация удалась, но что делать с частными случаями пока не понятно. Баланс между логистикой и количеством снаряжения для разных государств будет разным. Понимание того как этот баланс устроен требует большего понимания самой игры (что требует играть в игру правильно, а это не наш случай).",
            "Оптимизация инфраструктуры выглядит многообещающе, но оценка желания игроков применять это оптимизацию - нет. Также сам процесс нахождения оптимального алгоритма строительства с инфраструктурой выглядит достаточно сложно.",
            "В текущей программе упущен такой момент как возможность аннексии других государств. Этот процесс не обязательно сопровождается войной (а тем более серьезной войной), так что под концепцию довоенной экономики вполне подходит. В программе уже реализован алгоритм реализации контроля провинций, остаётся лишь добавить механизм добавления провинций по тэгу страны владельца или по принципу выделяемых стран (например возвращение польских территорий можно реализовать через добавление к СССР всех провинций Белоруссии и Украины, которые еще не входят в состав Советского Союза). Подобное может потребовать расчета строительства для аннексируемых стран (они же тоже развивают свою экономику параллельно вам), что замедлит расчет, но можно упростить его просто до строительства военных заводов, думаю это не должно добавить много погрешности.",
            "С интерфейсом пока всё совсем не здорово. Идеи приходящие мне в голову или звучат очень сложно или звучат еще менее удобно чем редактирование текстовых файлов. Так что gui пока застопорился.",
            "readme.txt будет обновляться по мере внесения изменений в программу. Если вы считаете что в нём чего-то не хватает, можете мне написать в дискорде или прямо в комментарии под этой статьей. В целом, планирую постепенно работать над читаемостью кода и документацией проекта.",
            "Весь код репозитория распространяется по лицензии MIT, которая гласит следующее: \"делайте с кодом что хотите, но не надо из-за него со мной судиться\". В общем, свободное ПО и всё такое, развлекайтесь, если хотите."
        ]
    },
    {
        "Название статьи": "Не только ORM (NoORM)",
        "Дата публикации": "2024-06-02, 14:46",
        "Автор статьи": "maslyaev ",
        "Статья целиком": [
            "Привет, Хабр! Хочу поделиться самодельной питонской библиотекой (ссылка на GitHub в конце статьи), существенно упрощающей взаимодествие с базами данных.",
            "На настоящий момент популярны два основных способа организовать работу с базой данных:",
            "Воспользоваться «низкоуровневым» интерфейсом СУБД. Например, если у нас база данных в SQLite, можем работать через стандартную библиотеку sqlite3.",
            "Задействовать какой-нибудь из популярных ORM-ов, например, SqlAlchemy.",
            "Первый вариант хорош для небольших скриптов, простеньких микросервисов и прочих изделий, объём исходного кода которых не превышает пары тысяч строк. В больших и серьёзных проектах хождение в базу данных через низкоуровневые интерфейсы становится мучительным, и народ предпочитает работать через ORM. Но ORM это тоже не сахар с мёдом. На примерах из туториала любой ORM выглядит как сбывшаяся мечта, но по мере роста функциональности системы, объёма базы данных и нагрузки выясняется, что ORM-ное счастье не было бесплатным. Ранее здесь я уже поворчал о том, что задача \"Object-Relational Mapping\" решения не имеет, и что-то с этим нужно делать. Предложенный тогда вариант просто взять и посадить себя на без-ORM-ную диету был явно непрактичным, поэтому в серьёзных проектах ничего другого не оставалось, как продолжать есть этот кактус, но при этом не прекращать попыток придумать альтернативу.",
            "Постепенно выкристаллизовалась идея NoORM (\"Not only ORM\", по аналогии с \"Not only SQL\"):",
            "Мы не объявляем священную войну ORM-ам, а гармонично с ними сосуществуем, давая альтернатианые решения там, где ORM-ы традиционно приносят головную боль.",
            "Мы не пытаемся сделать плюс ещё один тысяче первый, но на этот раз самый окончательно правильный ORM. Чётко осознаём, что задача \"ORM\" не имеет решения.",
            "Технология должна быть применима не только на маленьких поделках и DIY-проектах, но и на больших и сложных бэкендах.",
            "Фокус на улучшение developer experience. Все эти удобняшки современных IDE – атодополнение, подсветка синтаксиса, переход к объявлению – всё это очень сладко, вносит комфорт и повышает продуктивность.",
            "Технология должна быть подобна истинному дао – делать правильные вещи простыми и естественными, сопротивляясь использованию корявых и неэффективных решений.",
            "Назовём клиентом программу, общающуюся с сервером СУБД. Проигнорируем тот факт, что этот наш клиент сам, возможно, для кого-то является сервером. Взаимодействие с СУБД построено таким образом, что сервер в качестве запроса принимает, по сути, текст программы (SQL это тоже язык программирования, даже не пытайтесь спорить), исполняет её, и отдаёт назад результат. Структура результата зависит от текста запроса, а также от схемы базы данных.",
            "Весьма странный API, не правда ли? Нетипичный. Какой-то текст на вход, какая-то табличка (или число, или просто ничего) на выход – вот и вся схема, вот и весь контракт. Этим, конечно, достигается потрясающая функциональная гибкость, но с точки зрения кода клиента, написанного на «обычном» языке программирования, это сущий кошмар.",
            "Какое-то невнятное \"Any\"... В таком простеньком случае это можно и пережить, но по мере развития проекта код неизбежно превращается в отвратительную смесь языков программирования, в которой код на SQL разбросан по основной логике в виде строковых литералов. В таком сложно разбираться, такое трудно развивать, такое мучительно сопровождать.",
            "ORM-ы, собственно, нужны как раз для того, чтобы сделать взаимодействие с базой данных более естественным для того языка, на котором реализована логика приложения:",
            "Взаимодействие с БД через ORM можно схематично изобразить так:",
            "Примечательно здесь то, что работа с базой данных идёт через персистентные объекты, являющиеся экземплярами «модельных» классов, описывающих структуру БД. Эти персистентные объекты умеют себя прочитать из базы и в неё себя записать. Они живут внутри открытой сессии. И ещё эти объекты умеют «лениво» дотягивать из базы связанные с ними другие персистентные объекты. Эти самые персистентные объекты – корень всех проблем:",
            "По сути, это передача мутабельного объекта в другой процесс. Безобразно тупая затея. Мы запросили сущность «пользователь Вася» из базы данных в процесс своего бэкенда, и теперь где у нас теперь мастер-копия? Как мы их собираемся синхронизировать, в какой момент, и что собираемся делать с возможными коллизиями?",
            "Что случается с живущими в сессии объектами когда сессия закрывается? Что если они продолжают быть нужны в логике приложения? Что если эта логика продолжает считать, что это по-прежнему нормальные объекты, принадлежащие живой сессии?",
            "Невозможно найти единственно правильный баланс между eager- и lazy-загрузкой. Если увлекаемся lazy, получаем проблему N+1, и всё начинает страшно тормозить. Если увлекаемся eager, на каждый невинный чих ORM пытается вычитать полбазы, и тоже всё тормозит. Короче, у нас две педали, но обе они педали тормоза.",
            "Идея персистентных объектов – тяжёлое наследие платоновской концепции «Мира идеальных сущностей». Поначалу нам может показаться соблазнительно один раз на веки вечные и на все случаи жизни реализовать класс Person, но потом внезапно оказывается, что с точки зрения сервиса аутентификации пользователей Person это одно, с точки зрения бухгалтерии другое, а с точки зрения HR третье, и эти точки зрения местами противоречат друг другу. Мы пытаемся создать класс Person, экземпляры которого будут удобны и полезны везде, но в итоге у нас получается корявый, огромный и чрезвычайно капризный программный монстр, жрущий как аппаратные ресурсы, так и рабочее время сотрудников. Даже если база данных одна общая на всех, даже если таблица \"persons\" там тоже одна, всё же для разных целей нам бывает удобно делать совсем разные, порой весьма причудливые SELECT-ы. Одна из ключевых идей NoORM – отказ от использования персистентных объектов. Не модельных классов, заметьте, а именно персистентных объектов.",
            "Создаём в своей программе дополнительный слой, и тем самым рассмотренный ранее «весьма странный API» превращаем в обычный:",
            "Для любого императивного (и тем более функционального) языка программирования самая естественная в мире вещь это функция, которую можно вызвать с известно какими параметрами, и которая вернёт результат известно какого типа. С точки зрения кода-потребителя программный интерфейс БД выглядит как-то так:",
            "Что мы здесь видим:",
            "У нас есть модуль db_api, в котором есть модуль users, в котором есть функция get_users.",
            "Эта функция принимает на вход соединение с базой данных и отдаёт список объектов DbUser, у которых есть атрибуты email, id и username.",
            "Всё это замечательно дружит с удобняшками IDE, линтерами и mypy.",
            "Мы не свинячим SQL-запросами ровным слоем по всей кодовой базе, а собираем их в одном месте – в модуле db_api. В результате код, реализующий основную логику приложения становится более компактным, понятным, гладким, шелковистым и приятным на ощупь как котёнок.",
            "Что касается DbUser, то это никакой не персистентный объект, никакая не платоновская идеальная сущность, а всего лишь dataclass, в который заворачивается результат конкретного запроса. Вот как это выглядит в модуле db_api.users:",
            "У вас может возникнуть резонный вопрос: а не закончится ли это тем, что у нас в \"db_api\" будут сотни и тысячи каких-то маловразумительных датаклассов и функций, и ориентироваться в этом станет совсем невозможно? Честно скажу, у меня самого были такие опасения, когда в порядке эксперимента я взялся переводить с ORM на NoORM один приличного объёма сервис, который много и разнообразно общается с базой данных. Однако обошлось. Более того, стало значительно легче находить ответы на вопросы о том, как, где и для чего используются конкретные таблицы и поля базы данных. Стал проще рефакторинг. Избавившись от персистентных объектов, избавились от необходимости держать открытой сессию на протяжении всей обработки клиентского запроса. Плюс абсолютно предсказуемое поведение коммита – в базу пишется только то, что мы хотим в неё записать здесь и сейчас, и нет никаких персистентных объектов, которые по какой-то неясной причине тоже решили пристроиться к этому коммиту. Ну и, самое сладкое, все \"N+1\" стали видны как на ладони – если мы в потенциально длинном цикле вызываем какую-то функцию, в которую параметром передаём соединение с БД, мы же не просто так его туда передаём, а, очевидно, для того, чтобы сходить в БД столько раз, сколько раз прокрутится цикл.",
            "ORM это не только плохие капризные портящие нам кровь персистентные объекты, но и множество восхитительных удобств, от которых нет смысла отказываться:",
            "Модель структуры базы данных. Можно держать структуру базы данных в голове, можно нарисовать её тушью на ватмане, можно в разбросанных по столу черновиках, можно в заметочках в ноушене или на страничках в конфлюенсе, а можно в питонском коде под гитом. Последний вариант мне кажется самым симпатичным.",
            "Миграции. Они в любом случае боль, но ORM-мы умеют облегчать страдания. Глупо этим пренебрегать.",
            "Я тут сказал много злых слов про персистентные объекты, но если их не пускать в «боевой» код, а использовать только для генерации тестовых данных, то там они чудо как хороши. По сути, едва объявив структуру данных, мы сразу забесплатно «из коробки» получаем для неё реализованный CRUD. Когда нам абсолютно наплевать и на производительность, и на масштабируемость, и на конкурентное исполнение, тогда персистентные объекты – прекрасное решение.",
            "DB-API-функция извлечения данных, работающая через SqlAlchemy выглядит так:",
            "Знаю, некоторым в тягость такой стиль написания SQL, но нельзя не признать, что у него есть свои преимущества, особенно на простых запросах.",
            "По ходу «опытной эксплуатации» этой технологии выработались несколько рекомендаций, которых полезно придерживаться:",
            "Не надо нагружать бизнес-логикой объекты, возвращаемые DB API. Пусть это будут просто датаклассы или даже namedtuple-s. Впрочем, никто не мешает реализовать несколько дополнительных свойств, если их вычисление в SQL-запросе по каким-то причинам затруднительно.",
            "Объявление этих датаклассов – непосредственно перед функциями, которые их будут возвращать. Точно не в отдельном модуле. Между SQL-запросом и тем местом, где определяется структура его результата должно быть не далеко ходить. Идеально, если объявление датакласса вместе с SQL-запросом помещаются одновременно на один экран.",
            "С именами этих датаклассов сильно упариваться не нужно, но удобно, когда они выделяются визуально – когда мы видим, что имя типа переменной начинается с \"Db\", мы сразу понимаем, что значение взялось из базы данных.",
            "Удобно, когда выработано некоторое соглашение об именовании DB-API-функций. Мы используем префиксы \"get_\", \"ins_\", \"upd_\", \"del_\", \"upsert_\". Для функций, в которых принудительно отключается автокоммит, используем суффикс \"_no_commit\".",
            "Когда DB-API-функций мало, их можно держать в одном модуле \"db_api.py\". Если их становится больше, распиливаем этот модуль по функциональным областям, например, \"db_api/users.py\", \"db_api/orders.py\", \"db_api/warehause.py\".",
            "Если в монорепозитории живёт несколько подсистем, есть смысл иметь один общий модуль \"db_api\", но специфические вещи вынести в \"db_api\"-модули подсистем.",
            "Если какая-то часть системы по своей сути является скопищем SQL-запросов (например, коллекция даталоадеров для GraphQL), оставьте эти запросы где они есть. Просто избавьтесь от персистентных объектов, но ни в какое \"db_api\" не выносите.",
            "В принципе, NoORM-стиль можно практиковать и без дополнительной библиотеки, но тогда нам приходится каждый раз писать одинаковый код, вызывающий исполнение запроса и преобразующий результат в датаклассы. Это раздражает и утомляет. Кроме того, выгода от вынесения доступа к базе в отдельные DB-API-функции становится менее очевидной, и в результате всё заканчивается тем, что мы снова начинаем свинячить SQL-код где попало.",
            "Библиотека предельно проста в использовании. Всего лишь пять декораторов – sql_fetch_all для изготовления функции, возвращающей список объектов, а также sql_one_or_none, sql_scalar_or_none, sql_fetch_scalars и sql_execute для сами угадайте чего. Плюс немножко дополнительной функциональности, в частности, реестр функций, автоматически собирающий метрики. Всё это реализовано для:",
            "SQLite – через стандартную библиотеку sqlite3 и async через aiosqlite.",
            "Postgres – sync через psycopg2 и async через asyncpg.",
            "MySQL/MariaDB – sync через PyMySQL и async через aiomysql.",
            "Для всего остального, с чем работает SqlAlchemy, если выбран вариант «NoORM через ORM». Тоже в исполнениях sync и async.",
            "Если нужно что-то ещё, пишите в гитхаб в Issues. Руки чешутся добавить адаптеры для Mongo, но пока удаётся себя сдерживать. С интересом смотрю в сторону PonyORM, и если кому-нибудь это надо, могу добавить. Адаптера для Django нет и не будет, поскольку, к сожалению, работа через персистентные объекты там безальтернативна.",
            "Обещанная ссылка на гитхаб: здесь.",
            "P.S. «НоуОуАрЭм» – язык сломаешь, поэтому прижился вариант произношения /nuːrm/ – «нурм», «нурмализация», «сделаем сразу нурмально»."
        ]
    },
    {
        "Название статьи": "Создание масштабируемых RL систем с Ape-X",
        "Дата публикации": "2024-06-01, 11:56",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Ape-X представляет собой подход к обучению с подкреплением, разработанный для использования в масштабируемых распределенных системах.",
            "Основная идея Ape-X заключается в разделении ролей на акторов, которые взаимодействуют с окружением и собирают данные, и учеников, которые используют эти данные для обучения модели. Такое разделение позволяет ускорить процесс обучения и предотвратить заучивание субоптимальных политик.",
            "Акторы являются агентами, которые взаимодействуют со своими экземплярами среды. Каждый актор выбирает действия, основываясь на общей нейронной сети, которая используется для предсказания действий. Акторы аккумулируют свой опыт в общей памяти повторного воспроизведения. Этот опыт включает состояния, действия, вознаграждения и следующие состояния, полученные в результате взаимодействия со средой.",
            "Ученики занимаются обучением модели. Они извлекают образцы опыта из общей памяти воспроизведения и обновляют параметры нейронной сети на основе этих данных. За счёт того, что обучение происходит вне среды, ученики могут параллельно работать с большим количеством данных.",
            "Приоритетная буферизация опыта (в дальнейшем PER) была предложена для решения проблемы равномерного распределения опыта в стандартных методах опыта с повторным воспроизведением. В классическом опыте с повторным воспроизведением все записи имеют одинаковую вероятность быть выбраны для обучения, что может привести к медленному обучению и субоптимальным результатам. PER решает эту проблему, присваивая приоритеты каждому опыту на основе их важности.",
            "Если коротко, то PER работает так:",
            "Оценка ошибки TD:",
            "Каждому опыту присваивается приоритет на основе ошибки TD.",
            "Опыт с большей ошибкой TD получает более высокий приоритет, т.к он содержит больше информации для обучения модели.",
            "Обновление приоритетов:",
            "При каждом добавлении нового опыта или обновлении модели, приоритеты пересчитываются.",
            "Это позволяет системе адаптироваться к новым данным и изменяющимся условиям среды.",
            "Выбор опыта для обучения:",
            "Для выборки опытов используется стохастический процесс, основанный на приоритетах. Опыты с более высоким приоритетом имеют большую вероятность быть выбраны.",
            "Тем не менее, чтобы избежать переобучения на небольшом наборе данных, в процесс добавляется некоторая степень случайности.",
            "Коррекция смещения:",
            "Использование приоритетной выборки может ввести смещение в процесс обучения. Для его коррекции используется важностное взвешивание, что позволяет уменьшить вероятность ошибки из-за неравномерного выбора данных.",
            "PER позволяет быстрее выявлять и корректировать ошибки модели, ускоряя процесс сходимости. В Ape-X это мастхев, т.к множество параллельных акторов генерируют большие объемы данных, и важно по максимуму использовать эту информацию.",
            "Идем к другим не менее важным механизмам.",
            "Централизованная память воспроизведения:Все акторы отправляют свой опыт в централизованную память воспроизведения. Это позволяет ученикам иметь доступ к большему количеству данных и использовать их для более качественного обучения модели. Коммуникация между акторами и памятью воспроизведения осуществляется пакетами.",
            "Обучение вне политики:Ape-X использует обучение вне политик. Каждый актор может иметь свою стратегию исследования среды, что расширяет разнообразие получаемого опыта.",
            "Будем использовать PyTorch и Ray. PyTorch имееь средства для создания и обучения нейронных сетей, а Ray помогает управлять распределенными вычислениями и координировать работу множества акторов и учеников. Пошагово настроим окружения для Ape-X, используя AWS EC2 или локальные машины.",
            "Будем использовать Python 3.8 и более новые версии библиотек PyTorch и Ray. Для начала, создадим виртуальное окружение:",
            "Далее, установим необходимые библиотеки:",
            "Для масштабирования системы используем AWS EC2. Создаем несколько инстансов, которые будут работать в распределенной среде. Для этого заходим в консоль управления AWS, выбираем EC2 и создаем новый инстанс с нужными характеристиками (например, t2.large для акторов и t2.xlarge для учеников).",
            "После создания инстансов, подключаемся к ним через SSH:",
            "Устанавливаем необходимые пакеты на каждом инстансе:",
            "Окружение готово и мы готовы создавать конфигурационные файлы и запускать Ape-X. Начнем с создания файла config.py, который будет содержать основные параметры:",
            "Затем создадим файл train.py для запуска процесса обучения:",
            "Для запуска на одном узле достаточно выполнить команду:",
            "Для многоузловой конфигурации нужно запустить Ray на каждом узле:",
            "После этого можно запустить train.py, и все узлы будут участвовать в обучении.",
            "С Packer можно автоматизировать создание образов машин. Создадим файл конфигурации packer.json:",
            "Запускаем Packer для создания образа:",
            "Теперь есть готовый образ, который можно использовать для создания новых инстансов EC2 с уже установленными необходимыми библиотеками.",
            "Для запуска Ape-X на одной машине можно юзать этот скрипт:",
            "Для многоузловой конфигурации необходимо настроить несколько экземпляров и настроить взаимодействие между ними.",
            "Теперь мы получили полностью настроенное и работоспособное окружение для реализации Ape-X.",
            "В завершение предлагаю посетить открытые уроки по Reinforcement Learning от OTUS:",
            "13 июня: Применение нейросетевых моделей в обучении с подкреплением. Записаться",
            "17 июня: Построение торгового агента на базе фремворка FinRL. Записаться"
        ]
    }
][
    {
        "Название статьи": "Машинное обучение с Python и TensorFlow на Windows. Быстрый старт",
        "Дата публикации": "2024-06-06, 10:10",
        "Автор статьи": "Sber ",
        "Статья целиком": [
            "Словосочетание «машинное обучение» становится всё более значимым с каждым годом и проникает во все возможные сферы жизни, а с появлением в открытом доступе таких нейронных сетей как Chat GPT [1] интерес к машинному обучению стал высок как никогда. Но при этом многих отпугивает сложность создания своих систем на основе машинного обучения, потому что нужно одновременного использовать и настраивать много разных инструментов разработки.",
            "Поэтому я хочу представить вашему вниманию максимально простую инструкцию для быстрого погружения в мир машинного обучения. Инструкция ориентирована в первую очередь на начинающих программистов, мы будем применять Python 3 [2] с библиотекой TensorFlow [3]. Это лучший выбор для начинающих из-за простоты языка и большого сообщества разработчиков, использующих TensorFlow.",
            "Ваш процессор должен поддерживать AVX-инструкции [4], а видеокарта должна поддерживать архитектуру CUDA начиная с версии 3.5 (список подходящих видеокарт [5]):",
            "AVX (Advanced Vector Extensions) — это расширение системы команд x86-архитектуры для микропроцессоров Intel и AMD, предложенное Intel в марте 2008 года.",
            "CUDA (Compute Unified Device Architecture) — это программно-аппаратная архитектура параллельных вычислений, разработанная компанией NVIDIA. Она позволяет существенно увеличить вычислительную производительность благодаря использованию графических процессоров (GPU).",
            "Далее я буду говорить о TensorFlow 2.10, так как последующие версии требуют установки Windows Subsystem for Linux (использование WSL усложнит установку и ограничит количество подходящих версий Windows). TensorFlow 2.10 будет работать на Windows 7 и более новых версиях операционной системы. Версия Python должна быть от 3.9 до 3.11.",
            "Нужно начать с пакета Microsoft Visual C++ для Visual Studio 2015, 2017 и 2019. Далее установите Python с минимальным набором библиотек и менеджером пакетов — лучше всего для этого подходит Anaconda [6]. Она содержит в себе дистрибутив языка Python, систему управления средой разработки Anaconda Navigator (визуальный менеджер пакетов, управление дополнительным софтом и т. д.) и базовый набор библиотек. Но если вы не фанат визуальных сред разработки или не хотите ставить лишние программы на компьютер, то можно ограничиться Miniconda (это то же самое, что и Anaconda, но без лишних программ, только библиотеки и консольный менеджер пакетов Conda) [7].",
            "После установки Anaconda нужно запустить Anaconda Powerschell Prompt и выполнить в консоли команду:",
            "Она создаст новую среду conda (в рамках одной среды можно использовать определённый набор библиотек, это полезно, если для разных проектов нужны разные версии библиотек) и установит для неё Python 3.9.",
            "Затем с помощью команды activate активируем созданную выше среду (deactivate возвращает базовую среду):",
            "Теперь нужно установить свежие драйверы для вашей видеокарты [8], а если они у вас установлены, то можно переходить к установке пакетов CUDA и cuDNN. Это можно сделать выполнив одну команду:",
            "Указанные выше версии пакетов лучше не менять, так как могут быть проблемы с совместимостью библиотек. Выполнение этой команды выглядит так:",
            "Для корректной работы TensorFlow требует свежая версия pip (это менеджер пакетов). Обновите его:",
            "TensorFlow можно установить и через conda (или через Anaconda Navigator), но разработчики TensorFlow рекомендуют использовать именно pip. Устанавливаем:",
            "Нужно удостовериться, правильно ли всё настроено и установлено. Для этого выполните в консоли или вашей среде разработки код:",
            "Очень удобно для разработки использовать IDE Spyder, если её установить с помощью Anaconda Navigator для нужной среды. Выглядит установка так (выбираем среду и нажимаем установить):",
            "Обратите внимание на текущую среду, ибо именно в неё будет установлено выбранное ПО. После установки вы можете запустить Spyder из Навигатора от имени выбранной среды (это значит, что будут доступны библиотеки, которые загружены именно для среды tf).",
            "Теперь выполняем код в консоли, который я приводил выше:",
            "Если всё правильно сделано, то система должна вывести список доступных для TensorFlow устройств (в моем случае это видеокарта RTX 3060).",
            "Пришло время создать первый проект, использующий библиотеки машинного обучения. Для теста возьмём стандартный проект классификации ирисов [9]. Создайте скрипт с расширением .py и таким кодом:",
            "По ссылке [9] скачайте файл IRIS.csv из раздела «Входные данные» и разместите его в одной папке со скриптом. Этот файл содержит исходные данные для обучения модели. Затем запустите выполнение скрипта и дождитесь его завершения:",
            "Если скрипт отработал корректно, в конце вы увидите график обучения модели в разделе Plots.",
            "Примечание: при первой попытке выполнения этого скрипта интерпретатор выдаст ошибку об отсутствии необходимых библиотек. При реализации своих проектов вам придётся использовать и устанавливать различные библиотеки. Устанавливать их можно разными способами консольными менеджерами пакетов (pip или conda) или визуальным менеджером пакетов Anaconda Navigator. Старайтесь использовать только один способ, иначе может возникнуть путаница и конфликты между разными версиями библиотек. Я обычно использую визуальную установку через Anaconda Navigator, это позволяет более наглядно контролировать набор установленных пакетов и их версии. Пример визуальной установки пакета для программы классификации ирисов:",
            "Для установки пакета активируем вашу среду (tf в моём случае), выбираем раздел «Неустановленные», в поле поиска (справа) вводим название требуемой библиотеки, ставим на ней галочку и нажимаем Apply. После установки всех необходимых библиотек, которые импортируются в начале скрипта, программа классификации ирисов должна работать.",
            "На этом этапе можно поздравить вас с первым запуском проекта на основе TensorFlow. Теперь вы точно готовы к реализации своих проектов с применением технологий машинного обучения.",
            "Желаю вам успехов во всех ваших будущих проектах и спасибо за внимание. До новых встреч.",
            "https://openai.com/",
            "https://www.python.org/",
            "https://www.tensorflow.org/",
            "https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX",
            "https://developer.nvidia.com/cuda-gpus",
            "https://docs.anaconda.com/free/anaconda/install/windows/",
            "https://docs.anaconda.com/free/miniconda/",
            "https://www.nvidia.com/download/index.aspx",
            "https://www.kaggle.com/code/venkatkrishnan/iris-data-tensorflow-neural-network/notebook"
        ]
    },
    {
        "Название статьи": "Введение в gRPC: Основы, применение, плюсы и минусы. Часть I",
        "Дата публикации": "2024-06-05, 23:59",
        "Автор статьи": "0xN1ck ",
        "Статья целиком": [
            "gRPC (gRPC Remote Procedure Call) — это современная высокопроизводительная фреймворк для удаленных вызовов процедур, разработанная Google. gRPC позволяет клиентам и серверам общаться напрямую, используя протокол HTTP/2 и Protocol Buffers (protobuf) в качестве языка описания интерфейсов (IDL). Эта технология предоставляет возможность эффективного взаимодействия между различными компонентами распределенных систем, независимо от языка программирования.",
            "gRPC основывается на архитектуре клиент-сервер и поддерживает множество языков программирования, включая C++, Java, Python, Go, Ruby и многие другие. В основе gRPC лежат следующие ключевые компоненты:",
            "Protocol Buffers (protobuf): Это язык описания данных и инструмент сериализации, который используется для определения сервисов и их методов, а также для обмена данными между клиентом и сервером. Protobuf позволяет описывать структуру данных в специальном .proto файле, который затем компилируется в исходный код для выбранного языка программирования.",
            "HTTP/2: Протокол транспортного уровня, который обеспечивает мультиплексирование запросов, сжатие заголовков, и другие улучшения производительности по сравнению с HTTP/1.x. HTTP/2 позволяет отправлять несколько запросов через одно соединение, что значительно уменьшает задержки и улучшает пропускную способность.",
            "Stub-генерация: gRPC автоматически генерирует клиентские и серверные stub'ы на основе protobuf-файлов, что упрощает процесс интеграции и уменьшает количество шаблонного кода. Клиенты используют сгенерированные stub'ы для вызова методов на сервере так, как если бы они вызывали локальные функции.",
            "gRPC широко используется для построения распределенных систем и микросервисных архитектур. Вот несколько типичных сценариев его применения:",
            "Микросервисы: В больших системах, где микросервисы взаимодействуют друг с другом, gRPC обеспечивает эффективное и надежное общение с низкой задержкой. Это особенно полезно в системах, где важно минимизировать время отклика и обеспечить высокую пропускную способность.",
            "Взаимодействие между разными языками: Благодаря поддержке множества языков, gRPC позволяет разрабатывать системы, где компоненты написаны на разных языках программирования, легко взаимодействуя между собой. Это упрощает интеграцию различных технологий и позволяет выбирать наиболее подходящий язык для каждого компонента системы.",
            "Реализация API: gRPC идеально подходит для создания высокопроизводительных API, где критичны низкие задержки и высокая пропускная способность. API, реализованные с помощью gRPC, могут использоваться как внутри организации, так и предоставляться внешним пользователям.",
            "Мобильные и IoT приложения: gRPC отлично подходит для мобильных и IoT приложений благодаря своей эффективности и низкому потреблению ресурсов. HTTP/2 обеспечивает минимальное использование сетевых ресурсов, что особенно важно для устройств с ограниченными возможностями.",
            "Высокая производительность: Благодаря использованию HTTP/2 и protobuf, gRPC обеспечивает низкие задержки и высокую пропускную способность. Это делает его идеальным выбором для высоконагруженных систем.",
            "Ясно определенные интерфейсы: Использование protobuf для описания сервисов и сообщений обеспечивает четкую контрактность и минимизацию ошибок на этапе компиляции. Это упрощает процесс разработки и интеграции различных компонентов системы.",
            "Поддержка различных языков: gRPC поддерживает множество языков программирования, что позволяет интегрировать компоненты, написанные на разных языках, в единую систему. Это упрощает использование существующего кода и технологий.",
            "Би-ди стриминг: gRPC поддерживает не только однонаправленные и двунаправленные потоки, но и полный дуплекс, что позволяет реализовать сложные сценарии взаимодействия. Это особенно полезно для приложений, требующих постоянного обмена данными в реальном времени, таких как чаты или системы мониторинга.",
            "Автоматическая генерация кода: gRPC генерирует клиентские и серверные stub'ы, что упрощает разработку и снижает количество шаблонного кода. Это сокращает время разработки и уменьшает количество ошибок, связанных с ручным написанием кода.",
            "Крутая кривая обучения: Для новичков gRPC может показаться сложным из-за необходимости освоения protobuf и специфических особенностей HTTP/2. Однако, с практикой и доступными ресурсами, обучение становится легче.",
            "Ограниченная поддержка браузеров: gRPC не поддерживается большинством браузеров напрямую, что требует использования дополнительных прокси-серверов или gRPC-Web. Это добавляет дополнительную сложность при создании веб-приложений, использующих gRPC.",
            "Зависимость от protobuf: Использование Protocol Buffers как основного формата сериализации может быть ограничением для тех, кто предпочитает другие форматы, такие как JSON или XML. Хотя protobuf предлагает высокую производительность и компактность, он требует дополнительных шагов для сериализации и десериализации данных.",
            "Инфраструктурные требования: Для эффективного использования gRPC необходимо обеспечить поддержку HTTP/2 на уровне сетевой инфраструктуры, что может потребовать дополнительных настроек и ресурсов. Это может стать препятствием для некоторых организаций, особенно если их существующая инфраструктура не поддерживает HTTP/2.",
            "gRPC — это мощный инструмент для построения высокопроизводительных распределенных систем и микросервисов. Он обеспечивает эффективное общение между сервисами, поддерживает множество языков программирования и предлагает ясные и контрактно-ориентированные интерфейсы. Однако, как и любая технология, gRPC имеет свои недостатки и требует определенных усилий для освоения и интеграции.",
            "Если вы строите сложную распределенную систему или ищете способ улучшить взаимодействие между микросервисами, gRPC может стать отличным выбором, обеспечивая высокую производительность и надежность. Важно тщательно взвесить плюсы и минусы этой технологии и оценить ее применимость к вашему проекту, чтобы получить максимальную пользу от использования gRPC."
        ]
    },
    {
        "Название статьи": "Майним крипто-пойнты с помощью цветового автокликера на Python",
        "Дата публикации": "2024-06-05, 20:22",
        "Автор статьи": "temabed ",
        "Статья целиком": [
            "Привет, Хабр! Я продолжаю цикл небольших статей для энтузиастов и начинающих программистов о том, как интересно, а иногда и с выгодой, можно применять свои навыки.",
            "В последнее время широкое распространение получили разные крипто-проекты, которые обещают пользователям материальные вознаграждения за определенную активность. Особенно актуальны они стали после успеха Notcoin, который очень неплохо отблагодарил своих пользователей. Не буду углубляться как это всё работает, откуда там деньги, и много ли среди таких проектов скама (да), нас интересует лишь тот момент, что большинство этих проектов можно автоматизировать, а значит не терять драгоценное время.",
            "Да, большинство из таких проектов имеют достаточно примитивную механику, но при этом отнимают слишком много времени, обещая лишь туманную перспективу возможного заработка. А нас, взрослых людей, такой расклад не устраивает, поэтому за нас должен работать хотя бы робот.",
            "Сегодня на операционном столе будет приложение в телеграм, которое в будущем должно стать полноценной децентрализованной биржей, но в настоящее время позволяет новым пользователям лишь зарабатывать внутренние очки. Хочу сразу сказать, что даже руководитель проекта честно заявляет, что обмен этих очков на что-то более материальное в будущем вовсе на обязателен, имейте это ввиду. Я его тоже не рекламирую, и уж тем более не даю никаких финансовых рекомендаций, он нам нужен лишь в качестве примера.",
            "Механика мини игры внутри этого приложения такая: в течение небольшого времени сверху падают цветочки, которые нужно ловить простым нажатием. За них и начисляют очки. Однако, недавно в игре появились еще и бомбочки, при нажатии на которые счет обнуляется.",
            "В связи с нововведением в виде бомбочек бездумный автокликер использовать стало невозможно. Поэтому я делал попытку использования для убийства цветков компьютерного зрения. Но эксперимент вышел так себе, терминатор получился достаточно убогим, и мир ему захватывать еще не скоро.",
            "А сегодня рассмотрим механику всё того же самого простого автокликера, но определяющего цвет пикселя перед нажатием. Который делает клик только при условии, что пиксель в нужном диапазоне. Нам понадобятся всего три библиотеки.",
            "Pyautogui будет управлять мышью и определять цвет пикселя, на который наведен курсор. Keyboard нужен для назначения горячих клавиш, а time для временной задержки.",
            "Назначим три переменные.",
            "Work будет отвечать за работу кода в бесконечном цикле. Если True- выполняется следующая функция, если False - всё останавливается. Points содержит координаты двадцати точек, изначально равные нулю, так как мы их будем назначать после запуска программы. А index нужен, чтобы демонстрировать пользователю, какую по счету точку он назначает. И сразу напишем функцию присваивания координат точкам для кликов.",
            "Эта функция будет срабатывать при нажатии на горячую клавишу и последовательно присваивать координаты каждой из двадцати точек. Координаты будут браться с позиции, на которую в этот момент наведен курсор мыши.",
            "Следующая функция будем изменять значение переменной work и срабатывать так же при нажатии горячей клавиши. Она будет ставить на паузу работу автокликера.",
            "Теперь функция самого кликера.",
            "В цикле for программа пробегает по всем координатам для клика мышью, но на каждой итерации делается снимок экрана, и перед нажатием определяется цвет пикселя, на который указывает курсор. Важно, что метод getpixel возвращает кортеж с тремя числами, которые характеризую цвет по системе RGB. Методом тыка я пришел к выводу, что для определения цветка нам достаточно, чтобы первое число в картеже было в диапазоне от 134 до 218, а вот если второе и третье число равны, это уже свидетельствует о сером оттенке, в которые выкрашены бомбы, и на такое кликать не стоит. Соответствующие условия я и передал в конструкцию if.",
            "Ну и основная функция, которая будет записывать координаты для кликов нажатием на горячую клавишу \"=\" и запускать кликер нажатием на \"~\".",
            "Вот такая простенькая, но многофункциональная программа получилась. Код целиком:",
            "Применить такой код сможет любой, даже не обладая навыками программирования, и не только по назначению, указанному в статье. Вариантов масса, включая самые обычные компьютерные игры.",
            "Поэкспериментировать же с озвученным проектом возможно только лишь в случае, если у вас есть инвайт (пригласительная ссылка). К сожалению, у меня они уже закончились, поэтому поспрашивайте у знакомых либо в моем телеграм-канале под соответствующими постами.",
            "И последнее, эффективность описанного кода сейчас зависит только от стратегии, по которой вы разместили точки для кликов. У меня результат был так себе, но я нашел самый эффективный способ для фарма в таких простых играх - это дети. Хотя это уже читерство, согласен."
        ]
    },
    {
        "Название статьи": "Магия динамического маппинга. Реализация универсальной обработки файлов нефиксированной структуры на Python",
        "Дата публикации": "2024-06-05, 15:58",
        "Автор статьи": "spectr_dev ",
        "Статья целиком": [
            "Привет! На связи Никита Ильин из Spectr, Backend-разработчик с опытом более 5 лет.",
            "Один из проектов, с которым мы работаем, — IBP-платформа для планирования и прогнозирования спроса и продаж в ритейле. В статье поговорим о конкретной реализации для одной из задач в рамках этой платформы на Python и Django. При этом сама концепция может быть реализована абсолютно на любом фреймворке или платформе: Spring, .NET, Laravel.",
            "Мы разрабатываем IBP-платформу для крупной корпорации, где на основе данных, которые поступают из смежных систем, строятся прогнозы и аналитика. И одна из областей — работа с огромным количеством файлов из внешних источников: чтение, обработка, загрузка и запись всех этих данных в БД. При этом существует большое количество различных источников и форматов этих файлов.",
            "Глобально задача состоит в том, чтобы осуществить загрузку из внешних источников в единую внутреннюю систему для последующего анализа и прогноза.",
            "Информация об источниках.",
            "Сейчас в системе около 350 источников. При этом одновременно может поступать до 100 штук новых.",
            "Важное уточнение: один источник — это файл уникальной структуры. Если названия столбцов различаются, то это уже новый файл, новый источник данных.",
            "Информация о файле. Это обычный классический csv-формат. Разделителем может быть либо «;» либо «,». Число колонок — от 5 до 200, но распарсить нужно только количество, которое обозначено в техническом задании. В нашем случае доходило до 40 колонок, все остальное — системные поля. Число строк всегда большое — от 5 млн до 20 млн.",
            "Ниже представлен классический пример файла с условными обозначениями.",
            "Отлично, вся информация о данных у нас на руках! Что же делаем? Тут возможны два пути.",
            "Первая идея, которая у нас возникла, — статическая реализация или, по-другому, «решение в лоб». При такой реализации мы для каждого источника пишем свой парсер и применяем его — эффективно и быстро. Поговорим о преимуществах чуть подробнее.",
            "Быстрая разработка. Раньше уже существовало какое-то количество источников и мы уже знали, как работать по этому пайплайну: без лишних оптимизаций и размышлений. То есть мы приоритезируем источники, отдаем наиболее важные по значимости в первую очередь, а сами занимаемся другими.",
            "Работает надежно, как швейцарские часы. Когда мы пишем парсер на конкретный источник, мы можем написать на него тест. И перед тем как выкатывать, надо посмотреть — а точно ли поведение такое же, как ожидается? Тут мы говорим про стабильность и проверенность того, что наш парсер работает.",
            "Без непредвиденных side-эффектов. Все парсеры строго императивны и изолированы и, благодаря этому, более стабильны. Ведь когда мы говорим про enterprise-разработку, мы точно не хотим, чтобы один наш парсер сломал всю систему.",
            "Сходить на нужный внешний сервер по SFTP.",
            "Именно SFTP, потому что это спецификация нашей задачи, мы так договорились, нам так комфортно общаться между серверами.",
            "Забрать файл.",
            "Применить парсер.",
            "Сохранить в БД.",
            "Работа аналитика. Сверка с локальной структурой БД и проектирование перевода из внешнего имени во внутреннее. Нам нужно узнать, что, где и как хранится, какая между всем этим связь. Данные формируются в виде спецификации, требований и отдаются разработчику.",
            "Работа Backend-разработчика.",
            "Написание кода для валидации файла и его перевода. То есть это сам код, который осуществляет разбор данных в соответствии с ожидаемой структурой.",
            "Добавление конфигурации в общую структуру парсеров: на какой сервер идем, куда и как данные сохраняем. Мы обобщили все на уровне кода, а теперь надо это явно прописать, куда мы идем, чтобы получить данные о конкретном источнике.",
            "Например, когда у нас SFTP-сервер, я не думаю, что вы будете писать одни и те же четыре строчки кода в каждом парсере. Скорее всего, это будет какая-то функция или класс, которому передаем имя файла и доступы для SFTP-сервера, чтобы он пустил в систему.",
            "Работа тестировщика. Отладка, тесты на деве, стейдже на обрезанных данных. На этом этапе мы выявляем и устраняем ошибки в коде парсера. То есть тестировщик должен проверить две вещи:",
            "работает ли код. То есть он берет маленький кусочек файла, например 100–1000 строчек, и смотрит, встали ли данные в БД, в интерфейс, не свалилось ли что-то и работает ли функционал в целом;",
            "какая скорость. Нужно понять, удовлетворяем ли мы скорости загрузки и нет ли каких-то проблем.",
            "Работа тестировщика и DevOps-инженера. Пуско-наладка на реальных данных. Далее мы все выкатываем на прод, делаем первые итерации, проверяем, все ли работает: встали ли данные в интерфейс, ничего ли не потерялось, выдерживаем ли по ресурсам.",
            "Время разработки пропорционально количеству парсеров. Для разработки одного парсера нам нужно 5 дней. Сверка со структурой БД занимает 1 день, написание парсера — 2 дня, на отладку/тесты и пуско-наладку закладываем еще по одномуу дню.",
            "На один парсер — 5 дней, а на 100 — целых 1,5 года. Процесс, конечно, можно оптимизировать и вести параллельно: аналитику не ждать разработчика, а разработчику не ждать тестировщика. Но тем не менее все это в сумме — большой объем очень рутинной работы.",
            "Время доработок пропорционально количеству парсеров. Классический пример: есть 200 источников, при этом появилось требование о том, что столбец — это не целое число, а число с плавающей запятой. И теперь нужно это отвалидировать, чтобы все было в порядке, иначе данные не будут сходиться. А в случае переиспользования кода (DRY) нужно еще приложить усилия к тому, чтобы подумать, как это сделать. К тому же нужно заново пройти по пайплайну. Мы сделали изменения — значит, нам нужно все заново проверить, посмотреть, выкатить и замерить все парсеры.",
            "По итогу формула будет такой: время множественного изменения = время одного изменения * число парсеров. То есть время на расширение числа парсеров или валидируемых ими полей эквивалентно времени разработки. Не получится делать быстрее дублирующуюся работу, придется делать с такой же скоростью.",
            "Расширение числа парсеров или валидируемых ими полей запускает пайплайн заново. Подобная ситуация случилась лично с нами. Изначально было N парсеров, далее убрали 10 и потом добавили еще 15 сверху. А после этого в 20 имеющихся парсерах изменился состав файла и добавился еще один внешний сервер. Приходится начинать все сначала: аналитика –> разработка –> тестирование.",
            "Ключевой вопрос в этой всей ситуации — как преодолеть проблемы «решения в лоб» и сделать результат нашей работы максимально самодостаточным? Мы подумали об этом и пришли к другой идее — динамической реализации.",
            "Давайте вспомним No-code-приложения, например Tilda. Или такой конструктор мобильных приложений, где вы тащите формы, а затем система сама выполняет работу. Код генерируется — вы наслаждаетесь. Примерно то же самое мы сделали в рамках нашего проекта.",
            "В один момент мы подумали: «А что, если разработать пользовательский интерфейс, который позволит самостоятельно решать задачи, минуя аналитика, разработчика и тестировщика?» То есть пользователь сможет сам создать описание для своих действий в админке в виде шаблона. Затем наш магический механизм обработает созданный шаблон. А динамический парсер интерпретирует файлы, соответствующие структуре, описанной в шаблоне, без необходимости дополнительной ручной обработки.",
            "Эту идею мы назвали — шаблон динамического маппинга.",
            "На картинке ниже представлено, как это все можно изобразить с точки зрения пользовательского интерфейса. В этом списке темплейт — наш источник. Мы его назвали шаблоном.",
            "Далее представлено, какие примерно атрибуты могут быть у этого шаблона:",
            "название (name) — на что смотреть в интерфейсе;",
            "внешняя система (external system) — на какой сервер нужно пойти, чтобы достать конкретный файлик (уникальное системное имя шаблона);",
            "имя файлика и директория, в которой он лежит (filepath) — путь к файлу на сервере, с которым связан этот шаблон, и здесь же и имя файла;",
            "системное обозначение для источника (system name) — выбор из внешних систем, куда мы будем подключаться, чтобы туда идти по пути выше.",
            "Дальше мы уже говорим о том, что внутри этого источника. Это находится в отдельной сущности — attributes. Эта сущность включает в себя такие элементы, как:",
            "name — чтобы человеку было на что смотреть в интерфейсе;",
            "system_name — уникальное системное имя поля шаблона, которое мы должны искать в файле;",
            "type — тип данных поля, такие как float, str, str_alpha_numeric, date, int, bool;",
            "field_representations (представления поля) — JSON-структура, представляющая отображение поля на БД;",
            "template (шаблон, Foreign Key) — связь шаблона с общей инфой, к которому относится данное поле.",
            "На каждом этапе присутствует валидация, которая проверяет, например, наличие файлика, полей в файлике, соответствие типу. И, в конечном итоге, это все может записаться в БД.",
            "Сейчас мы построили чисто концептуальное решение. Давайте разберемся, какой результат нам бы принес этот подход — поговорим о его преимуществах.",
            "Элемент продукта закончен",
            "Такой формат реализации сокращает все возможные согласования. Вместо написания отдельного парсера для каждого нового файла и его источника создаем шаблоны, описывающие структуру файла и определяющие соответствие полей самостоятельно. Это существенно сокращает процесс работы, к тому же позволяет сразу же проверить результат. И это будет работать уже завтра. Сегодня написал — завтра это уже готово, сегодня придумал — завтра уже на проде валидируешь новый файл.",
            "Настройка и поддержка, которая оптимизирует время",
            "В случае со статической реализацией каждый день на протяжении полутора лет придется заниматься переводом спецификаций в код. Естественно, это не творческая и скучная задача. Мы все-таки хотим закрыть эту задачу и, конечно, сделать это наиболее интересным для нас способом.",
            "При динамической реализации настройка и поддержка будут намного интереснее, чем просто сконструировать «решение в лоб» и сидеть полтора года переводить спецификации.",
            "Возникает ряд вопросов. А что если где-то что-то отвалится? А почему данные не загружаются? А как вообще это все сохранить в табличку и как это все будет выглядеть? А как эти 20 млн строчек обработать? Явно придется над всем этим поразмыслить. 5 часов подумать — 1 час написать код.",
            "Масштабируемость",
            "Появляются новые требования:",
            "добавить в N-количестве источников проверку на дубликаты и действие, которое надо совершать, если они есть;",
            "а еще в M-источниках добавить проверку на отрицательность.",
            "При этом непонятно, будут ли все эти поля в файле.",
            "Так у нас появились дополнительные атрибуты у поля шаблона:",
            "required (обязательное) — флаг, указывающий, является ли поле обязательным для заполнения;",
            "can_be_negative (может быть отрицательным) — флаг, указывающий, может ли поле содержать отрицательные значения;",
            "contained_in_md_table (содержится в таблице md) — имя таблицы md, в которой содержится это поле (если применимо);",
            "contained_in_md_attribute (содержится в атрибуте md) — имя атрибута md, в котором содержится это поле (если применимо);",
            "duplicates_in_table (дубликаты в таблице) — имя таблицы, в которой разрешены дубликаты этого поля (если применимо);",
            "duplicates_attribute (атрибут дубликатов) — атрибут, определяющий дубликаты этого поля (если применимо);",
            "duplicates_action (действие с дубликатами) — выбор из действий по обработке дубликатов: обновление или пропуск.",
            "И на все это есть 5 дней — вспоминаем сроки разработки одного парсера. В такой системе мы на этапе валидации прописываем новые условия один раз, а дальше клиент уже сам работает с шаблонами и сам отвечает за выбранные им параметры.",
            "admin API (CRUD — в админке) — могут вносить изменения и создавать новые записи;",
            "user API (Read — для всех пользователей) — есть возможность только читать.",
            "Python и Django — это решение удобно для нас, к тому же мы используем его с начала работы над проектом.",
            "Blazingly-Fast Polars — о том, почему мы выбрали именно этот инструмент, рассказывал наш тимлид в статье Битва медведей: Pandas против Polars. Если кратко: этот вариант для наших вариантов использования работает быстрее, чем Pandas.",
            "Paramiko — библиотека для подключения по SFTP. Очень красиво и надежно.",
            "SQLAlchemy — в качестве дополнительной ОRМ. Удобный интерфейс, быстро и красиво.",
            "Здесь мы перевели сущность модели в сущность шаблона, которая была до этого. Из интересного — здесь есть функция temporary_table_name, в ней мы получаем temporary-имя. Это название временной таблицы. О том, для чего нам нужна временная таблица, поговорим чуть позже.",
            "Далее то же самое делаем для TemplateField. Можно просто взять, перевести на другой язык — и все готово.",
            "Как я и говорил, у админов будет полный набор CRUD-операций: чтение, создание, обновление. Но следующий момент более интересный.",
            "Вот эти два поинта ниже нам нужны для того, чтобы у клиента в интерфейсе была возможность посмотреть, какие у нас есть таблички и атрибуты и поля этой таблички.",
            "Классические представления — обычные классы доступа. Здесь идет пагинация по страницам. Далее, когда мы предоставляем список шаблонов, мы передаем только основную информацию о них — название. А когда отдаем только один шаблон, мы его отдаем вместе с атрибутивным составом, чтобы пользователь мог убедиться, что у него все правильно загружается.",
            "На этом этапе мы получаем какой-то путь файла у этого темплейта, то есть не пишем явно — идти на такой-то сервер. А просто говорим — взять у темплейта название файла. И на выходе получается путь файла.",
            "Далее через библиотеку мы подключаемся по SFTP, забираем файл.",
            "У темплейта есть филды — template.fields. Мы их забираем — это и будут наши правила валидации.",
            "django_file = InMemoryUploadedFile",
            "validate_file(django_file, list(template.fields.all()))",
            "BasicFileHeadersValidator. В процессе валидации проверяем наличие данных, дублирование колонок, наличие требуемых колонок. Если какой-то из этих пунктов не проходит, мы отправляем пользователю ошибку, так как нам незачем загружать файл, у которого нет требуемых нам колонок. Мы знаем, что он заведомо сохранит его туда, куда нам не надо.",
            "BasicFileReaderValidator / GeneralValuesFieldsValidator. Базовое чтение (проверка на кодировку, соответствие строк размеру), перевод названий колонок согласно шаблону, нормализация данных и проверка их на соответствие типам, генерация дата-фрейма.",
            "Даже если взять 10 млн строк, с учетом того, что параллельно работает 100 источников, cкорее всего, они упадут по памяти.",
            "Что с этим делать? Классический вариант — разбиение на чанки. В результате этого получается кусок файла, c которым можно продолжать работать. Далее с ним проводим соответствующие этапы валидации и формируем соответствующий дата-фрейм. Но в такой структуре важно, чтобы мы не захотели сделать дополнительную логику — например, агрегацию, дезагрегацию, суммирование, фильтрацию, категоризацию. Для всех этих работ мы используем сохранение во временную таблицу, а потом при необходимости все это сохраняем в основную БД.",
            "Если требование состоит в том, что решение нужно кастомное, то это делает уже другой разработчик в рамках другой задачи. Он идет во временную таблицу, забирает все нужные данные и, соответственно, с ними делает то, что нужно ему. При этом временную таблицу нужно каждый раз чистить перед загрузкой, иначе вы упадете по памяти в БД или будете работать со старыми данными.",
            "Что будем использовать дальше? Во-первых, SQLAlchemy. Собираем из дата-фрейма название колонок, делаем сущности колонок, берем название таблицы и с помощью контекстного менеджера вставляем какое-то количество записей. В нашем случае — 1 млн.",
            "Что такое контекстный менеджер",
            "В Python можно использовать удобный декоратор для этого паттерна. Перед началом мы напишем такой connection_url, где мы вставляем доступ к БД. Здесь конструкция try-finally говорит о том, что мы пытаемся отдать наш движок подключения к БД. Но в любом случае, какие бы ошибки ни были, этот движок будет в конечном итоге закрываться. Это нужно для того, чтобы в БД не висело открытое подключение.",
            "Берем нужную нам модель представления данных, написанную на Django.",
            "Здесь есть специфичный для Django код. Но я почти уверен, что таким образом можно получить табличку из основной БД на любом другом языке.",
            "Дальше мы смотрим поля у этой таблицы и пытаемся найти ее первичный ключ. Для нашего проекта специфично то, что может быть три разных первичных ключа, в зависимости от таблички: обычный ID, External ID либо Name ID.",
            "Итак, мы получили название первичного ключа. Далее мы идем в БД и проверяем, какие записи с этими ID уже есть.",
            "Мы берем их, кладем в оперативную память.",
            "После этого мы идем по дата-фрейму и проверяем, есть ли у нас такая запись.",
            "Если не смогли найти с этим ID объект среди тех, что мы вытащили из БД, то сохраняем ее как новый объект, кладем в какую-то структуру (в нашем случае список) и затем позже создадим их в БД.",
            "Но если мы все-таки смогли найти этот ID, то берем и меняем у найденного объекта все атрибуты.",
            "Условно, в файле одно значение, в БД другое значение для этой строчки — поменяли, сохранили в структуру. И здесь происходит множественное сохранение/обновление. Важно использовать batch_size. Потому что создавать запрос на миллион строчек — невыгодно. Делаем batch_size, разбиваем структуру на множество запросов и делаем с ним.",
            "batch_size=1000 — число объектов в каждом запросе.",
            "Заклинание освоено! А теперь поговорим о недостатках такого решения.",
            "Трудности в отладке и тестировании. Это интересно, но приходится думать, тратить время, пытаться понять, где ошибка, почему не можем распарсить — затратно дебажить.",
            "Строгость в соблюдении принципов. Важно соблюдать всю последовательность действий, про которую мы говорили ранее (этапы реализации динамического решения). Мы не должны запихнуть весь файл в систему, а на выходе говорить, что сделаем полностью кастомную работу.",
            "Ограничение в функциональности. Здесь как раз идет речь об отсутствии некоторых кастомных полей, все динамическое. И если вы тронете хотя бы одну строчку, то для всех остальных это изменение автоматически применится. Один файл, но его тяжело поддерживать.",
            "Дополнительное время на валидацию. Многие знают, что функции в Python вызывать довольно дорого, поэтому вы можете столкнуться с тем, что валидация может занимать много времени. Ни в коем случае не пытайтесь запустить код, который кажется примерно рабочим. Посмотрите, какие есть практики использования функции, почему нельзя использовать лямбду-функцию в цикле и т. д.",
            "Дополнительное время на сохранение во временную БД и/или основную БД. Мы говорим, что у нас есть временная таблица, и после нее вы можете делать некоторые кастомные операции, которые вы хотите. Но в то же время мы тратим минуту-две на то, чтобы сохранить эти данные, и, плюс ко всему, они занимают какую-то дополнительную память на диске.",
            "Как мы можем обойти эти минусы:",
            "Расширение поддерживаемых форматов файлов. Здесь имеются в виду Excel и все подобные структуры. Нам не потребуется писать отдельный парсер для каждого такого формата. Можно просто добавить новый вариант ридера, добавить его в общую структуру и смотреть на расширение файла перед загрузкой.",
            "Оптимизация производительности. Пытаемся ускорить валидацию, оптимизировав все возможные куски, меняя пайплайн вызова валидаторов.",
            "Развитие интерфейсов и возможных конфигураций. Админу нужно дать возможность делать batch_size для источника. То есть он знает, что, например, у него будет 100 валидированных колонок, и здесь batch_size в 100 тыс. — это слишком много, а 1,5 тыс. — уже нормально. Пусть у него будет возможность заменить на любое значение, которое он сам посчитает нужным. Дальше, если говорить о других возможных интерфейсах, у нас есть проверка на отрицательность на случай дублирования (мы меняем эту строчку или пропускаем ее и говорим, что нам все равно, есть ли она).",
            "И на этом все! В статье я поделился своим опытом, основанным на решении конкретной задачи. Надеюсь, что материал поможет вам выбрать правильное решение в аналогичной ситуации и покажет, как можно творчески подходить к задачам. А если у вас появятся вопросы — буду рад на них ответить в комментариях к этой статье!",
            "Статья подготовлена по мотивам доклада Никиты Ильина, Backend-разработчика в Spectr, на митапе #DevTalks. Ссылка на запись доклада:"
        ]
    },
    {
        "Название статьи": "Получение списка людей, посещающих определенные места",
        "Дата публикации": "2024-06-05, 15:12",
        "Автор статьи": "fire64 ",
        "Статья целиком": [
            "Представьте: вы ведете Telegram-канал о животных и хотите пригласить в него посетителей зоопарка. Или вам нужно собрать контакты потенциальных клиентов, посещающих определенный торговый центр. Как это сделать?",
            "Полиция может легко получить такую информацию от мобильных операторов, но что делать обычному человеку?",
            "Ответ – использовать Telegram и его функцию \"Люди рядом\" в сочетании с Python-скриптом.",
            "\"Люди рядом\": эта функция Telegram показывает контакты пользователей, находящихся поблизости, с примерным расстоянием до них (500 м, 1 км, 2 км и 3 км). Отображаются первые 100 ближайших контактов.",
            "Python-скрипт: с помощью библиотеки telethon можно получить доступ к этой информации и автоматизировать процесс сбора контактов.",
            "Установка:",
            "Скачайте и установите Python с официального сайта: https://www.python.org/downloads/",
            "Установите необходимые модули:",
            "Регистрация приложения Telegram:",
            "Зарегистрируйте свое приложение на сайте Telegram: https://core.telegram.org/api/obtaining_api_id",
            "Важно: используйте свой реальный номер телефона, привязанный к Telegram-аккаунту, а не бота.",
            "Создание скрипта:",
            "Создайте файл с расширением .py и вставьте код скрипта (https://pastebin.com/pYPA8PF0).",
            "Замените следующие значения:",
            "api_id = (ваш API ID)",
            "api_hash = (ваш API Hash)",
            "phone_number = '' (ваш номер телефона)",
            "Запустите скрипт.",
            "Выберите на карте нужное местоположение.",
            "Укажите радиус поиска (500, 1000, 2000 или 3000 метров).",
            "Нажмите кнопку \"Начать поиск\".",
            "Скрипт автоматически получит список пользователей Telegram, находящихся в заданном радиусе, и добавит их в ваши контакты.",
            "Данные обновляются Telegram каждые 30 минут.",
            "Отображаются только первые 100 пользователей.",
            "Поиск работает только в регионе, к которому привязан ваш номер телефона.",
            "Отображаются только пользователи со включенной видимостью. В среднем это 10-15%",
            "Важно использовать этот метод этично и уважительно по отношению к другим пользователям. Не рассылайте спам и не используйте полученную информацию в незаконных целях.",
            "Помните: эта информация предназначена только для ознакомления. Перед использованием подобных скриптов убедитесь, что вы не нарушаете правила Telegram и законодательство вашей страны."
        ]
    },
    {
        "Название статьи": "Как в Tele2 автоматизировали тестирование SAP ERP с помощью Python",
        "Дата публикации": "2024-06-05, 13:34",
        "Автор статьи": "a_valeeva ",
        "Статья целиком": [
            "Привет, Хабр! Меня зовут Анастасия Валеева, я – руководитель группы обеспечения качества в Tele2. Наша команда работает в большинстве своём с SAP ERP, и мы не понаслышке знаем, что автоматизация данной платформы — дело далеко не тривиальное. В этой статье я хочу поделиться с вами, как и зачем мы автоматизировали тестирование с помощью Python.",
            "SAP ERP – гибкий инструмент в руках нашей команды. Мы дорабатываем функциональность системы под потребности конкретного бизнес сегмента. Эти изменения производятся по запросу бизнес-пользователей. Объём и влияние доработок могут быть различными, но одно остаётся неизменным – каждая доработка является уникальной. Таким образом, это не простое устранение багов и улучшения текущего функционала, не изменение версионности продукта после оптимизации, а, как правило, абсолютно новый «продукт» в системе. В случае автоматизации функционального тестирования нам потребуется писать автотест на каждую доработку/разработку, что занимает больше времени, чем ручное тестирование (написание автотеста, отладка, оптимизация) + данный автотест с каждой новой разработкой будет уже неактуален, и нужно будет создавать новые и новые из раза в раз. Делаем выводы, что автоматизировать функциональные тексты для нас нерелевантно. А вот регрессионные тесты, которые мы проводим после каждого изменения системы, представляют собой более шаблонные варианты, шаги повторяются, и от их автоматизации есть профит.",
            "Сейчас мы работаем с SAP ERP и интегрированными продуктами (FileNet, BW, Fiori), однако, импортозамещение идёт полным ходом, и мы проводим пилотный проект по миграции на новую платформу. Так или иначе, созданный нами инструмент для автотестов универсален и может быть применён в работе с новой системой.",
            "Из множества инструментов автоматизации мы выбрали для ознакомления четыре наиболее совместимых с SAP ERP:",
            "SAP Scripting;",
            "Tricentis Tosca;",
            "eCatt;",
            "CBTA.",
            "Анализируя, мы исходили из трёх основных для нас факторов: скорость освоения, простота и гибкость, а также бюджет. По каждому из инструментов мы отметили свои плюсы и минусы, собрали информацию в единую таблицу. И вот что у нас получилось.",
            "По количеству зелёных блоков мы увидели, что нашим критериям в большей степени соответствует SAP Scripting.",
            "Принцип работы данного инструмента состоит в том, что он записывает все действия пользователя в системе, на выходе формирует файл в формате .vbs, который в последующем можно запускать в SAP. Соответственно, при запуске этого файла система будет повторять ваши предварительно записанные шаги. Кроме того, данный файл можно корректировать: удалять лишнее, дописывать недостающее или даже полностью переписать. Для этого необходимо открыть файл либо в блокноте, либо в любом другом редакторе, работающем с кодом.",
            "В процессе пилотирования SAP Scripting помимо технических вопросов мы решали несколько административных задач: удобство использования, гибкость, кастомизация, универсальность, прозрачность.",
            "Мы хотели внедрить такой инструмент, который будет полезен не только группе тестирования, но и другим смежным группам нашего подразделения. И поскольку мы говорим об автоматизации, одним из основополагающих факторов для нас было минимальное участие человека в этом процессе. Согласитесь, часто хочется просто нажать на волшебную кнопку \"РАБОТАТЬ\", чтобы оно всё само заработало :)",
            "Добиться данного магического эффекта «работает само» нам помог Python. За это отвечала коллега из моей команды — она написала скрипт для робота, который сейчас работает буквально по одному клику.",
            "Что касается прозрачности, то мы пошли по пути, доступному для любого пользователя. Для этого «прикрутили» Python к файлу Excel. Это означает, что сейчас провести регресс может любой сотрудник — достаточно зайти в файл автотеста и нажать кнопку «СТАРТ».",
            "Бизнес-процесс состоит из набора бизнес-операций. Например, создание логистического заказа состоит из заведения заказа, смены статуса подписания договора, деблокирования заказа и создания счёта-фактуры. Для обеспечения полного регрессионного тестирования мы автоматизируем всю цепочку шагов. На выходе получаем Excel-документ со скриншотами и подробной информацией по каждому шагу тестирования. Причём регресс может запустить любой пользователь, не только тестировщик, это доступно в том числе для менеджеров со стороны бизнеса. А полученные данные (скрипты) можно использовать также для генерации тестовых данных.",
            "Существует несколько способов выполнения автотестов.",
            "1. Отдельно по каждому бизнес-процессу. По каждому модулю финансовой системы SAP ERP создан файл Excel, в котором есть кнопка вызова макроса. По вызову этой кнопки запускается Visual Basic for Applications. VBA обращается к системе SAP и вызывает на выполнение ранее записанный скрипт vbs.Таким образом, мы можем выполнять тестирование по отдельному модулю или бизнес-операции.",
            "2. По всему модулю или нескольким модулям.Для этих нужд как раз используется Python. Наш робот обращается к SAP, открывая рабочее окно. Далее вызывает необходимые файлы Excel, которые работают по описанному принципу макросов на VBA. Таким образом, мы получаем следующую цепочку:",
            "При этом пользователю необходимо только единожды нажать кнопку ВЫПОЛНИТЬ.",
            "Запуск SAP GUI",
            "Заведение функции для чтения файла Excel",
            "Подключение к Excel",
            "На каждом листе в Excel есть подробная входная и выходная информация, при этом входную информацию можно корректировать. Большая часть листов связана между собой, чтобы можно было провести всю цепочку на одних данных, а последующие шаги не зависели от дополнительных действий пользователя.",
            "Все скриншоты, которые создаются в процессе регресса, генерируются вместе с документами и проводками. Лишние скриншоты можно удалить прямо на странице в Excel. При необходимости сотрудник может по номеру документа найти нужную проводку или операцию в SAP. Это является прозрачным и удобным способом анализа логов тестирования.",
            "Рядом с каждым шагом в файле появляется текстовое описание, статус «успешно» или «не успешно» пройден шаг и цветовой индикатор — зеленый означает успешно пройденный этап, красный сигнализирует об ошибках.",
            "Если ошибка является блокирующей для системы и дальнейшее прохождение шагов невозможно, то скрипт остановится, выдаст информационное сообщение и сохранит изменения в файл. Если ошибка не влияет на последующие шаги, то скрипт продолжит работу, а в конце выдаст лог в Excel с отображением корректных и некорректных шагов. При таком раскладе у нас появляется возможность увидеть проблему в моменте и исправить её.",
            "Также, завершение работы скрипта сопровождается звуковым оповещением.",
            "Дополнительно мы настроили автоматическое удаление листов из общей папки через три дня после их создания.",
            "Что в итоге",
            "Мы посчитали, сколько рабочего времени ручных тестировщиков мы экономим при использовании инструмента автоматизации. Получилось, что на один кейс при использовании SAP Scripting мы тратим 31 секунду против 148 секунд при ручном тестировании. Таким образом, 80% времени инженеров высвободилось на другие задачи, и мы смогли повысить эффективность тестирования.",
            "Данный вариант автоматизации является гибким к изменениям. В случае переезда на другую финансовую систему мы перенаправим нашего робота на Python на вызов нужной нам программы. Сейчас одна из наших основных задач – обеспечить качество работы текущего функционала и уже на этой надёжной основе реализовывать улучшения и внедрять новые фичи. Для нашей команды автоматизация тестирования SAP ERP стала интересным и полезным опытом, а бизнесу предоставила доступную, понятную и безотказную систему проверки рабочих процессов."
        ]
    },
    {
        "Название статьи": "Быстрый интерфейс, быстрый деплой",
        "Дата публикации": "2024-06-05, 11:01",
        "Автор статьи": "funtastick ",
        "Статья целиком": [
            "Салют! Не так давно создатели знаменитого pydantic выпустили новый фреймворк — FastUI, который позволяет создавать пользовательские интерфейсы с помощью декларативного кода на Python. В этой статье рассмотрим создание простого приложения и деплой его в Cloud Apps. ❯ Обзор По заявлению авторов фреймворка, фронтенду не нужно (и не следует) знать ничего о приложении, которое вы создаете, вместо этого он должен просто предоставить все компоненты, необходимые для создания интерфейса, а затем бэкенд может предоставить необходимые данные и параметры компонентов. Реализовано это таким образом, что FastUI инкапсулирует описание компонентов интерфейса и в виде классов, затем запускается простое React приложение, которое обращается к эндпоинтам за данными и компонентами. ❯ Пример Для примера давайте напишем простое приложение, предоставляющее информацию о городах из списка с возможностью пагинации. Данные для экспериментов любезно предоставили создатели фреймворка. Для начала опишем pydantic модель и функцию для чтения данных. from pydantic import BaseModel, Field, TypeAdapter import json from pathlib import Path class City(BaseModel): id: int city: str = Field(title=\"Name\") city_ascii: str = Field(title=\"City Ascii\") lat: float = Field(title=\"Latitude\") lng: float = Field(title=\"Longitude\") country: str = Field(title=\"Country\") iso2: str = Field(title=\"ISO2\") iso3: str = Field(title=\"ISO3\") admin_name: str = Field(title=\"Admin Name\") capital: str = Field(title=\"Capital\") population: float = Field(title=\"Population\") def cities_list() -> list[City]: cities_file = Path(__file__).parent / \"cities.json\" with open(cities_file, \"r\", encoding=\"utf-8\") as f: data = json.load(f) cities = [City(**city) for city in data] return cities Далее напишем каркас для нашего примера, с помощью FastAPI. Опишем два роута, первый возвращает необходимые компоненты и данные, а второй — простое React приложение, которое отвечает за запрос и отображение компонентов, полученных из предыдущего. from fastapi import FastAPI from fastapi.responses import HTMLResponse from fastui import AnyComponent, FastUI from fastui import components as c, prebuilt_html from fastui.components.display import DisplayLookup, DisplayMode from fastui.events import BackEvent, GoToEvent app = FastAPI() @app.get(\"/api/cities\", response_model=FastUI, response_model_exclude_none=True) def cities_view(page: int = 1, country=None): cities = cities_list() page_size = 10 # Количество записей в таблице, отображаемых на странице filter_form_initial = {} return c.Page( # Page - базовый контейнер для остальных компонентов components=[ c.Table( # Table - базовая разметка таблицы data=cities[(page - 1) * page_size : page * page_size], #Создаём срез данных для заполнения таблицы data_model=City, #Передаём модель данных columns=[ # Описываем столбцы таблицы DisplayLookup( #Указываем содержимое и размер столбца в процентах field=\"city\", table_width_percent=33 ), DisplayLookup(field=\"country\", table_width_percent=33), DisplayLookup(field=\"population\", table_width_percent=33), ], ), c.Pagination(page=page, page_size=page_size, total=len(cities)), #Кнопки для пагинации ] ) @app.get(\"/{path:path}\") async def html_landing() -> HTMLResponse: \"\"\"Простое React приложение, идёт последним, т.к. соответствует всем маршрутам\"\"\" return HTMLResponse(prebuilt_html(title=\"Большие города\")) Результат работы представлен на рисунке ниже: ❯ Деплой Для деплоя приложений на FastUI можно воспользоваться сервисом Apps, к сожалению рассмотренный фреймворк только набирает популярность, поэтому мы воспользуемся опцией: «деплой из Dockerfile». Для этого достаточно создать Dockerfile и разместить его в корне репозитория. FROM python:3.11 COPY . /app WORKDIR /app RUN pip install -r requirements.txt CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] EXPOSE 8000 Обратите внимание, что при отсутствии в Dockerfile параметра EXPOSE, APPS будет слушать порт 8080 контейнера. Далее достаточно предоставить сервису доступ к аккаунту на github. Затем остаётся следить за логами деплоя: В случае успешного развёртывания появиться удобный дашборд с графиками нагрузки на виртуальную машину: ❯ Заключение В данной статье мы рассмотрели лишь малую часть возможностей фреймворка, однако можно отметить, что FastUI предоставляет новый подход к созданию веб-приложений и позволяет существенно ускорить разработку. Возможно, захочется почитать и это: ➤ Timeweb Cloud CLI ➤ Бесплатный прокси к Docker Hub ➤ Фантастически быстрый деплой веб-приложения ➤ Учимся летать: симуляция эволюции на Rust ➤ Age of Empires – культовая попытка сделать Цивилизацию в реал-тайме Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩"
        ]
    },
    {
        "Название статьи": "Как я создавал аудиоплеер на python с FFmpeg",
        "Дата публикации": "2024-06-04, 18:10",
        "Автор статьи": "Niamorro ",
        "Статья целиком": [
            "Всех приветствую. Сегодня хочу поделиться опытом создания своего первого проекта на Python. Мой проект — это простой аудиоплеер, и я хочу рассказать, как я его создавал, с какими сложностями столкнулся и что из этого вышло.",
            "Выбор языка для первого проекта — это всегда непросто. Я выбрал Python по нескольким причинам:",
            "Простота синтаксиса. Python очень читабельный и понятный, что идеально подходит для новичков.",
            "Богатая стандартная библиотека и сообщество. Множество готовых решений и библиотек, которые можно использовать в своих проектах.",
            "Популярность в разработке. Python — один из самых популярных языков программирования, и навыки работы с ним будут полезны в будущем.",
            "Моя цель была написать простой аудиоплеер, который мог бы играть основные аудиоформаты. Я хотел, чтобы пользователь мог выбирать треки, ставить их на паузу и останавливать, так же изменять скорость проигрывания.",
            "Выбор библиотек занял действительно много времени, так как нужно было выбрать библиотеки которые обновляются, и имеют необходимый мне функционал. Я использовал несколько библиотек:",
            "PySide6: библиотека для создания интерфейсов созданная разработчиками Qt, имеет хорошую поддержку сообщества и регулярные обновления, в дополнение к ней использовал qdarktheme для стилизации интерфейса.",
            "FFmpeg: Универсальный инструмент для обработки видео и аудио.",
            "Sounddevice: Библиотека для воспроизведения и записи звука в Python.",
            "Mutagen: Библиотека для извлечения данных из аудиофайлов.",
            "Выбор файла:",
            "Пользователь выбирает аудиофайл из меню \"Файл\". Поддерживаемые форматы включают MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Выбранный файл передаётся в FFmpeg через подпроцесс для извлечения необработанных аудиоданных. Используемая команда:",
            "Чтение аудиоданных:Аудиоданные считываются блоками и сохраняются в массив NumPy для эффективной обработки.",
            "Регулировка громкости:Регулировка громкости осуществляется путём умножения аудиомассива на коэффициент громкости.",
            "Регулировка скорости воспроизведения:Скорость воспроизведения (например, 2x) управляется через библиотеку sounddevice путём изменения частоты дискретизации.",
            "Поток вывода:Обработанные аудиоданные передаются на аудиовыход через библиотеку sounddevice.",
            "Управление воспроизведением:Элементы управления, такие как воспроизведение/пауза, следующий/предыдущий трек и перемотка, обрабатываются через класс AudioTrigger.",
            "Воспроизведение/Пауза:Использует класс AudioTrigger для начала/остановки аудиопотока.",
            "Следующий/Предыдущий трек:Обновляет текущий индекс трека и загружает следующий/предыдущий трек в плейлисте.",
            "Перемотка:Регулирует позицию воспроизведения, пересчитывая индекс позиции на основе значения ползунка.",
            "Виджет очереди треков:Отображает добавленные ранее папки.",
            "Виджет плейлиста:Отображает содержимое папки.",
            "Виджет информации о треке:Показывает метаданные и обложку для воспроизводимого трека.",
            "Если хотите ознакомиться с исходным кодом или внести свой вклад в проект,",
            "приглашаю вас посетить страницу GitHub проекта. Там вы найдёте весь исходный код аудиоплеера.",
            "Также у проекта есть веб-сайт, где вы можете скачать готовые .exe и .deb пакеты для Windows и Linux. Здесь же доступна подробная документация по установке и использованию программы.",
            "Работа с FFmpeg требовала правильной организации буферизации аудиоданных, чтобы избежать прерываний и задержек при воспроизведении.",
            "Решение: Буферизация данных в массив NumPy.",
            "Треки воспроизводились с неправильной скоростью из-за некорректной частоты дискретизации.",
            "Решение: Я считываю частоту дискретизации трека и открываю аудиопоток с настройками именно для того трека, который в данный момент должен воспроизводиться.",
            "В результате я создал аудиоплеер с основными функциональными возможностями:",
            "Проигрывание аудиофайлов: Поддерживаются популярные форматы MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Управление воспроизведением: Воспроизведение, пауза, остановка, перемотка, следующий/предыдущий трек.",
            "Регулировка скорости воспроизведения: Возможность воспроизводить треки быстрее или медленнее.",
            "Плейлист: Добавление папок с треками.",
            "Информация о треках: Отображение метаданных и обложек альбомов.",
            "Тёмная тема: Благодаря qdarktheme, аудиоплеер имеет современный и стильный интерфейс.",
            "Добавить поддержку потокового аудио: Возможность воспроизводить музыку из интернет-радиостанций и стриминговых сервисов.",
            "Расширить функциональность плейлиста: Добавить возможность создания и сохранения пользовательских плейлистов.",
            "Поддержка эквалайзера: Добавить эквалайзер для настройки звука.",
            "Создание аудиоплеера на Python оказалось полезным опытом. Я научился работать с аудио на низком уровне, обрабатывать потоки и создавать пользовательский интерфейс.",
            "Буду рад любым отзывам и предложениям по улучшению плеера. Спасибо за внимание!",
            "Ссылки:",
            "Исходный код на GitHub",
            "Сайт проекта с загрузкой пакетов"
        ]
    },
    {
        "Название статьи": "Как мониторинг связан с тестированием. Преимущества мониторинга для бизнеса: как экономить время и деньги",
        "Дата публикации": "2024-06-04, 15:59",
        "Автор статьи": "luffity ",
        "Статья целиком": [
            "Привет! Проходя множество собеседований, я не раз слышал вопросы по типу: «Что такое мониторинг?», «Как это связано с тестированием?», «Зачем это нужно?». Для меня, волей случая ставшего специалистом по мониторингу чуть больше года назад, это тривиальные вопросы, однако многие компании либо не знают, что это такое, либо не видят в этом пользы. На одном из последних интервью я услышал интересное мнение от QA Lead о том, что assert должен быть в каждом тесте. Смелое заявление, подумал я. Поэтому, собственно, вы и читаете эту статью.Разберёмся, что такое мониторинг и с чем его едят. А главное, зачем он нужен вообще.",
            "Думаю, начать с небольшого введения обо мне будет наиболее верным для погружения в тему. Сейчас я занимаю должность middle SDET в ООО «МИТ» (если проще, то DIXY Group). Попал я туда как AQA, причём единственным. Я находился в группе по мониторингу корпоративных сервисов, соответственно, кроме меня там были только специалист по мониторингу (мой Lead), DevOps и системный админ. Похоже на стандартное начало анекдота…",
            "Моим заданием на испытательный срок стало написание сервиса по мониторингу интернет-соединения на торговых точках компании. И тут я подумал: какой, блин, мониторинг? С другой стороны, работа на дороге не валяется, тем более настолько приятная. Дело было вечером, делать было нечего…",
            "По итогу этого задания ко мне пришло осознание того, как тесты могут трансформироваться в нечто большее, чем проверки, прогоняемые раз в релиз. Они могут быть целой экосистемой, даже можно сказать: «Глазом Бога» (кто понял отсылку к Форсажу, спасибо).",
            "Начнём мы, конечно же, с поверхности и определим для себя, что такое мониторинг и как он связан с тестированием. В самом распространённом смысле под тестированием понимают сверку ожидаемого и фактического результатов на конечном наборе тестов. Если мы берём стандартные парадигмы тестирования, то такие проверки могут выполняться при добавлении новой фичи, раз в спринт, при релизе и много когда ещё. Однако все эти тестовые прогоны не преследуют цель отслеживать состояние продукта (программного или бизнесового) постоянно.",
            "Как раз здесь и вступает в игру мониторинг. Если о-о-очень грубо сказать, то это, по сути, те же наборы тестов, только несущие цель постоянного наблюдения за состоянием продукта. Но тут стоит уточнить, что это не тестовые кейсы в их привычном понимании.",
            "Думаю, будет проще понять наглядно.",
            "Это всем понятный простой тест. Мы переходим на страницу и ожидаем заголовок. Как можно заметить, для того чтобы исполнить свою главную функцию – сопоставить ожидаемый и фактический результаты, тест содержит в себе assert. Это несомненно верный подход к написанию тестов, так как это позволит более точно валидировать ошибку, а также правильно отобразить её в отчётах, например Allure.",
            "А теперь взглянем на код скрипта мониторинга, который проверяет доступность ресурса.",
            "Сразу бросается в глаза отсутствие assert. Но в таком случае, как такой скрипт вообще можно считать информативным, если он не выводит ошибки? Именно поэтому мы добавим дополнительное действие. Например, найдём какую-то кнопку и нажмём на неё. Теперь, если ресурс не прогрузился или сломался, мы получим TimeoutException и сообщение о том, что именно скрипт не смог сделать.",
            "Возникает вопрос: почему бы тогда точно так же не поставить assert и не ждать лишнее время для выпадения TimeoutException ? Справедливо! Однако возьмём во внимание, что данный скрипт не нацелен на то, чтобы просто проверить доступность ресурса и отследить ошибку в отчёте. Если мы предполагаем, что скрипт гоняется бесконечно, пока смерть сервера не разлучит вас, то отчётом в данном случае будет не Allure, например (хотя я и его прикрутил к скриптам для Project Manager’а), а сервисы для графического отображения типа Grafana или сервисы мониторинга типа Prometheus. Да и сам скрипт, помимо успеха или провала теста, должен собирать ещё кучу полезных данных. В данном примере это может быть время прохождения скрипта, что может дать нам представление о том, в каком состоянии находится сервис. Особенно если учесть, что всегда можно настроить параметры интернет-соединения или любые другие моменты, имитирующие пользователя. И тут мы плавно перейдём к другому вопросу.",
            "Теперь стоит сказать и о том, что мониторинг может быть как на микро-, так и на макроуровне. Под микроуровнем обычно понимают низкоуровневый мониторинг, например, физического оборудования, элемента большого сервиса или что-то подобное. На макроуровне мониторинг предстает как UX-тестирование или тестирование пользовательских путей.",
            "Немного про микроуровень. Вернёмся к проекту по мониторингу интернет-соединения на торговых точках компании. По сути, скрипт достаёт из базы данных ID магазинов, конвертирует их в IP-адреса маршрутизаторов в магазинах и пингует их несколько раз. Помимо того, что в таблице в Grafana этот скрипт отображает «Up» или «Down» в зависимости от доступности каналов, он также собирает время отклика, хранит историю падений и содержит в себе данные об операторе SIM-карты, номере телефона и многое другое. Не очень-то похоже на тест.",
            "Теперь про макроуровень. Высокоуровневый мониторинг уже больше похож на UI/UX-тестирование. В его основе лежит постоянное отслеживание пользовательского пути через UI. Например, для сайта доставки продуктов — от захода пользователя на сайт и выбора товаров до оплаты. Помимо прочего, такой скрипт также собирает множество данных.",
            "В чём, собственно, разница? Основными критериями, отличающими мониторинг от тестирования, являются:",
            "Цель в постоянном наблюдении. Мониторинг — большой брат для ваших сервисов, который безустанно следит за ними;",
            "Сбор данных. Помимо отчётов о тестировании, мониторинг собирает ещё кучу данных;",
            "Быстрое реагирование. Думаю, тут и пояснять не надо. Если у вас есть тестовый сервер или синтетика, то критической баге будет сложно пролезть в прод;",
            "Имитация пользователя. Хоть UX-тесты и тесты пользовательских путей позволяют имитировать действия пользователя, но пишут их далеко не в первую очередь (информация со 100+ собеседований. Всем API подавай, а на пользователей мы кладём...).",
            "Что в итоге польза? Ну, тут я расскажу лучше пару «До и После» примеров.",
            "До разработки мною сервиса мониторинга интернет-соединения на валидацию проблем и выезд оперативной группы на точку уходило два-три рабочих дня. Более того, часто это были ложные вызовы, так как при неработающем основном канале включался резервный. Мониторинг позволил проходить весь процесс за два часа. Процент ложных вызовов за год его работы составляет не более 0,2%. А уж сколько денег это экономит компании, говорить не приходится, если учитывать, что к этому мониторингу подключена вся первая линия поддержки. Во всех магазинах Дикси. По всей России. Даже не думал, что час простоя торговой точки может стоить так много…",
            "А как вам такая новость в ленте: «Основной сайт и сайт доставки магазина Дикси не работают!»? Именно такую новость прочло руководство компании, заваривая утренний кофе. Да, узнавать о падении основных сервисов компании из новостей — это, видимо, не весело. Мне кажется, кофе точно не полезет после такого. Стоит ли говорить, что после этого случая мониторинг был внедрён на все сервисы?",
            "Забавно, правда?",
            "Но остался ещё один вопрос. Специалист по мониторингу и специалист по тестированию — это один и тот же профессионал? Мне кажется, специалист по мониторингу ближе к SDET, чем к AQA. Всё-таки я считаю, что автоматизатор тестирования должен знать и уметь меньше. AQA как бы и должен иметь представление о контейнеризации, но как бы и просто собрать контейнер в Docker достаточно. Специалист по мониторингу должен бы и под каждый свой мониторинг собрать контейнер, и доставить его, и обслужить если что, и k8s знать бы по-хорошему, ноды и воркеры – лучшие друзья. И опять-таки, ты же не знаешь, что может быть важно для бизнеса. Возможно, придётся выйти за рамки PyTest, Selenium и Appium. Уметь разобраться в различных библиотеках, знать асинхронные подходы, парадигмы проектирования, сильные и слабые стороны рабочего языка программирования — всё это важные навыки специалиста по мониторингу. Так что да, SDET более подходящее описание для его деятельности.",
            "Ссылочка на телегу"
        ]
    },
    {
        "Название статьи": "Кратко про Seq2Seq-модели",
        "Дата публикации": "2024-06-04, 09:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Seq2Seq модели — это архитектуры ML, предназначенные для задач, связанных с последовательными данными, типо машинного перевода, суммирования текста, создания описаний к пикчам и прочие задачи, где требуется преобразование одной последовательности в другую.",
            "В этой статье в общих деталях рассмотрим то, как реализуются Seq2Seq модели.",
            "Seq2Seq модели состоят из двух основных частей: энкодера и декодера.",
            "Энкодер преобразует входную последовательность в контекстный вектор, содержащий обобщённое представление всей входной информации. Этот вектор затем используется декодером для генерации выходной последовательности, о декодере чуть ниже.",
            "Перед тем, как подать данные в энкодер, текстовые данные преобразуются в числовые представления с помощью эмбеддинга. Это делается с помощью слоя Embedding, который преобразует каждый токен во входной последовательности в вектор фиксированной размерности. Например, слово milk может быть представлено как вектор размерности 300.",
            "Основу энкодера RNN, обычно реализованные с использованием LSTM или GRU. Эти сети обрабатывают входную последовательность пошагово:",
            "На каждом шаге RNN принимает эмбеддинговое представление текущего токена и скрытое состояние от предыдущего шага.",
            "Выход каждого шага включает новое скрытое состояние, которое передаётся на следующий шаг вместе со следующим токеном.",
            "В конце последовательности RNN генерирует контекстный вектор, который является финальным скрытым состоянием. Этот вектор обобщает всю информацию из входной последовательности и передаётся в декодер для дальнейшей генерации выходной последовательности. Контекстный вектор — это своего рода сжатая версия входной последовательности, включающая в себя её смысл.",
            "Для улучшения качества представления входной последовательности часто используются двунаправленные RNN. В этом случае два RNN работают параллельно: один — слева направо, другой — справа налево. Их состояния объединяются на каждом шаге, что позволяет учитывать как предшествующие, так и последующие слова для каждого токена в последовательности.",
            "Пример реализации энкодера на Keras с LSTM:",
            "Здесь входные данные сначала проходят через эмбеддинговый слой, который преобразует их в векторы фиксированной размерности. Затем эти векторы подаются в LSTM, который на выходе даёт финальные скрытые состояния, использующиеся в качестве контекстного вектора.",
            "В отличие от энкодера, декодер генерирует данные на основе предыдущих предсказаний и контекстного вектора, предоставленного энкодером.",
            "Подобно энкодеру, декодер принимает токены, которые сначала преобразуются в числовые представления с помощью эмбеддинга. Однако, в случае декодера на вход подаются не только реальные данные, но и предсказанные токены на предыдущих шагах.",
            "Декодер также реализован с использованием RNN, как LSTM или GRU. На каждом шаге декодер принимает:",
            "Контекстный вектор от энкодера.",
            "Предыдущий предсказанный токен (или начальный токен для первого шага).",
            "Скрытое состояние от предыдущего шага декодера. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности.",
            "На каждом шаге RNN декодера производит новое скрытое состояние и выходной вектор. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности",
            "Для улучшения качества генерации используется механизм внимания, который позволяет декодеру фокусироваться на различных частях входной последовательности на каждом шаге генерации выходной последовательности. Механизм внимания вычисляет веса для каждого состояния энкодера, определяя важность каждого токена входной последовательности в текущий момент времени.",
            "Пример реализации декодера на Keras с LSTM и механизмом внимания:",
            "Декодер принимает начальные состояния от энкодера и генерирует выходную последовательность.",
            "Машинный перевод — это одна из наиболее базовых задач для Seq2Seq моделей. Реализуем Seq2Seq модельку для перевода с английского на французский язык с использованием Keras.",
            "Для этой задачи будем использовать датасеты французских и английских предложений (они есть на kaggle):",
            "Следующий пример - текстовое суммирование. Это задача генерации краткого представления текста. Реализуем Seq2Seq модель с использованием механизма внимания.",
            "Для этой задачи будем использовать датасет новостей, где заголовок является суммарным представлением статьи:",
            "Реализуем генерацию описаний к изображениям — это задача, где Seq2Seq модели используются для генерации текста, описывающего содержание изображения. Будем использовать предобученную модель InceptionV3 для экстракции признаков изображения и Seq2Seq модельку для генерации текста:",
            "Seq2Seq модели - это очень мощный инструмент для решения задач, связанных с последовательными данными. Они позволяют преобразовывать входные последовательности в выходные с высокой точностью, в особенности при использовании механизмов внимания (об этом не забываем).",
            "В завершение хочу порекомендовать бесплатные вебинары курса ML Advanced:",
            "Современные модели прогнозирования типа TimesNet и TimeGPT",
            "H2O, TPOT, Autokeras - а вы что, за меня и модели строить будете?"
        ]
    },
    {
        "Название статьи": "Как подружить Llama-3 и YouTube имея всего 40 строк кода",
        "Дата публикации": "2024-06-03, 21:57",
        "Автор статьи": "evg_dc ",
        "Статья целиком": [
            "Сделаем Телеграм бота которому можно кинуть ссылку на YouTube видео и поговорить с ним о содержимом этого видео.",
            "За основу возьмем бота работающего на Llama 3-70b из моей прошлой статьи. Можно использовать и любую другую языковую модель включая ChatGPT или локальную запущенную на Ollama.",
            "Создать Телеграм бота и получить его токен (как это сделать, смотрите инструкции на просторах интернета, например здесь).",
            "Зарегистрироваться на Groq и получить api key (нужен VPN).Заходим по этой ссылке, регистрируем аккаунт, генерим ключ. Платежная карта не требуется.",
            "Замените в коде GROQ_API_KEY на api ключ полученный в Groq и TELEGRAM_BOT_TOKEN на токен телеграм бота, все должно быть в кавычках.",
            "После получения сообщения от пользователя ищем в тексте сообщения ссылку на YouTube видео. Делаем это перебирая все слова и проверяя их на наличие URL. Если ссылка на видео найдена, используя библиотеку youtube-transcript-api забираем транскрипцию.",
            "Далее, полученную транскрипцию подставляем языковой модели в виде сообщения от функции. Здесь мы немного обманываем модель, потому что такой функции нет, но лучше делать так чем ставить транскрипцию в системное сообщение. Модель заточена под работу с функциями и все правильно поймет.",
            "Как и в предыдущей версии, бот будет запоминать последние 6 сообщений и поддерживать диалог.",
            "Запускаем скрипт и в Телеграм боте задаем вопрос со ссылкой на видео:",
            "Посмотреть как это работает можно в YouTubeGPT.",
            "Еще есть НашGPT - это как ChatGPT только модель Llama 3-70b."
        ]
    },
    {
        "Название статьи": "Python в Excel жив?",
        "Дата публикации": "2024-06-03, 17:24",
        "Автор статьи": "Gonchar_POTT ",
        "Статья целиком": [
            "Уже больше месяца экспериментирую, исследую, как разные схемы (паттерны) осознанного дыхания влияют на вариабельность сердечного ритма (Heart Rate Variability, HRV на чуждом языке). В скромной, но не совсем уж крошечной Excel-таблице со столбцами “Паттерн”, “HRV”, “Пульс” набралось 258 записей и мне понадобилось выбрать победителя -- дыхательный паттерн, дающий на выходе максимальное значение HRV. Не вручную же сортировать эти записи!",
            "Да, я знаю: есть Pivot Table & Power Query. Но Pivot Table мне не по душе необходимостью после каждого изменения таблицы-источника делать REFRESH, во-первых, избыточной сложностью создания, во-вторых. И просто так не нравятся Pivot Table, что главное. Что же касается Power Query, то сочетание слов вызывает у меня трепет и учащенное сердцебиение: не понимаю, что это за зверь такой и насколько он страшный или полезный.",
            "Поэтому для подсчета результатов -- и выбора победителя -- применил относительно недавно появившуюся в Excel функцию GROUPBY в связке с XLOOKUP. И, раз уж пришлось группировать данные, решил сравнить нативные функции Excel с GROUPBY от Pandas (мы ведь помним, что сейчас Python можно запустить внутри Excel).",
            "Написал простой код:",
            "Поместил код через =PY( в ячейку А1 Excel",
            "И он прекрасно справился с задачей и выдал таблицу с результатами:",
            "breathing_pattern",
            "HRV",
            "HR",
            "8",
            "physiological sighs moderate",
            "59",
            "65",
            "7",
            "physiological sighs light",
            "57",
            "62",
            "1",
            "4.4-6.6",
            "56",
            "59",
            "6",
            "following pulse",
            "55",
            "61",
            "0",
            "4.2-0-6.4-0",
            "53",
            "62",
            "3",
            "6-6",
            "53",
            "61",
            "4",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "61",
            "2",
            "5-5",
            "52",
            "63",
            "5",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "Комбинация функций GROUPBY и XLOOKUP тоже отработала без изъянов:",
            "breathing_pattern",
            "HRV",
            "HR",
            "physiological sighs moderate",
            "59",
            "65",
            "physiological sighs light",
            "57",
            "63",
            "4.4-6.6",
            "56",
            "60",
            "following pulse",
            "55",
            "61",
            "4.2-0-6.4-0",
            "54",
            "63",
            "6-6",
            "54",
            "61",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "62",
            "5-5",
            "53",
            "63",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "* Для внимательных: разница в данных между двумя таблицами -- плод Python-овского округления до целых чисел.",
            "* В “нативном” подходе нет отсечки паттернов с количеством замеров менее шести.",
            "Выводы и наблюдения по теме:",
            "В Python итоговая таблица сама автоматически изменяет размеры при добавлении новых паттернов. GROUPBY by Excel ведет себя так же, а вот связка GROUPBY&XLOOKUP уже потребует редактирования формул: нужно изменять адреса диапазонов ячеек, к которым обращается XLOOKUP.",
            "Вопреки большему размеру Python-код мне кажется проще и для написания, и для чтения-понимания. Хотя писать код в ячейке Excel -- весьма извращенное удовольствие.",
            "Исполнение кода Python требует интернет-соединения.",
            "Выводы и наблюдения не совсем по теме:",
            "Для меня лично схема дыхания “physiological sighs light” (легкие физиогические вздохи) -- оптимальный выбор.",
            "Более шести месяцев я придумывал, зачем мне может понадобиться живущий в Excel Python и наконец нашел.",
            "Буду благодарен за советы и критику. Постараюсь ответить на вопросы."
        ]
    },
    {
        "Название статьи": "Майним крипто-коины с помощью Python и компьютерного зрения",
        "Дата публикации": "2024-06-03, 13:58",
        "Автор статьи": "temabed ",
        "Статья целиком": [
            "После внезапного обогащения энтузиастов, которые поиграли в начале года в приложение Notcoin в телеграм, подобные проекты стали расти как грибы. Да и грибников заметно поприбавилось. Но в данной статье мы не будем касаться тем блокчейна или финансов, а рассмотрим простой пример применения компьютерного зрения для фарма поинтов в самом популярном, после Notcoin, проекте - хомяке комбате. Название явно на что-то намекает, но да ладно.",
            "Это не первый проект, который я автоматизирую, и не самый нуждающийся в этом. Да и без компьютерного зрения с автоматизацией хомяка можно спокойно обойтись. Но с ним, во-первых, интереснее, а во-вторых - это просто хороший пример с минимумом строк кода для демонстрации возможностей библиотеки cv2. Статья, соответственно, предназначена для энтузиастов и начинающих специалистов.",
            "Начнем с того, что мы установим все необходимые зависимости и импортируем их в свой проект. Вот они, слева направо.",
            "С помощью pyautogui наш бот будет управлять мышью. Keyboard пригодится для назначения горячих клавиш, чтобы управлять работой бота. cv2 наградит бота зрением, пусть и компьютерным, с помощью которого тот будет находить совпадения с искомым изображением. А numpy пригодится для работы с большими массивами, но тут он почти для галочки, не бойтесь. Модуль time тоже понадобится, чтобы ставить таймауты в работе программы.",
            "Далее напишем небольшую конструкцию- переключатель.",
            "Функция change при вызове всего лишь меняет значение переменной work, которая будет использована в бесконечном цикле. И если work будет False, работа нашего кода будет останавливаться. И наоборот запускаться, в противоположном случае.",
            "Кстати, забыл упомянуть, что разработчики проекта, над которым мы сейчас проводим эксперимент, большие молодцы, и убрали возможность пользоваться приложением на десктопных устройствах. Поэтому для его запуска понадобится эмулятор Android.",
            "Теперь определим основную логику работы бота:",
            "Он ищет совпадение с изображением полностью заполненной энергии.",
            "Если находит совпадение, ищет изображение монеты и кликает на неё энное количество раз.",
            "И всё это работает в бесконечном цикле.",
            "Значит со скриншота приложения необходимо вырезать две области, которые помечены красным, и разместить их в отдельные файлы, конечно же.",
            "Теперь напишем функцию для кликов по монетке. Она будет принимать путь к исходному изображению, а так же порог чувствительности для компьютерного зрения и интервал (таймаут) в секундах.",
            "Переменной template будет присвоено исходное изображение монетки, но в оттенках серого, так как мы указали в параметрах 0. Это необходимость, так как в оттенках серого компьютер зрит лучше. Сразу вычисляем высоту и ширину исходника, и присваиваем переменным. А далее по ходу исполнения кода он делает скриншот, сравнивает с исходником, получает координаты области с совпадением, и делает 260 даблкликов по ней. Координаты я ищу немного кривовато и в итоге loc содержит большой массив, из которого я использую лишь самые первые координаты, после чего цикл прерываю. Но лучше сделать не смог, извините.",
            "А теперь напишем аналогичную функцию, но с задачей искать совпадение с картинкой полной энергии, после чего вызывать функцию click.",
            "В целом всё аналогично. Добавил лишь ожидание горячей клавиши, чтобы можно было остановить программу в любое время нажатием на тильду (Ё). Ну и для красоты заключил в try-except.",
            "И это всё. Пишем последние строки и запускаем скрипт (не забыв нажать на Ё для запуска логики в цикле).",
            "По сути, этот код многофункционален, и его без труда, с минимальными изменениями, можно переделать под любые другие задачи. На всякий случай оставлю и полную версию кода:",
            "Не судите меня строго по этому скрипту, большую часть жизни я вообще бегал с пистолетиком, и пристрастился к разработке сравнительно недавно, поэтому всего лишь юный падаван в возрасте. Хотя могу писать и большие скучные штуки, но вот писать о них получится дольше, чем сам код. А если будут вопросы- добро пожаловать в телеграм. У меня там небольшой клуб по интересам."
        ]
    },
    {
        "Название статьи": "Сравниваем популярные алгоритмы кластеризации DBSCAN и OPTICS",
        "Дата публикации": "2024-06-03, 13:51",
        "Автор статьи": "evaclick ",
        "Статья целиком": [
            "Привет, Хабр)",
            "Поговорим сегодня о 2 популярных алгоритмах кластеризации — DBSCAN и OPTICS, посмотрим их особенности и сравним",
            "Поехали!",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Ну, DBSCAN в особом в представлении не нуждается, всё-таки один из самых популярных алгоритмов кластеризации. Поэтому по минимуму теории.",
            "DBSCAN (Density-based spatial clustering of applications with noise, плотностной алгоритм пространственной кластеризации с присутствием шума), как следует из названия, оперирует плотностью данных. На вход он просит матрицу близости точек и два параметра — радиус -окрестности и количество соседей .",
            "Эпсилон-окрестность для любого вектора в метрическом признаковом пространстве определяется как множество точек, отстоящих от не более чем на :",
            "где — выбранная метрика (например, евклидовое расстояние).",
            "В общих чертах, алгоритм DBSCAN можно представить как последовательность следующих этапов:",
            "найти новые точки в -окрестности каждой точки и определить основные точки с более чем соседями.",
            "найти связные компоненты основных точек на графе соседей, игнорируя все неосновные точки.",
            "назначить каждую неосновную точку ближайшему кластеру, если кластер является -соседним, в противном случае считаем точку шумом.",
            "Вот так можно использовать DBSCAN из Sci-Kit Learn + с интерактивными ползунками, работает в Colab'е (в Jupyter Notebook какие-то траблы с этим, если кто знает — please, help):",
            "С использованием DBSCAN в Julia и R особых проблем тоже не возникает —",
            "— Julia:",
            "— R:",
            "В идеальном случае DBSCAN может иметь линейную сложность , но не стоит особо на это рассчитывать. Если не пересчитывать каждый раз точек, то ожидаемая сложность — . Худший случай (плохие данные или брутфорс-реализация) — . Наивные реализации DBSCAN любят отъедать памяти под матрицу расстояний — это явно избыточно. Многие версии DBSCAN умеют работать и с более щадящими структурами данных: sklearn и R реализации можно оптимизировать при помощи KD-tree прямо из коробки.",
            "DBSCAN не вычисляет самостоятельно центры кластеров, однако вряд ли это проблема, особенно учитывая произвольную форму кластеров. Зато DBSCAN автоматически определяет выбросы, что довольно здорово.",
            "Соотношение , где — размерность пространства, можно интуитивно рассматривать как пороговую плотность точек данных в области пространства. Ожидаемо, что при одинаковом соотношении , и результаты будут примерно одинаковы. Иногда это действительно так, но есть причина, почему алгоритму нужно задать два параметра, а не один. Во-первых типичное расстояние между точками в разных датасетах разное — явно задавать радиус приходится всегда. Во-вторых, играют роль неоднородности датасета. Чем больше и , тем больше алгоритм склонен «прощать» вариации плотности в кластерах. С одной стороны, это может быть полезно: неприятно увидеть в кластере «дырки», где просто не хватило данных. С другой стороны, это вредно, когда между кластерами нет чёткой границы или шум создаёт «мост» между скоплениями. Тогда DBSCAN запросто соединит две разные группы. В балансе этих параметров и кроется сложность применения DBSCAN: реальные наборы данных содержат кластеры разной плотности с границами разной степени размытости. В условиях, когда плотность некоторых границ между кластерами больше или равна плотности каких-то обособленных кластеров, приходится чем-то жертвовать.",
            "Существуют варианты DBSCAN, способные смягчить эту проблему. Идея состоит в подстраивании в разных областях по ходу работы алгоритма. К сожалению, возрастает количество параметров алгоритма.",
            "Ок, теперь давайте немного поговорим о плюсах и минусах DBSCAN.",
            "Плюсы DBSCAN",
            "• DBSCAN не требует указания числа кластеров в отличие, скажем, от метода k-средних",
            "• DBSCAN может найти кластеры произвольной формы. Он может найти даже кластеры полностью окружённые (но не связанные с) другими кластерами.",
            "• DBSCAN имеет понятие шума и устойчив к выбросам.",
            "• DBSCAN требует лишь двух параметров ( и ) и большей частью нечувствителен к порядку точек в датасете. Однако, точки, находящиеся на границе двух различных кластеров могут оказаться в другом кластере, если изменить порядок точек, а назначение кластеров единственно с точностью до изоморфизма.",
            "Проблемы DBSCAN",
            "• DBSCAN не полностью однозначен — краевые точки, которые могут быть достигнуты из более чем одного кластера, могут принадлежать любому из этих кластеров, что зависит от порядка просмотра точек (тут стоит сказать, что существует DBSCAN❋, который трактует краевые точки как шум и тем самым достигается полностью однозначный результат)",
            "• Качество DBSCAN зависит от способа измерения расстояния. Наиболее часто используемой метрикой расстояний является евклидова метрика. В случае кластеризации данных высокой размерности эта метрика может оказаться почти бесполезной, что делает трудным делом нахождение подходящего значения . Этот эффект, однако, присутствует в любом другом алгоритме, основанном на евклидовом расстоянии.",
            "• DBSCAN не может хорошо разделить на кластеры наборы данных с большой разницей в плотности, поскольку не удается выбрать приемлемую для всех кластеров комбинацию и .",
            "Что ж, теперь давайте теперь переключимся на алгоритм OPTICS (Ordering Points To Identify the Clustering Structure).",
            "Основная идея OPTICS похожа на DBSCAN, но алгоритм предназначен для избавления от одной из главных слабостей алгоритма DBSCAN — проблемы обнаружения кластеров в данных, имеющих различные плотности. Для этого используется граф достижимости, который определяет достижимое расстояние для каждой точки, которая в дальнейшем будет относиться к ближайшему кластеру. Такой подход позволяет ещё лучше определять кластеры разной плотности, особенно если они расположены близко друг к другу, однако это увеличивает время работы алгоритма.",
            "Реализация OPTICS есть в библиотеке Sci-Kit Learn; вот как можно её импортировать и использовать:",
            "С R тоже проблем нет:",
            "Хорошо, давайте немного об особенностях OPTICS",
            "Плюсы OPTICS:",
            "Устойчивость к шуму (впрочем как и у DBSCAN): OPTICS способен обрабатывать данные с шумом и выбросами.",
            "Способность обнаруживать кластеры любой формы",
            "Не требует заранее заданного числа кластеров",
            "Проблемы OPTICS:",
            "Не всегда эффективен для плотных кластеров: OPTICS может иметь проблемы с эффективным обнаружением плотных кластеров, особенно если они имеют сложные формы.",
            "А вот несколько сфер, где регулярно используется OPTICS:",
            "Анализ сетей и обнаружение аномалий: OPTICS используется для анализа социальных сетей, транспортных сетей и других сетевых структур для выявления кластеров и аномалий.",
            "Биоинформатика: OPTICS применяется в биоинформатике для кластеризации геномных данных, выявления генных паттернов и классификации биологических образцов.",
            "Медицинская диагностика: OPTICS может быть применен для кластеризации медицинских данных, таких как результаты тестов, симптомы пациентов и история заболеваний, с целью выявления паттернов заболеваний или групп пациентов схожего профиля. .",
            "Итак, пришло время сравнить DBSCAN и OPTICS",
            "Вот DBSCAN:",
            "...а вот и OPTICS:",
            "И давайте возьмём для начала , , потом поменяем.",
            "Что мы видим? Для данного датасета DBSCAN выделяет кластеры более логичным и понятным способов, но в кластеризации OPTICS тоже есть пара интересных моментов. Как можно увидеть, точки вокруг главных кластеров DBSCAN безнадёжно отмечает как шум, в то время как OPTICS пытается нащупать кластеры и среди этих точек тоже. Это одна из главных фишек OPTICS — метод способен видеть кластеры разной плотности одновременно за счёт того, что он менее чувствителен к параметру .",
            "Вот довольно показательный пример — и тут OPTICS тоже выделил кластер в точках, которые забраковал DBSCAN:",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Описание алгоритма DBSCAN от Sci-Kit Learn",
            "Описание алгоритма OPTICS от Sci-Kit Learn",
            "Наглядная визуализация DBSCAN",
            "Что ж, надеюсь, статья была полезной)",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)"
        ]
    },
    {
        "Название статьи": "Реализация принципа единственной ответственности на Python",
        "Дата публикации": "2024-06-03, 07:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Сегодня мы рассмотрим одну из основополагающих концепций SOLID-принципов — принцип единственной ответственности или сокращенно - SRP. Разберем, что такое SRP и как правильно его применять в Python.",
            "Принцип единственной ответственности гласит, что каждый класс, метод или модуль должен иметь только одну причину для изменения. Проще говоря, каждый компонент вашей системы должен отвечать только за одну функциональность. Т.е если вам нужно внести изменение, связанное с этой функциональностью, вам придется изменить только один компонент.",
            "Когда каждый класс или модуль выполняет одну четко определенную задачу, становится гораздо проще понять его назначение и взаимодействие с другими частями системы.",
            "Что будет, если не соблюдать SRP?",
            "Если класс или модуль берет на себя несколько обязанностей, это приводит к увеличению сложности кода. Такой код сложнее читать, понимать и поддерживать. Также, когда один класс выполняет несколько задач, изменение в одной из них может непредсказуемо повлиять на другие.",
            "Классы, которые нарушают SRP, обычно плохо масштабируются и трудно переиспользуются. Их невозможно легко адаптировать для других целей или проектов.",
            "Для начала рассмотрим класс, который нарушает принцип единственной ответственности. Представим себе класс UserManager, который одновременно отвечает за создание юзера, валидацию данных и сохранение юзера в БД:",
            "Класс нарушает SRP, т.к выполняет несколько задач: валидацию email, создание пользователя и сохранение его в базу данных.",
            "Для исправления нарушения SRP нужно разделить обязанности на отдельные классы: User, UserValidator, UserDatabase, и UserCreator. Каждый класс будет отвечать только за одну задачу:",
            "Теперь каждый класс отвечает за одну конкретную задачу, что соответствует принципу единственной ответственности.",
            "Рассмотрим другой пример, обработку заказов в интернет-магазине. Изначально есть класс, который нарушает SRP, т.к он одновременно обрабатывает заказ, валидирует данные и отправляет уведомления:",
            "Рефакторинг этого класса для соответствия SRP:",
            "Фасадный паттерн помогает упростить взаимодействие между сложными подсистемами, предоставляя простой интерфейс для клиента. С фасадом можно скрыть сложность подсистем и предоставлять единый интерфейс для взаимодействия с ними.",
            "Предположим, есть система обработки заказов, включающая несколько классов для управления заказами, оплатами и уведомлениями. Без фасадного паттерна клиенту пришлось бы взаимодействовать с каждым из этих классов напрямую:",
            "А с использованием фасадного паттерна все будет выглядеть так:",
            "Интерфейсы и абстрактные классы помогают разделить обязанности и четко определить контракт, который должен реализовать класс.",
            "Создание интерфейсов для валидации, сохранения и уведомления:",
            "Разделяем обязанности на интерфейсы, что позволяет каждому классу реализовывать только свои специфические методы, соответствующие SRP.",
            "Для поддержки SRP и других принципов SOLID в Python можно использовать различные библиотеки.",
            "Pylint помогает анализировать код на наличие ошибок и несоответствий стилю, а также выявляет нарушения принципов SOLID, включая SRP.",
            "Mypy - статический анализатор типов для Python, который помогает обнаруживать типовые ошибки и улучшать структуру кода.",
            "Pytest помогает создавать модульные тесты для каждого отдельного компонента.",
            "Dataclasses модуль позволяет создавать классы данных, которые следуют SRP, отделяя логику данных от поведения.",
            "Про другие архитектурные принципы и инструменты коллеги из OTUS рассказывают в рамках практических онлайн-курсов. Также хочу напомнить о том, что в календаре мероприятий вы можете зарегистрироваться на ряд интересных и абсолютно бесплатных вебинаров."
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 12: Дата и время (издание 2024)",
        "Дата публикации": "2024-06-02, 19:47",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это двенадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как работать с датами и временем таким образом, чтобы это работало для всех ваших пользователей, независимо от того, где они проживают.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения",
            "Глава 12: Дата и время (Эта статья)",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Один из аспектов моего приложения для ведения микроблогов, который я долгое время игнорировал, - это отображение дат и времени. До сих пор я просто позволял Python отображать объект datetime в модели User и даже не потрудился отобразить его в модели Post. В этой главе вы узнаете, как работать с этими временными метками.",
            "Ссылки на GitHub для этой главы: Browse, Zip, Diff.",
            "Использование Python на сервере для отображения дат и времени, которые отображаются пользователям в их веб-браузерах, на самом деле не очень хорошая идея, потому что то, что сервер считает своим местным временем, не будет иметь смысла для пользователей, которые живут в другом часовом поясе.",
            "Совершенно ясно, что сервер должен управлять временем, которое является согласованным и независимым от его собственного местоположения и местоположения пользователей. Если это приложение разрастется до такой степени, что потребуется несколько производственных серверов в разных регионах мира, я бы не хотел, чтобы каждый сервер записывал временные метки в базу данных в разных часовых поясах, потому что это сделало бы невозможной работу с этими временами. Поскольку UTC является наиболее используемым единым часовым поясом и поддерживается в классе datetime, именно его я и собираюсь использовать.",
            "В главе 4 вы видели, как создавать временные метки UTC для записей в блоге. В качестве напоминания, вот краткий пример, показывающий, как это было сделано.:",
            "Но с этим подходом связана важная проблема. Пользователям из разных мест будет ужасно сложно определить, когда была сделана публикация, если они будут видеть время в часовом поясе UTC. Им нужно было бы заранее знать, что время указано в UTC, чтобы они могли мысленно подогнать его к своему собственному часовому поясу. Представьте пользователя, скажем, в часовом поясе PDT на Западном побережье США, который публикует что-то в 15: 00 и сразу видит, что сообщение появляется в 10: 00 по времени UTC, или, если быть более точным, в 22: 00. Это будет очень запутанно.",
            "Хотя стандартизация временных меток в соответствии с UTC имеет большой смысл с точки зрения сервера, это создает проблему удобства использования для пользователей. Цель этой главы - представить решение, которое сохраняет все временные метки, управляемые сервером, в часовом поясе UTC, не отталкивая пользователей.",
            "Очевидным решением проблемы является преобразование всех временных меток из сохраненных единиц UTC в местное время каждого пользователя при их отображении. Это позволяет серверу продолжать использовать UTC для обеспечения согласованности, в то время как преобразование \"на лету\", адаптированное к каждому пользователю, решает проблему удобства использования. Сложная часть этого решения - знать местоположение каждого пользователя.",
            "На многих веб-сайтах есть страница конфигурации, где пользователи могут указывать свой часовой пояс. Для этого мне потребуется добавить новую страницу с формой, в которой я представляю пользователям раскрывающийся список часовых поясов. При первом входе на сайт пользователей могут попросить ввести их часовой пояс в рамках регистрации.",
            "Хотя это достойное решение, решающее проблему, немного странно просить пользователей вводить часть информации, которую они уже настроили в своей операционной системе. Кажется, было бы эффективнее, если бы я мог просто получить настройки часового пояса с их компьютеров.",
            "Как выясняется, веб-браузер знает часовой пояс пользователя и предоставляет его через стандартные API JavaScript даты и времени. На самом деле есть два способа воспользоваться информацией о часовом поясе, доступной через JavaScript:",
            "Подход \"старой школы\" заключался бы в том, чтобы веб-браузер каким-то образом отправлял информацию о часовом поясе на сервер, когда пользователь впервые входит в приложение. Это можно было бы сделать с помощью вызова Ajax или гораздо проще с помощью мета-тега обновления. Как только сервер узнает часовой пояс, он может сохранить его в сеансе пользователя или записать в таблицу users в базе данных, и с этого момента корректировать с его помощью все временные метки во время отображения шаблонов.",
            "Подход \"новой школы\" заключается в том, чтобы ничего не менять на сервере и позволить преобразованию UTC в местный часовой пояс происходить в браузере с использованием JavaScript.",
            "Оба варианта допустимы, но второй имеет большое преимущество. Знания часового пояса пользователя не всегда достаточно для представления дат и времени в формате, ожидаемом пользователем. Браузер также имеет доступ к конфигурации языкового стандарта системы, которая определяет такие параметры, как время утра / вечера в сравнении с 24-часовыми часами, формат отображения даты DD / MM / ГГГГ в сравнении с MM / DD / ГГГГ и многие другие культурные или региональные стили.",
            "И если этого недостаточно, у подхода новой школы есть еще одно преимущество. Есть библиотека с открытым исходным кодом, которая выполняет всю эту работу!",
            "Moment.js это небольшая библиотека JavaScript с открытым исходным кодом, которая выводит отображение даты и времени на новый уровень, поскольку предоставляет все мыслимые варианты форматирования, а затем и некоторые другие. Некоторое время назад я создал Flask-Moment, небольшое расширение Flask, которое позволяет очень легко интегрировать moment.js в ваше приложение.",
            "Итак, давайте начнем с установки Flask-Moment:",
            "Это расширение добавляется в приложение Flask обычным способом:",
            "app/__init__.py: Пример Flask-Moment.",
            "В отличие от других расширений, Flask-Moment работает вместе с moment.js, поэтому все шаблоны приложения должны включать эту библиотеку. Чтобы гарантировать, что эта библиотека всегда доступна, я собираюсь добавить ее в базовый шаблон. Это можно сделать двумя способами. Самый прямой способ - явно добавить тег <script>, который импортирует библиотеку, но Flask-Moment упрощает задачу, предоставляя функцию moment.include_moment(), которая генерирует тег <script>:",
            "app/templates/base.html: Включить moment.js в базовый шаблон.",
            "В большинстве случаев библиотеки JavaScript, используемые приложением, включены в конец содержимого <body>, где находится загрузочный JavaScript-код.",
            "Moment.js делает класс moment доступным для браузера. Первым шагом для отображения временной метки является создание объекта этого класса, передающего желаемую временную метку в формате ISO 8601. Вот пример, запущенный в консоли JavaScript браузера:",
            "Если вы не знакомы со стандартным форматом даты и времени ISO 8601, этот формат выглядит следующим образом:",
            "Я уже решил, что буду работать только с часовыми поясами UTC, поэтому последней частью всегда будет +00:00 или в некоторых случаях эквивалент Z, который представляет UTC в стандарте ISO 8601.",
            "В объекте moment предусмотрено несколько методов для различных вариантов рендеринга. Ниже приведены некоторые из наиболее распространенных вариантов:",
            "В этом примере создается объект moment, инициализированный 28 июня 2021 года в 21:45 по Гринвичу. Вы можете видеть, что все параметры, которые я пробовал выше, отображаются в UTC + 1, который является часовым поясом, настроенным на моем компьютере. Вы можете ввести вышеуказанные команды в консоли вашего браузера, убедившись, что в странице, на которой вы открываете консоль, включена moment.js. Вы можете сделать это в микроблоге, при условии, что вы внесли вышеуказанные изменения для включения moment.js, или также на https://momentjs.com/.",
            "Обратите внимание, как разные методы создают разные представления. С помощью метода format() вы управляете форматом выходных данных с помощью строки формата. Метод fromNow() интересен тем, что он отображает временную метку по отношению к текущему времени, поэтому вы получаете выходные данные, такие как \"минуту назад\" или \"через два часа\" и т.д.",
            "Если вы работали непосредственно в JavaScript, приведенные выше вызовы возвращают строку с отображаемой временной меткой. Затем вам предстоит вставить этот текст в нужное место на странице, что, к сожалению, требует работы с DOM. Расширение Flask-Moment значительно упрощает использование moment.js за счет включения в ваши шаблоны объекта moment, аналогичного объекту JavaScript.",
            "Давайте посмотрим на временную метку, которая отображается на странице профиля. Текущий шаблон user.html позволяет Python генерировать строковое представление времени. Теперь я могу отобразить эту временную метку с помощью Flask-Moment следующим образом:",
            "app/templates/user.html: Отрисовка временной метки с помощью moment.js.",
            "Итак, как вы можете видеть, Flask-Moment использует синтаксис, аналогичный синтаксису библиотеки JavaScript, с одним отличием, заключающимся в том, что аргументом для moment() теперь является объект Python datetime, а не строка ISO 8601. Вызов moment(), выполняемый из шаблона, автоматически генерирует необходимый код JavaScript для вставки отображаемой временной метки в нужное место DOM.",
            "Второе место, где я могу воспользоваться преимуществами Flask-Moment, находится во вложенном шаблоне _post.html, который вызывается с главной страницы и страницы пользователя. В текущей версии шаблона каждому сообщению предшествует строка \"username says:\". Теперь я могу добавить временную метку, отображаемую с помощью fromNow():",
            "app/templates/_post.html: Отрисовка временной метки во вложенном шаблоне post.",
            "Ниже вы можете увидеть, как выглядят обе эти временные метки при рендеринге с помощью Flask-Moment и moment.js:"
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 11: Дизайн приложения (издание 2024)",
        "Дата публикации": "2024-06-02, 19:46",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это одиннадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как заменить базовые HTML-шаблоны новым набором, основанным на платформе пользовательского интерфейса Bootstrap.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения (Эта статья)",
            "Глава 12: Даты и время",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Вы уже некоторое время играете с моим приложением для ведения микроблогов, поэтому, я уверен, вы заметили, что я не потратил слишком много времени на то, чтобы оно выглядело хорошо, или, лучше сказать, я вообще не тратил на это времени. Шаблоны, которые я собрал, довольно простые, без какого-либо пользовательского оформления. Мне было полезно сосредоточиться на реальной логике приложения, не отвлекаясь на написание красивых HTML и CSS.",
            "Но я уже долго сосредоточен на серверной части этого приложения. Итак, в этой главе я сделаю перерыв и потрачу некоторое время на то, чтобы показать вам, что можно сделать, чтобы приложение выглядело немного более отточенным и профессиональным.",
            "Эта глава будет немного отличаться от предыдущих, потому что я не собираюсь так подробно, как обычно, описывать сторону Python, которая, в конце концов, является основной темой этого туториала. Создание красивых веб-страниц - обширная тема, которая в значительной степени не связана с веб-разработкой на Python, но я расскажу о некоторых основных рекомендациях и идеях о том, как подойти к этой задаче, и у вас также будет приложение с измененным дизайном, которое можно изучить и перенять опыт.",
            "Ссылки на GitHub для этой главы: Обзор, Zip, Diff.",
            "Хотя мы можем утверждать, что программирование - это сложно, наши усилия ничто по сравнению с усилиями веб-дизайнеров, которым приходится создавать веб-страницы, которые красиво и единообразно выглядят в списке веб-браузеров. За последние годы они стали лучше, но в некоторых браузерах все еще есть непонятные ошибки или причуды, которые сильно усложняют задачу создания веб-страниц, которые везде выглядят красиво. Это еще сложнее, если вам также нужно настроить для браузеров планшетов и смартфонов с ограниченным количеством ресурсов и экранов.",
            "Если вы, как и я, разработчик, который просто хочет создавать прилично выглядящие веб-страницы, но у вас нет времени или интереса изучать низкоуровневые механизмы для эффективного достижения этой цели путем написания необработанного HTML и CSS, то единственным практическим решением является использование CSS фреймворка для упрощения задачи. Выбрав этот путь, вы потеряете некоторую творческую свободу, но, с другой стороны, ваши веб-страницы будут хорошо выглядеть во всех браузерах без особых усилий. Фреймворк CSS предоставляет коллекцию высокоуровневых классов CSS с готовыми стилями для распространенных типов элементов пользовательского интерфейса. Большинство этих фреймворков также предоставляют дополнения JavaScript для вещей, которые нельзя выполнить строго с помощью HTML и CSS.",
            "Одним из самых популярных CSS-фреймворков является Bootstrap. Если вы хотите увидеть, какие страницы можно создавать с помощью этого фреймворка, в документации есть несколько примеров.",
            "Вот некоторые преимущества, которые вы получаете при использовании Bootstrap для оформления ваших веб-страниц:",
            "Аналогично выглядит во всех основных веб-браузерах",
            "Настройка размеров для экранов настольных компьютеров, планшетов и телефонов",
            "Настраиваемые макеты",
            "Красиво оформленные панели навигации, формы, кнопки, оповещения, всплывающие окна и т.д.",
            "Самый простой способ использовать Bootstrap - это просто импортировать файл bootstrap.min.css в ваш базовый шаблон. Вы можете либо загрузить копию этого файла и добавить его в свой проект, либо импортировать его непосредственно из CDN. Затем вы можете начать использовать CSS-классы общего назначения, которые он предоставляет, согласно документации, что довольно неплохо. Возможно, вы также захотите импортировать JavaScript-код фреймворка, чтобы использовать самые продвинутые функции.",
            "Как и большинство проектов с открытым исходным кодом, Bootstrap постоянно развивается. Оригинальная версия мега-учебника Flask была создана для Bootstrap 3. Редакция, которую вы сейчас читаете, создана для Bootstrap 5.3. Текущий подход к интеграции Bootstrap является довольно общим и может быть адаптирован к более новым версиям Bootstrap.",
            "Первым шагом в интеграции Bootstrap с Microblog является добавление его файлов CSS и JavaScript в базовый шаблон. На странице быстрого запуска Bootstrap в качестве примера приведена короткая, но полная HTML-страница, которую я копирую ниже для вашего удобства:",
            "Подход, который я могу применить, чтобы объединить это с моим шаблоном base.html, заключается в том, чтобы использовать приведенный выше в качестве нового базового шаблона, заменив теги <title> и <h1> заголовком и основным содержимым исходного базового шаблона соответственно.",
            "Следующий шаг - заменить базовую панель навигации на более удобную из Bootstrap. На странице документации по панели навигации Bootstrap вверху показан хороший пример. Используя этот пример в качестве руководства, я создал панель навигации со ссылками \"Index\", \"Explore\", \"Profile\", \"Login\" и \"Logout\" из микроблога. Для удобства я настроил профиль, а также ссылки для входа и выхода так, чтобы они отображались в крайнем правом углу.",
            "При использовании Bootstrap полезно знать о некоторых базовых примитивах компоновки. Одним из наиболее важных является контейнер, который определяет область содержимого страницы. Два основных контейнера называются container и container-fluid. В первом случае страница настраивается на использование одной из пяти предопределенных ширин страницы и центрирует содержимое в окне браузера. С другой стороны обтекающий контейнер дает вам доступ ко всей ширине страницы. Для этого приложения я решил использовать контейнер по умолчанию, потому что он предотвращает слишком широкое расширение страницы независимо от размера экрана, поэтому часть содержимого страницы будет заключена в один из этих контейнеров следующим образом:",
            "Последняя часть HTML-разметки в шаблоне base.html, которую необходимо адаптировать, - это раздел, отображающий отображаемые сообщения. Компонент Alert от Bootstrap прекрасно подходит для этой задачи.",
            "Вы можете получить полностью переработанный шаблон base.html из репозитория Github для этой главы. Ниже вы можете увидеть упрощенную структуру, если хотите иметь представление о том, как она выглядит.:",
            "app/templates/base.html: Переработанный базовый шаблон.",
            "Благодаря обновленному базовому шаблону внешний вид приложения уже заметно улучшен без необходимости изменять строки кода Python. Если вы хотите убедиться в этом сами, загрузите копию base.html из репозитория GitHub по ссылкам, приведенным в начале этой главы.",
            "Область, в которой Bootstrap проделывает фантастическую работу, заключается в рендеринге полей формы, которые выглядят намного приятнее и чище, чем поля по умолчанию, предоставляемые браузером. В документации по Bootstrap также есть раздел о формах. В начале этого раздела приведен пример формы входа в систему, который показывает базовую структуру HTML.",
            "HTML-код, необходимый для каждого поля, довольно длинный. Ниже вы можете увидеть одно из текстовых полей из примера формы в документации:",
            "Но это слишком просто для нужд Microblog, который включает проверку полей и, возможно, потребуется показывать пользователю ошибки проверки. На странице документации есть раздел о проверке на стороне сервера, в котором показано, как оформить поля с сообщением об ошибке. Вот пример.:",
            "К сожалению, о необходимости вводить такое количество шаблонов для каждого поля в каждой форме не может быть и речи. Это заняло бы слишком много времени и чревато ошибками. Одним из решений является использование макросов Jinja, которые позволяют вам определять повторно используемые фрагменты HTML, а затем вызывать их из ваших шаблонов, как если бы они были функциями.",
            "Например, макрос Jinja для текстового поля, подобного показанному выше, будет иметь вид:",
            "Обратите внимание, как используются условные обозначения для выборочного добавления стиля ошибки, если поле содержит одно или несколько сообщений об ошибках.",
            "Поскольку макрос определен в файле с именем bootstrap_wtf.html, который расположен в каталоге templates, он может быть вызван, когда потребуется отобразить поле. Например:",
            "Макрос отображения полей можно расширить, чтобы он также поддерживал отображение флажков, раскрывающихся списков выбора, кнопок отправки и других типов полей. Он также может принимать второй аргумент с логическим значением, указывающим, следует ли автоматически переводить поле в фокус страницы, что должно быть сделано для первого поля формы. Для еще большего удобства можно создать другой макрос для рендеринга всей формы, просто перебрав поля формы и вызвав form_field() макрос для каждого из них.",
            "Полный bootstrap_wtf.html файл доступен в репозитории GitHub, ссылка на который приведена в начале этой главы. Он включает в себя более полную версию макроса form_field(), показанного выше, и второй макрос с именем quick_form(), который принимает объект формы и отображает все его поля с помощью первого макроса.",
            "Как это выглядит, когда реализовано в реальной форме? Ниже вы можете увидеть переработанный шаблон register.html в качестве примера:",
            "app/templates/register.html: Шаблон регистрации пользователя.",
            "Разве это не здорово? Оператор import вверху работает аналогично импорту Python на стороне шаблона. Который добавляет макрос wtf.quick_form(), который в одной строке кода отображает полную форму, включая ошибки проверки, и все оформлено в соответствии с фреймворком Bootstrap.",
            "Еще раз, я не собираюсь показывать вам все изменения, которые я сделал для других форм в приложении, но все эти изменения внесены в шаблоны, которые вы можете загрузить или просмотреть на GitHub.",
            "Логика представления, которая отображает отдельные записи в блоге, была абстрагирована в подшаблон под названием _post.html. Все, что мне нужно сделать с этим шаблоном, это внести некоторые незначительные корректировки, чтобы он хорошо выглядел в Bootstrap.",
            "app/templates/_post.html: Переработанный подшаблон публикации.",
            "Ссылки на страницы - это еще одна область, в которой Bootstrap предоставляет поддержку. Для этого я просто еще раз обратился к документации Bootstrap и адаптировал один из их примеров. Вот как это выглядит на странице index.html:",
            "app/templates/index.html: Переработаны ссылки на страницы.",
            "Обратите внимание, что в этой реализации вместо скрытия следующей или предыдущей ссылки, когда в этом направлении больше нет содержимого, я применяю отключенное состояние, из-за которого ссылка будет отображаться серым цветом.",
            "Я не собираюсь показывать это здесь, но аналогичное изменение необходимо применить к шаблону user.html. Пакет для загрузки этой главы включает эти изменения.",
            "Чтобы внести в ваше приложение эти изменения, пожалуйста, загрузите zip-файл для этой главы и соответствующим образом обновите свои шаблоны.",
            "Ниже вы можете увидеть несколько фотографий до и после, чтобы увидеть трансформацию. Имейте в виду, что это изменение было достигнуто не затрагивая ни одной строки кода приложения!",
            "Следующая глава =>"
        ]
    },
    {
        "Название статьи": "Расширяем возможности Keras с помощью кастомных слоев",
        "Дата публикации": "2024-06-02, 18:07",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Keras предоставляет мощные инструменты для создания сложных нейронных сетей. Однако иногда стандартного набора слоев недостаточно для решения некоторых задач. В таких случаях на помощь приходят кастомные слои.",
            "Кастомные слои позволяют адаптировать архитектуру модели под особенности данных, улучшая тем самым производительность и точность моделек.",
            "Каждый кастомный слой начинается с определения нового класса, наследующего от tf.keras.layers.Layer. В __init__ происходит инициализация слоя, где можно задать параметры, необходимые для работы слоя:",
            "Тут units определяет количество нейронов, а activation указывает функцию активации. super(CustomLayer, self).__init__(**kwargs) вызывает конструктор базового класса Layer.",
            "Метод build вызывается Keras при первом использовании слоя. Его юзают для создания параметров слоя, которые зависят от размера входных данных:",
            "В методе создаются веса kernel и bias. Функция add_weight создает и регистрирует переменные слоя, которые будут обновляться во время тренировки.",
            "Метод call содержит основную логику вычислений слоя. Он принимает входные данные и возвращает выходные:",
            "В этом методе выполняется умножение входных данных на веса и добавление смещения. Если определена функция активации, она применяется к выходным данным.",
            "После определения кастомного слоя его можно использовать в моделях Keras как обычный слой:",
            "Другие полезные методы:",
            "add_weight: Добавляет переменную веса в слой.",
            "compute_output_shape: Возвращает форму выходных данных на основе формы входных.",
            "get_config: Возвращает конфигурацию слоя в виде словаря, что полезно для сериализации.",
            "Dense слой выполняет простую линейную операцию: умножение входного вектора на матрицу весов и добавление смещения, а затем применяется функция активации:",
            "Convolutional слои применяют свертку фильтра к входным данным, что позволяет выделять пространственные особенности:",
            "Recurrent слои используются для обработки последовательных данных. Один из наиболее распространнных типов рекуррентных слоев — это LSTM:",
            "Dropout слой используется для регуляризации модели, предотвращая переобучение путем случайного зануления некоторых нейронов во время тренировки:",
            "BatchNormalization слой нормализует активации предыдущего слоя, улучшая скорость обучения модельки:",
            "Больше практических инструментов и кейсов коллеги из OTUS рассматривают в рамках практических онлайн-курсов. Напомню, что с полным каталогом курсов можно ознакомиться по ссылке."
        ]
    },
    {
        "Название статьи": "Enbeddrus — обучение независящей от языка эмбеддинг-модели",
        "Дата публикации": "2024-06-02, 17:31",
        "Автор статьи": "efreelancer ",
        "Статья целиком": [
            "Приветствую, хабровчане!",
            "Сегодня хочу рассказать вам историю о том, как я обучил простую и компактную независящую от языка (language agnostic) модель-эмбеддер, которая умеет работать с техническими текстами о PHP и способна извлекать схожие эмбеддинги для параллельных текстов на английском и русском языках.",
            "Основная причина, по которой я решил заняться этим проектом, заключается в том, что мои заметки, код и документация, накопленные за более чем десять лет практики, представляют собой солянку текстов о разных технологиях, языках программирования, пометки о настройке серверов Linux и т.д. на русском и английском языках. Поэтому мне захотелось сделать Retrieval-Augmented Generation (RAG) помогалку, которая сможет принимать запросы пользователя (меня) и эффективно находить информацию в столь разношерстой базе данных, независимо от того на каком языке я сделал запрос и на каком языке написана документация.",
            "Для достижения этой цели как-раз и необходима независимая от языка модель-эмбеддер, которая будет одинаково хорошо работать с техническими текстами на русском и английском языках.",
            "Ещё одним важным аспектом было то, чтобы модель потребляла как можно меньше ресурсов и, если возможно, чтобы её можно было преобразовать в формат GGUF.",
            "Но прежде чем приступить к созданию своего собственного велосипеда, я решил поискать готовые решения, ведь подобная идея очевидна и, возможно, уже реализована другими.",
            "Спойлер: идея не нова, и подобных решений уже достаточно много.",
            "Для построения системы, которая может извлекать одинаковые эмбеддинги для схожих текстов на русском и английском языках, существует несколько решений, например...",
            "Ссылки: arxiv:1907.04307 , kaggle, github",
            "Это проект разработан инженерами Google и поддерживает 16 языков.",
            "Свойства: ~110m параметров, принимает на вход 128 токенов текста и извлекает из них 512-мерный эмбеддинг.",
            "Плюс: поддерживает русский язык.",
            "Минусы: модель основана на Tensorflow, а так же что с 2019го года не было обновлений.",
            "Ссылки: arxiv:1710.04087, github",
            "Это одна из первых попыток инженеров FB создать модель которая способна выполнять задачи по извлечению независящих от языка эмбеддингов.",
            "Плюс: поддерживает русский язык.",
            "Минусы: в наличии имеются веса для пар языков, навроде en-ru, en-de и т.д., весов нет на HuggingFace, ну и с 2018го года проект не развивается.",
            "Ссылки: arvix:2205.12654, github, pypi",
            "Ещё одна модель разработана инженерами FB и, как сказано в ридми на GitHub, поддерживает более 200 языков (хотя если пройти по ссылочкам и посчитать то получится 147 языков).",
            "Свойства: ~256m параметров, принимает 1024 токенов на вход и извлекает из них 1024-мерный эмбеддинг.",
            "Плюсы: она основана на PyTorch и имеет логику переключения между языками которая явно перекочевала из NLLB (о которой я кстати рассказывал в публикации \"Перевод на разные языки используя модель NLLB\" у себя в блоге на Дзен).",
            "Минусы: весов нет на HuggingFace, а модель несовместима с llama.cpp поэтому её не получится конвертировать в GGUF, чтобы можно было запускать на слабом железе (или же в паре с ollama).",
            "Ссылки: arXiv:1908.10084, сайт",
            "Модели Sentence-BERT представляют собой модифицированную версию предобученной BERT, специально адаптированную для генерации эмбеддингов предложений, multilingual версия позволяет извлекать эмбеддинги из текста на разных языках, а paraphrase модели позволяют извлекать похожие эмбеддинги парафраз на разных языках.",
            "Вот пару примечательных моделей, обученных разными способами:",
            "paraphrase-multilingual-MiniLM-L12-v2 имеет 118m параметров, принимает 256 токенов на вход и возвращает 384-мерный эмбеддинг.",
            "paraphrase-multilingual-mpnet-base-v2 имеет 278m параметров, принимает на вход 512 токенов и возвращает 768-мерный эмбеддинг.",
            "Обе эти модели обучены на комбинации из датасетов:",
            "SNLI о котором говорится в публикации \"A large annotated corpus for learning natural language inference\" (570k примеров)",
            "Multi-Genre NLI, подробнее в работе \"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\" (433k примера)",
            "Плюсы: поддерживает русский язык, можно конвентировать в GGUF.",
            "Минусы: модели не очень хорошо понимают технический текст (особенно русский технический жаргон), нет версии в формате GGUF, и к числу фатальных недостатков могу отнести, что эти модели обучил не я ;)",
            "Пришёл к выводу, что тему обучения подобных модей-эмбеддеров уже достаточно хорошо изучили и что можно без особых сложностей реализовать мою задумку.",
            "В качестве базовой модели решил взять модель google-bert/bert-base-multilingual-uncased, потому что:",
            "У этой крохи всего 168m параметров, что чуть больше чем у paraphrase-multilingual-MiniLM-L12-v2, но меньше чем у paraphrase-multilingual-mpnet-base-v2;",
            "На вход она принимает 512 токенов, а на выходе возвращает 768-мерный эмбеддинг, столько же у paraphrase-multilingual-mpnet-base-v2;",
            "Модель обучена на датасете wikipedia представляющем из себя Text Corpora, а там, сами понимаете, примеров текста больше, чем SNLI и Multi-Genre NLI вместе взятые;",
            "Модель uncased, то есть обучение происходило на регистронезависимых текстах (сиречь всё переводилось в lowercase).",
            "С моделью определились, теперь перейдём к вопросу выбора датасета...",
            "Изначально я хотел собрать больше датасетов, но, собирая датасет по PHP, я понял, какой это трудоёмкий процесс, и решил уменьшить свои амбиции.",
            "Итак, после поиска в интернете я нашёл только один подходящий датасет: OPUS PHP v1 на 2k примеров, содержащий пары текстов на русском и английском языках, по теме PHP.",
            "Из указанного датасеста я использовал только английский корпус (так как русский корпус был очень низкого качества), далее задействовал инстанс LibreTranslate для перевода английских текстов на русский и очистил данные от аномалий и шума (сценарий dataset_php_build.ipynb). Затем вручную перевёл кривые места с помощью Google и Yandex Translate и экспортировал результат в CSV формат. Данные отсортировал и удалил дубликаты (сценарий dataset_php_undup.py) после чего осталось 1.6k примеров.",
            "В финале попросил ChatGPT сгенерировать 100 примеров пар технического текста о PHP на русском и английском языках для сплита eval, а очищенные данные использовал для сплита train.",
            "Результат выгрузил (сценарий dataset_php_publish.ipynb) на HuggingFace: evilfreelancer/opus-php-en-ru-cleaned .",
            "Для создания эффективного эмбеддера, способного работать с техническими текстами о PHP на русском и английском языке, я решил провести обучение модели в два этапа, сначала выполнить Domain Adaptation, чтобы модель могла работать с техническими текстами на английском языке, а после этого обучить её на Parallel Corpora из русских и английских текстов.",
            "Для Domain Adaptation я использовал метод Generative Pseudo Labeling (GPL) (arXiv:2112.07577), данный метод позволяет проводить обучение модели на основе неразмеченных данных, генерируя псевдометки и улучшая качество работы модели для специфических доменов.",
            "Библиотека gpl имеет захардкоженный формат входного датасета и читает данные по определённым путям, поэтому пришлось слегка конвертировать тренировочный датасет и положить результат в директорию datasets (сценарий: dataset_php_convert.py).",
            "Для адаптации модели bert-base-multilingual-uncased к домену английских текстов про PHP я использовал в качестве шаблона скрипт, предложенный авторами проекта GPL на их странице на GitHub, получился следующего вида код:",
            "Полный скрипт тренировки train_domain.py можно найти в репозитории проекта на GitHub.",
            "Процесс обучения включает в себя несколько этапов:",
            "Используется генератор запросов, такой как BeIR/query-gen-msmarco-t5-base-v1, для создания синтетических запросов на основе текстов из корпуса;",
            "С помощью ретриверов, таких как msmarco-distilbert-base-v3 и msmarco-MiniLM-L-6-v3, которые работают с косинусным сходством, извлекаются наиболее релевантные документы для сгенерированных запросов;",
            "Кросс-энкодер, такой как cross-encoder/ms-marco-MiniLM-L-6-v2, используется для создания псевдометок, присваивая оценочные метки соответствия между запросами и документами;",
            "Модель обучается с использованием MarginMSELoss, которая позволяет модели лучше адаптироваться к новому домену.",
            "И так, наша модель обучена работать с новым доменом, поэтому переходим к следующему шагу.",
            "Для обучения модели на параллельных корпусах я использовал метод обучения моделей на разных языках, описанный в примере на сайте Sentence Transformers. Этот метод позволяет обучать мультиязычные модели, используя параллельные тексты на разных языках (заготовка скрипта make_multilingual.py).",
            "Для оценки качества модели я написал юпитер-блокнот, который загружает базовую и дообученную модель, прогоняет пары из eval сплита датасета evilfreelancer/opus-php-en-ru-cleaned и анализирует разницу между эмбеддингами, построенными для текстов на разных языках. Результаты визуализируются в виде графиков. Скрипт можно найти здесь.",
            "На графике видно, что базовая модель bert-base-multilingual-uncased распределяет русские и английские тексты в изолированные кластеры точек, ну а наша задача сделать так, чтобы эти точки были расположены как можно ближе друг к другу.",
            "Подобную задачу позволяет решать MSELoss, так как она минимизирует разницу между эмбеддингом, сгенерированным моделью-учителем (на английском языке) и эмбеддингом, сгенерированным моделью-учеником (на русском языке).",
            "Теперь пару слов про датасеты, решил остановиться на следующем наборе:",
            "evilfreelancer/opus-php-en-ru-cleaned (1.6k) - ранее созданный датасет параллельных текстов на английском и русском языках;",
            "Helsinki-NLP/opus_books (17.5k) - датасет OPUS параллельных текстов из книг.",
            "Выбрал я их потому, что мои первые эксперименты с обучением модели на только PHP датасете показали, что у модели происходит overfitting в результате чего падала общее качество работы модели, поэтому самым логичным решением было добавить ещё один Parallel Corpora общего назначения.",
            "Помимо этого в скрипт обучения я хотел сразу заложить возможность обучать на множестве разных датасетов (имеющих разные форматы данных), в результате чего получилась функция:",
            "В дальнейшем планирую добавить в неё больше датасетов на разные технические темы, но на этапе прототипирования того что есть более чем достаточно.",
            "Двигаемся дальше.",
            "Полный скрипт тренировки train_parallel.py можно найти в репозитории проекта на GitHub, в качестве модели-учителя возьмём google-bert/bert-base-multilingual-uncased, а в качестве модели-ученика ту, что мы обучили ранее на шаге Domain Adaptation.",
            "Обучение происходит в несколько этапов:",
            "Сначала мы загружаем датасеты (функция read_datasets);",
            "Далее выполняем их преобразование в нужный формат, после чего сохраняем на диске (функциия prepare_datasets)",
            "Инициализируем модель-учитель и модель-ученик (тут)",
            "Инициализируем MSELoss, передав ей на вход указатель на модель-ученика (тут)",
            "Запускаем обучение модели-ученика",
            "По завершению обучению давайте попробуем протестировать модель и понять стала ли на лучше извлекать эмбеддинги.",
            "Как видно на графике эмбеддинги извлечённые из русских и английских текстов где-то наложились друг на друга, точность похожести поднялась с 0.83 до 0.94, при этом модель также хорошо разделяет фразы различающиеся по смыслу.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1-domain",
            "Посмотрел я на этот график и пришла в голову мысль, а что если попробовать обучить базовую модель сразу на Parallel Corpora, пропустив шаг с Domain Adaptation?",
            "Правим скрипт тренировки, меняем модель-ученика, получается вот так:",
            "Опять запускаем тренировку и ждём некоторое время, по завершению прогоняем тесты и смотрим что получилось.",
            "Как видно на графиках если обучать сразу на Parallel Corpora модель быстрее, так как не нужно выполнять Domain Adaptation, и лучше обучается извлекать эмбеддинги из параллельных текстов, ведь косинусное расстояние в таком случае между близкими по смыслу фразами на разных языках в среднем в районе 0.97, что выше чем у модели изначально обученной на домене текстов про PHP.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1",
            "Отсюда можно сделать вывод, что дообучение мультиязыковой модели bert-base-multilingual-cased через Domain Adaptation с последующем обучением на Parallel Corpora не имеет особого смысла и проще сразу дообучать её на Parallel Corpora.",
            "Осталось выполнить самую малость, для начала я хочу конвертировать модель в формат GGUF, чтобы можно было использовать обученные модели через llama.cpp, но на этом моменте сильно не будем заострять внимание, сошлюсь на мою публикацию \"Как конвертировать модель BERT в формат GGUF?\" в моём блоге и PR который я создал в проекте llama.cpp.",
            "Но если кратко команды конвертации нужно выполнять и корня проекта llama.cpp и выглядят они следующим образом:",
            "По её завершению в директории models появятся файлы: enbeddrus-v0.1-f16.gguf и enbeddrus-v0.1-domain-f16.gguf.",
            "Полученные модели я выгрузил на серверы Ollama следующим образом:",
            "Выгруженные модели находятся тут и скачать их можно следующей командой:",
            "Содержимое Modelfile'ов можно найти в директории models проекта на GitHub.",
            "https://github.com/EvilFreelancer/enbeddrus",
            "https://huggingface.co/datasets/evilfreelancer/opus-php-en-ru-cleaned",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1-domain",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1",
            "https://ollama.com/evilfreelancer/enbeddrus",
            "Благодаря работе над проектом enbeddrus были достигнуты следующие цели:",
            "Удалось разобрался с тем как подобные модели устроены и как они работают, а так же с тем как их можно обучать;",
            "Был собран датасет с Parallel Corpora тематических текстов о PHP на русском и английском;",
            "Удалось разобраться с методами оценки моделей, а также с тем как эту оценку красиво визуализировать;",
            "Была обучена модель, которая эффективно работает с текстами на двух языках и может быть использована в RAG-системе для поиска и анализа информации.",
            "Полученные результаты подтверждают, что обучение мультиязычных эмбеддеров на основе параллельных корпусов является эффективным подходом для создания моделей, способных работать с текстами на разных языках.",
            "Спасибо за внимание и за что дочитал публикацию до конца! Если у вас есть вопросы или вы хотите связаться со мной, ссылки на мои контакты в социальных сетях можно найти в моём профиле на Хабре."
        ]
    },
    {
        "Название статьи": "Я научу вас неправильно играть в Hearts of iron. Оптимизация довоенной экономики: часть 2",
        "Дата публикации": "2024-06-02, 17:02",
        "Автор статьи": "Glasssparrow ",
        "Статья целиком": [
            "В прошлой части мы создали инструментарий, настало время им воспользоваться.",
            "За долю секунды мы можем провести симуляцию нескольких внутриигровых лет, что позволяет нам применить простейший метод исследования - метод перебора. И, раз уж мы всё равно будем перебирать, стоит также построить графики.",
            "В качестве испытуемой страны мы выберем, конечно, Советский Союз, условия будем выбирать близкие к реальному прохождению.",
            "Во-первых, рассмотрим торговлю. Торговля в игре зависит от многих факторов и не может быть оценена в симуляции, т.к. мы не работаем со всем миром (это потребует больших вычислительных мощностей). Таким образом, торговлю можно взять только из игры, что я и сделал, прокрутив 5 внутриигровых лет и записав количество фабрик получаемых от торговли. При этом закупки брались равными нулю (закупки сильно зависят от того что производит игрок, потому для общего случая я их просто игнорировал).",
            "1 установить_торговлю 18 120 установить_торговлю 10 210 установить_торговлю 7 365 установить_торговлю 5 730 установить_торговлю 9 1100 установить_торговлю 13 1450 установить_торговлю 25 1800 установить_торговлю 18",
            "Во-вторых, рассмотрим технологии. Нам важны технология индустрии и технология строительства. Моменты их развития также были получены из игры следующим образом: технологии исследовались без опережения по времени (с некоторыми погрешностями, конечно), в первую очередь строительство и индустрия, во вторую электроника, всё остальное исследовалось лишь для того чтобы сбить накапливающиеся во время простоя дополнительные 30 дней исследований. Никаких фокусов, решений и политик на исследования использовано не было.",
            "188 construction_tech # технология строительства 1 328 industry_tech # технология индустрии 1 511 construction_tech # технология строительства 2 511 industry_tech # технология индустрии 2 1285 construction_tech # технология строительства 3 1285 industry_tech # технология индустрии 3 1950 construction_tech # технология строительства 4 1950 industry_tech # технология индустрии 4",
            "В-третьих, обратим внимание на фокусы. Рассмотрим 4 варианта: 1) Стандартное быстрое закрытие паранойи с советником на гражданское строительство и частичной мобилизацией2) Оно же, но дополнительно поставим свободную торговлю, когда будет политка3) Также быстро закрываем паранойю, но частичную мобилизацию берем до советника на гражданское строительство (это будет стоить нам 30 политки)4) Вообще никаких фокусов не берем, только советник и частичная мобилизация.",
            "sov # тэг страны.140 добавить_гражданского_советника # если идти по пути Сталина то политки как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранойи 350 добавить_лимит_фабрик 0.1 # фокус ветки паранойи 350 добавить_бонус_строительства 0.05 # фокус ветки паранойи 350 добавить_фабрики 2 # фокус ветки паранойи 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov140 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная #320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная",
            "В общем и целом, получаем достаточно неплохие бонусы на промышленность, но отнюдь не максимальные (есть еще плакаты на -15% товаров народного потребления, есть множество других бонусов на промышленность в фокусах), но для общего случая этого и не нужно, мы (пока) не собираемся ограничивать игрока в свободе выбора фокусов.",
            "Рассмотрим получившиеся графики зависимости максимума количества военных заводов от количества фабрик которые мы строим (это важно, фабрики от фокусов в счетчик не идут) до переключения на военную промышленность.",
            "Видим что 5% от свободной торговли дают нам лишь 4 завода, при том что 2 завода мы можем получить за 30 политки, просто взяв частичную мобилизацию до советника. Скупают ресурсы у СССР или не так активно чтобы свободная торговля была в плюс, или слишком поздно, когда ресурсы уже нужны для производства.",
            "Также видим что бонусы от фокусов в ветке паранойи дают большое преимущество: 116 заводов против 145 (если мы не берем скидку, то нет смысла брать советника до мобилизации экономики). Честно говоря, не ожидал такой разницы, бонусы не выглядели для меня настолько сильными (+5% к скорости строительства, -2% тнп, 10% к лимиту зданий в провинции, 2 фабрики, 2 завода). Товаров народного потребления дают не так много, +5% это как бонус от свободной торговли, места под строительство у СССР и так хватает. Но в сумме разница выходит почти в 30 заводов.",
            "Ну и самое важное: видим малые производные на достаточно широком участке. В сущности, при строительстве от 28 до 50 фабрик, количество военных заводов на 1 января 1941 года остаётся +/- стабильным.",
            "Как было сказано выше, количество заводов относительно постоянно на участке от 28 фабрик до 50 фабрик, рассмотрим это две точки подробнее:",
            "Если начинать строить заводы раньше, то снаряжения к итоговой дате будет больше (см. площадь под графиками военных заводов. Также стоит учесть что эффективность производства будет нарастать со временем, значит, соотношение снаряжение будет больше соотношения площадей), однако я не уверен, насколько следует переходить к максимизации снаряжения. В игру добавили новую систему снабжения (да, для меня она всё еще новая), теперь она требует строительства пунктов снабжения и железных дорог. Таким образом, нельзя считать экономику эффективной если мы можем произвести снаряжение, но не можем доставить его до фронта. Портальные технологии еще более увеличивают важность логистики, т.к. вся произведенная техника сначала телепортируется в столицу и уже после этого отправляется на фронт вместе с пряниками из карамельной страны.",
            "Помимо оптимизации момента перехода с фабрик на военные заводы, можно также оптимизировать количество инфраструктуры. Можно оценить выгодность инфраструктуры при помощи следующего несложного скрипта (нужен только базовый python, никаких сторонних библиотек):",
            "Отдельно отмечу, что приведенный выше код не учитывает что построенные в процессе фабрики тоже будут строить.",
            "Строительство инфраструктуры, это совсем не то же самое что переход с фабрик на военные заводы. Наиболее выгодное количество инфраструктуры разнится в зависимости от количества слотов под строительство. Да и выгода не всегда велика. Скажем так, оптимальное строительство инфраструктуры потребует от игрока определенных усилий.",
            "Наиболее универсальная оптимизация удалась, но что делать с частными случаями пока не понятно. Баланс между логистикой и количеством снаряжения для разных государств будет разным. Понимание того как этот баланс устроен требует большего понимания самой игры (что требует играть в игру правильно, а это не наш случай).",
            "Оптимизация инфраструктуры выглядит многообещающе, но оценка желания игроков применять это оптимизацию - нет. Также сам процесс нахождения оптимального алгоритма строительства с инфраструктурой выглядит достаточно сложно.",
            "В текущей программе упущен такой момент как возможность аннексии других государств. Этот процесс не обязательно сопровождается войной (а тем более серьезной войной), так что под концепцию довоенной экономики вполне подходит. В программе уже реализован алгоритм реализации контроля провинций, остаётся лишь добавить механизм добавления провинций по тэгу страны владельца или по принципу выделяемых стран (например возвращение польских территорий можно реализовать через добавление к СССР всех провинций Белоруссии и Украины, которые еще не входят в состав Советского Союза). Подобное может потребовать расчета строительства для аннексируемых стран (они же тоже развивают свою экономику параллельно вам), что замедлит расчет, но можно упростить его просто до строительства военных заводов, думаю это не должно добавить много погрешности.",
            "С интерфейсом пока всё совсем не здорово. Идеи приходящие мне в голову или звучат очень сложно или звучат еще менее удобно чем редактирование текстовых файлов. Так что gui пока застопорился.",
            "readme.txt будет обновляться по мере внесения изменений в программу. Если вы считаете что в нём чего-то не хватает, можете мне написать в дискорде или прямо в комментарии под этой статьей. В целом, планирую постепенно работать над читаемостью кода и документацией проекта.",
            "Весь код репозитория распространяется по лицензии MIT, которая гласит следующее: \"делайте с кодом что хотите, но не надо из-за него со мной судиться\". В общем, свободное ПО и всё такое, развлекайтесь, если хотите."
        ]
    }
][
    {
        "Название статьи": "Машинное обучение с Python и TensorFlow на Windows. Быстрый старт",
        "Дата публикации": "2024-06-06, 10:10",
        "Автор статьи": "Sber ",
        "Статья целиком": [
            "Словосочетание «машинное обучение» становится всё более значимым с каждым годом и проникает во все возможные сферы жизни, а с появлением в открытом доступе таких нейронных сетей как Chat GPT [1] интерес к машинному обучению стал высок как никогда. Но при этом многих отпугивает сложность создания своих систем на основе машинного обучения, потому что нужно одновременного использовать и настраивать много разных инструментов разработки.",
            "Поэтому я хочу представить вашему вниманию максимально простую инструкцию для быстрого погружения в мир машинного обучения. Инструкция ориентирована в первую очередь на начинающих программистов, мы будем применять Python 3 [2] с библиотекой TensorFlow [3]. Это лучший выбор для начинающих из-за простоты языка и большого сообщества разработчиков, использующих TensorFlow.",
            "Ваш процессор должен поддерживать AVX-инструкции [4], а видеокарта должна поддерживать архитектуру CUDA начиная с версии 3.5 (список подходящих видеокарт [5]):",
            "AVX (Advanced Vector Extensions) — это расширение системы команд x86-архитектуры для микропроцессоров Intel и AMD, предложенное Intel в марте 2008 года.",
            "CUDA (Compute Unified Device Architecture) — это программно-аппаратная архитектура параллельных вычислений, разработанная компанией NVIDIA. Она позволяет существенно увеличить вычислительную производительность благодаря использованию графических процессоров (GPU).",
            "Далее я буду говорить о TensorFlow 2.10, так как последующие версии требуют установки Windows Subsystem for Linux (использование WSL усложнит установку и ограничит количество подходящих версий Windows). TensorFlow 2.10 будет работать на Windows 7 и более новых версиях операционной системы. Версия Python должна быть от 3.9 до 3.11.",
            "Нужно начать с пакета Microsoft Visual C++ для Visual Studio 2015, 2017 и 2019. Далее установите Python с минимальным набором библиотек и менеджером пакетов — лучше всего для этого подходит Anaconda [6]. Она содержит в себе дистрибутив языка Python, систему управления средой разработки Anaconda Navigator (визуальный менеджер пакетов, управление дополнительным софтом и т. д.) и базовый набор библиотек. Но если вы не фанат визуальных сред разработки или не хотите ставить лишние программы на компьютер, то можно ограничиться Miniconda (это то же самое, что и Anaconda, но без лишних программ, только библиотеки и консольный менеджер пакетов Conda) [7].",
            "После установки Anaconda нужно запустить Anaconda Powerschell Prompt и выполнить в консоли команду:",
            "Она создаст новую среду conda (в рамках одной среды можно использовать определённый набор библиотек, это полезно, если для разных проектов нужны разные версии библиотек) и установит для неё Python 3.9.",
            "Затем с помощью команды activate активируем созданную выше среду (deactivate возвращает базовую среду):",
            "Теперь нужно установить свежие драйверы для вашей видеокарты [8], а если они у вас установлены, то можно переходить к установке пакетов CUDA и cuDNN. Это можно сделать выполнив одну команду:",
            "Указанные выше версии пакетов лучше не менять, так как могут быть проблемы с совместимостью библиотек. Выполнение этой команды выглядит так:",
            "Для корректной работы TensorFlow требует свежая версия pip (это менеджер пакетов). Обновите его:",
            "TensorFlow можно установить и через conda (или через Anaconda Navigator), но разработчики TensorFlow рекомендуют использовать именно pip. Устанавливаем:",
            "Нужно удостовериться, правильно ли всё настроено и установлено. Для этого выполните в консоли или вашей среде разработки код:",
            "Очень удобно для разработки использовать IDE Spyder, если её установить с помощью Anaconda Navigator для нужной среды. Выглядит установка так (выбираем среду и нажимаем установить):",
            "Обратите внимание на текущую среду, ибо именно в неё будет установлено выбранное ПО. После установки вы можете запустить Spyder из Навигатора от имени выбранной среды (это значит, что будут доступны библиотеки, которые загружены именно для среды tf).",
            "Теперь выполняем код в консоли, который я приводил выше:",
            "Если всё правильно сделано, то система должна вывести список доступных для TensorFlow устройств (в моем случае это видеокарта RTX 3060).",
            "Пришло время создать первый проект, использующий библиотеки машинного обучения. Для теста возьмём стандартный проект классификации ирисов [9]. Создайте скрипт с расширением .py и таким кодом:",
            "По ссылке [9] скачайте файл IRIS.csv из раздела «Входные данные» и разместите его в одной папке со скриптом. Этот файл содержит исходные данные для обучения модели. Затем запустите выполнение скрипта и дождитесь его завершения:",
            "Если скрипт отработал корректно, в конце вы увидите график обучения модели в разделе Plots.",
            "Примечание: при первой попытке выполнения этого скрипта интерпретатор выдаст ошибку об отсутствии необходимых библиотек. При реализации своих проектов вам придётся использовать и устанавливать различные библиотеки. Устанавливать их можно разными способами консольными менеджерами пакетов (pip или conda) или визуальным менеджером пакетов Anaconda Navigator. Старайтесь использовать только один способ, иначе может возникнуть путаница и конфликты между разными версиями библиотек. Я обычно использую визуальную установку через Anaconda Navigator, это позволяет более наглядно контролировать набор установленных пакетов и их версии. Пример визуальной установки пакета для программы классификации ирисов:",
            "Для установки пакета активируем вашу среду (tf в моём случае), выбираем раздел «Неустановленные», в поле поиска (справа) вводим название требуемой библиотеки, ставим на ней галочку и нажимаем Apply. После установки всех необходимых библиотек, которые импортируются в начале скрипта, программа классификации ирисов должна работать.",
            "На этом этапе можно поздравить вас с первым запуском проекта на основе TensorFlow. Теперь вы точно готовы к реализации своих проектов с применением технологий машинного обучения.",
            "Желаю вам успехов во всех ваших будущих проектах и спасибо за внимание. До новых встреч.",
            "https://openai.com/",
            "https://www.python.org/",
            "https://www.tensorflow.org/",
            "https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX",
            "https://developer.nvidia.com/cuda-gpus",
            "https://docs.anaconda.com/free/anaconda/install/windows/",
            "https://docs.anaconda.com/free/miniconda/",
            "https://www.nvidia.com/download/index.aspx",
            "https://www.kaggle.com/code/venkatkrishnan/iris-data-tensorflow-neural-network/notebook"
        ]
    },
    {
        "Название статьи": "Введение в gRPC: Основы, применение, плюсы и минусы. Часть I",
        "Дата публикации": "2024-06-05, 23:59",
        "Автор статьи": "0xN1ck ",
        "Статья целиком": [
            "gRPC (gRPC Remote Procedure Call) — это современная высокопроизводительная фреймворк для удаленных вызовов процедур, разработанная Google. gRPC позволяет клиентам и серверам общаться напрямую, используя протокол HTTP/2 и Protocol Buffers (protobuf) в качестве языка описания интерфейсов (IDL). Эта технология предоставляет возможность эффективного взаимодействия между различными компонентами распределенных систем, независимо от языка программирования.",
            "gRPC основывается на архитектуре клиент-сервер и поддерживает множество языков программирования, включая C++, Java, Python, Go, Ruby и многие другие. В основе gRPC лежат следующие ключевые компоненты:",
            "Protocol Buffers (protobuf): Это язык описания данных и инструмент сериализации, который используется для определения сервисов и их методов, а также для обмена данными между клиентом и сервером. Protobuf позволяет описывать структуру данных в специальном .proto файле, который затем компилируется в исходный код для выбранного языка программирования.",
            "HTTP/2: Протокол транспортного уровня, который обеспечивает мультиплексирование запросов, сжатие заголовков, и другие улучшения производительности по сравнению с HTTP/1.x. HTTP/2 позволяет отправлять несколько запросов через одно соединение, что значительно уменьшает задержки и улучшает пропускную способность.",
            "Stub-генерация: gRPC автоматически генерирует клиентские и серверные stub'ы на основе protobuf-файлов, что упрощает процесс интеграции и уменьшает количество шаблонного кода. Клиенты используют сгенерированные stub'ы для вызова методов на сервере так, как если бы они вызывали локальные функции.",
            "gRPC широко используется для построения распределенных систем и микросервисных архитектур. Вот несколько типичных сценариев его применения:",
            "Микросервисы: В больших системах, где микросервисы взаимодействуют друг с другом, gRPC обеспечивает эффективное и надежное общение с низкой задержкой. Это особенно полезно в системах, где важно минимизировать время отклика и обеспечить высокую пропускную способность.",
            "Взаимодействие между разными языками: Благодаря поддержке множества языков, gRPC позволяет разрабатывать системы, где компоненты написаны на разных языках программирования, легко взаимодействуя между собой. Это упрощает интеграцию различных технологий и позволяет выбирать наиболее подходящий язык для каждого компонента системы.",
            "Реализация API: gRPC идеально подходит для создания высокопроизводительных API, где критичны низкие задержки и высокая пропускная способность. API, реализованные с помощью gRPC, могут использоваться как внутри организации, так и предоставляться внешним пользователям.",
            "Мобильные и IoT приложения: gRPC отлично подходит для мобильных и IoT приложений благодаря своей эффективности и низкому потреблению ресурсов. HTTP/2 обеспечивает минимальное использование сетевых ресурсов, что особенно важно для устройств с ограниченными возможностями.",
            "Высокая производительность: Благодаря использованию HTTP/2 и protobuf, gRPC обеспечивает низкие задержки и высокую пропускную способность. Это делает его идеальным выбором для высоконагруженных систем.",
            "Ясно определенные интерфейсы: Использование protobuf для описания сервисов и сообщений обеспечивает четкую контрактность и минимизацию ошибок на этапе компиляции. Это упрощает процесс разработки и интеграции различных компонентов системы.",
            "Поддержка различных языков: gRPC поддерживает множество языков программирования, что позволяет интегрировать компоненты, написанные на разных языках, в единую систему. Это упрощает использование существующего кода и технологий.",
            "Би-ди стриминг: gRPC поддерживает не только однонаправленные и двунаправленные потоки, но и полный дуплекс, что позволяет реализовать сложные сценарии взаимодействия. Это особенно полезно для приложений, требующих постоянного обмена данными в реальном времени, таких как чаты или системы мониторинга.",
            "Автоматическая генерация кода: gRPC генерирует клиентские и серверные stub'ы, что упрощает разработку и снижает количество шаблонного кода. Это сокращает время разработки и уменьшает количество ошибок, связанных с ручным написанием кода.",
            "Крутая кривая обучения: Для новичков gRPC может показаться сложным из-за необходимости освоения protobuf и специфических особенностей HTTP/2. Однако, с практикой и доступными ресурсами, обучение становится легче.",
            "Ограниченная поддержка браузеров: gRPC не поддерживается большинством браузеров напрямую, что требует использования дополнительных прокси-серверов или gRPC-Web. Это добавляет дополнительную сложность при создании веб-приложений, использующих gRPC.",
            "Зависимость от protobuf: Использование Protocol Buffers как основного формата сериализации может быть ограничением для тех, кто предпочитает другие форматы, такие как JSON или XML. Хотя protobuf предлагает высокую производительность и компактность, он требует дополнительных шагов для сериализации и десериализации данных.",
            "Инфраструктурные требования: Для эффективного использования gRPC необходимо обеспечить поддержку HTTP/2 на уровне сетевой инфраструктуры, что может потребовать дополнительных настроек и ресурсов. Это может стать препятствием для некоторых организаций, особенно если их существующая инфраструктура не поддерживает HTTP/2.",
            "gRPC — это мощный инструмент для построения высокопроизводительных распределенных систем и микросервисов. Он обеспечивает эффективное общение между сервисами, поддерживает множество языков программирования и предлагает ясные и контрактно-ориентированные интерфейсы. Однако, как и любая технология, gRPC имеет свои недостатки и требует определенных усилий для освоения и интеграции.",
            "Если вы строите сложную распределенную систему или ищете способ улучшить взаимодействие между микросервисами, gRPC может стать отличным выбором, обеспечивая высокую производительность и надежность. Важно тщательно взвесить плюсы и минусы этой технологии и оценить ее применимость к вашему проекту, чтобы получить максимальную пользу от использования gRPC."
        ]
    },
    {
        "Название статьи": "Майним крипто-пойнты с помощью цветового автокликера на Python",
        "Дата публикации": "2024-06-05, 20:22",
        "Автор статьи": "temabed ",
        "Статья целиком": [
            "Привет, Хабр! Я продолжаю цикл небольших статей для энтузиастов и начинающих программистов о том, как интересно, а иногда и с выгодой, можно применять свои навыки.",
            "В последнее время широкое распространение получили разные крипто-проекты, которые обещают пользователям материальные вознаграждения за определенную активность. Особенно актуальны они стали после успеха Notcoin, который очень неплохо отблагодарил своих пользователей. Не буду углубляться как это всё работает, откуда там деньги, и много ли среди таких проектов скама (да), нас интересует лишь тот момент, что большинство этих проектов можно автоматизировать, а значит не терять драгоценное время.",
            "Да, большинство из таких проектов имеют достаточно примитивную механику, но при этом отнимают слишком много времени, обещая лишь туманную перспективу возможного заработка. А нас, взрослых людей, такой расклад не устраивает, поэтому за нас должен работать хотя бы робот.",
            "Сегодня на операционном столе будет приложение в телеграм, которое в будущем должно стать полноценной децентрализованной биржей, но в настоящее время позволяет новым пользователям лишь зарабатывать внутренние очки. Хочу сразу сказать, что даже руководитель проекта честно заявляет, что обмен этих очков на что-то более материальное в будущем вовсе на обязателен, имейте это ввиду. Я его тоже не рекламирую, и уж тем более не даю никаких финансовых рекомендаций, он нам нужен лишь в качестве примера.",
            "Механика мини игры внутри этого приложения такая: в течение небольшого времени сверху падают цветочки, которые нужно ловить простым нажатием. За них и начисляют очки. Однако, недавно в игре появились еще и бомбочки, при нажатии на которые счет обнуляется.",
            "В связи с нововведением в виде бомбочек бездумный автокликер использовать стало невозможно. Поэтому я делал попытку использования для убийства цветков компьютерного зрения. Но эксперимент вышел так себе, терминатор получился достаточно убогим, и мир ему захватывать еще не скоро.",
            "А сегодня рассмотрим механику всё того же самого простого автокликера, но определяющего цвет пикселя перед нажатием. Который делает клик только при условии, что пиксель в нужном диапазоне. Нам понадобятся всего три библиотеки.",
            "Pyautogui будет управлять мышью и определять цвет пикселя, на который наведен курсор. Keyboard нужен для назначения горячих клавиш, а time для временной задержки.",
            "Назначим три переменные.",
            "Work будет отвечать за работу кода в бесконечном цикле. Если True- выполняется следующая функция, если False - всё останавливается. Points содержит координаты двадцати точек, изначально равные нулю, так как мы их будем назначать после запуска программы. А index нужен, чтобы демонстрировать пользователю, какую по счету точку он назначает. И сразу напишем функцию присваивания координат точкам для кликов.",
            "Эта функция будет срабатывать при нажатии на горячую клавишу и последовательно присваивать координаты каждой из двадцати точек. Координаты будут браться с позиции, на которую в этот момент наведен курсор мыши.",
            "Следующая функция будем изменять значение переменной work и срабатывать так же при нажатии горячей клавиши. Она будет ставить на паузу работу автокликера.",
            "Теперь функция самого кликера.",
            "В цикле for программа пробегает по всем координатам для клика мышью, но на каждой итерации делается снимок экрана, и перед нажатием определяется цвет пикселя, на который указывает курсор. Важно, что метод getpixel возвращает кортеж с тремя числами, которые характеризую цвет по системе RGB. Методом тыка я пришел к выводу, что для определения цветка нам достаточно, чтобы первое число в картеже было в диапазоне от 134 до 218, а вот если второе и третье число равны, это уже свидетельствует о сером оттенке, в которые выкрашены бомбы, и на такое кликать не стоит. Соответствующие условия я и передал в конструкцию if.",
            "Ну и основная функция, которая будет записывать координаты для кликов нажатием на горячую клавишу \"=\" и запускать кликер нажатием на \"~\".",
            "Вот такая простенькая, но многофункциональная программа получилась. Код целиком:",
            "Применить такой код сможет любой, даже не обладая навыками программирования, и не только по назначению, указанному в статье. Вариантов масса, включая самые обычные компьютерные игры.",
            "Поэкспериментировать же с озвученным проектом возможно только лишь в случае, если у вас есть инвайт (пригласительная ссылка). К сожалению, у меня они уже закончились, поэтому поспрашивайте у знакомых либо в моем телеграм-канале под соответствующими постами.",
            "И последнее, эффективность описанного кода сейчас зависит только от стратегии, по которой вы разместили точки для кликов. У меня результат был так себе, но я нашел самый эффективный способ для фарма в таких простых играх - это дети. Хотя это уже читерство, согласен."
        ]
    },
    {
        "Название статьи": "Магия динамического маппинга. Реализация универсальной обработки файлов нефиксированной структуры на Python",
        "Дата публикации": "2024-06-05, 15:58",
        "Автор статьи": "spectr_dev ",
        "Статья целиком": [
            "Привет! На связи Никита Ильин из Spectr, Backend-разработчик с опытом более 5 лет.",
            "Один из проектов, с которым мы работаем, — IBP-платформа для планирования и прогнозирования спроса и продаж в ритейле. В статье поговорим о конкретной реализации для одной из задач в рамках этой платформы на Python и Django. При этом сама концепция может быть реализована абсолютно на любом фреймворке или платформе: Spring, .NET, Laravel.",
            "Мы разрабатываем IBP-платформу для крупной корпорации, где на основе данных, которые поступают из смежных систем, строятся прогнозы и аналитика. И одна из областей — работа с огромным количеством файлов из внешних источников: чтение, обработка, загрузка и запись всех этих данных в БД. При этом существует большое количество различных источников и форматов этих файлов.",
            "Глобально задача состоит в том, чтобы осуществить загрузку из внешних источников в единую внутреннюю систему для последующего анализа и прогноза.",
            "Информация об источниках.",
            "Сейчас в системе около 350 источников. При этом одновременно может поступать до 100 штук новых.",
            "Важное уточнение: один источник — это файл уникальной структуры. Если названия столбцов различаются, то это уже новый файл, новый источник данных.",
            "Информация о файле. Это обычный классический csv-формат. Разделителем может быть либо «;» либо «,». Число колонок — от 5 до 200, но распарсить нужно только количество, которое обозначено в техническом задании. В нашем случае доходило до 40 колонок, все остальное — системные поля. Число строк всегда большое — от 5 млн до 20 млн.",
            "Ниже представлен классический пример файла с условными обозначениями.",
            "Отлично, вся информация о данных у нас на руках! Что же делаем? Тут возможны два пути.",
            "Первая идея, которая у нас возникла, — статическая реализация или, по-другому, «решение в лоб». При такой реализации мы для каждого источника пишем свой парсер и применяем его — эффективно и быстро. Поговорим о преимуществах чуть подробнее.",
            "Быстрая разработка. Раньше уже существовало какое-то количество источников и мы уже знали, как работать по этому пайплайну: без лишних оптимизаций и размышлений. То есть мы приоритезируем источники, отдаем наиболее важные по значимости в первую очередь, а сами занимаемся другими.",
            "Работает надежно, как швейцарские часы. Когда мы пишем парсер на конкретный источник, мы можем написать на него тест. И перед тем как выкатывать, надо посмотреть — а точно ли поведение такое же, как ожидается? Тут мы говорим про стабильность и проверенность того, что наш парсер работает.",
            "Без непредвиденных side-эффектов. Все парсеры строго императивны и изолированы и, благодаря этому, более стабильны. Ведь когда мы говорим про enterprise-разработку, мы точно не хотим, чтобы один наш парсер сломал всю систему.",
            "Сходить на нужный внешний сервер по SFTP.",
            "Именно SFTP, потому что это спецификация нашей задачи, мы так договорились, нам так комфортно общаться между серверами.",
            "Забрать файл.",
            "Применить парсер.",
            "Сохранить в БД.",
            "Работа аналитика. Сверка с локальной структурой БД и проектирование перевода из внешнего имени во внутреннее. Нам нужно узнать, что, где и как хранится, какая между всем этим связь. Данные формируются в виде спецификации, требований и отдаются разработчику.",
            "Работа Backend-разработчика.",
            "Написание кода для валидации файла и его перевода. То есть это сам код, который осуществляет разбор данных в соответствии с ожидаемой структурой.",
            "Добавление конфигурации в общую структуру парсеров: на какой сервер идем, куда и как данные сохраняем. Мы обобщили все на уровне кода, а теперь надо это явно прописать, куда мы идем, чтобы получить данные о конкретном источнике.",
            "Например, когда у нас SFTP-сервер, я не думаю, что вы будете писать одни и те же четыре строчки кода в каждом парсере. Скорее всего, это будет какая-то функция или класс, которому передаем имя файла и доступы для SFTP-сервера, чтобы он пустил в систему.",
            "Работа тестировщика. Отладка, тесты на деве, стейдже на обрезанных данных. На этом этапе мы выявляем и устраняем ошибки в коде парсера. То есть тестировщик должен проверить две вещи:",
            "работает ли код. То есть он берет маленький кусочек файла, например 100–1000 строчек, и смотрит, встали ли данные в БД, в интерфейс, не свалилось ли что-то и работает ли функционал в целом;",
            "какая скорость. Нужно понять, удовлетворяем ли мы скорости загрузки и нет ли каких-то проблем.",
            "Работа тестировщика и DevOps-инженера. Пуско-наладка на реальных данных. Далее мы все выкатываем на прод, делаем первые итерации, проверяем, все ли работает: встали ли данные в интерфейс, ничего ли не потерялось, выдерживаем ли по ресурсам.",
            "Время разработки пропорционально количеству парсеров. Для разработки одного парсера нам нужно 5 дней. Сверка со структурой БД занимает 1 день, написание парсера — 2 дня, на отладку/тесты и пуско-наладку закладываем еще по одномуу дню.",
            "На один парсер — 5 дней, а на 100 — целых 1,5 года. Процесс, конечно, можно оптимизировать и вести параллельно: аналитику не ждать разработчика, а разработчику не ждать тестировщика. Но тем не менее все это в сумме — большой объем очень рутинной работы.",
            "Время доработок пропорционально количеству парсеров. Классический пример: есть 200 источников, при этом появилось требование о том, что столбец — это не целое число, а число с плавающей запятой. И теперь нужно это отвалидировать, чтобы все было в порядке, иначе данные не будут сходиться. А в случае переиспользования кода (DRY) нужно еще приложить усилия к тому, чтобы подумать, как это сделать. К тому же нужно заново пройти по пайплайну. Мы сделали изменения — значит, нам нужно все заново проверить, посмотреть, выкатить и замерить все парсеры.",
            "По итогу формула будет такой: время множественного изменения = время одного изменения * число парсеров. То есть время на расширение числа парсеров или валидируемых ими полей эквивалентно времени разработки. Не получится делать быстрее дублирующуюся работу, придется делать с такой же скоростью.",
            "Расширение числа парсеров или валидируемых ими полей запускает пайплайн заново. Подобная ситуация случилась лично с нами. Изначально было N парсеров, далее убрали 10 и потом добавили еще 15 сверху. А после этого в 20 имеющихся парсерах изменился состав файла и добавился еще один внешний сервер. Приходится начинать все сначала: аналитика –> разработка –> тестирование.",
            "Ключевой вопрос в этой всей ситуации — как преодолеть проблемы «решения в лоб» и сделать результат нашей работы максимально самодостаточным? Мы подумали об этом и пришли к другой идее — динамической реализации.",
            "Давайте вспомним No-code-приложения, например Tilda. Или такой конструктор мобильных приложений, где вы тащите формы, а затем система сама выполняет работу. Код генерируется — вы наслаждаетесь. Примерно то же самое мы сделали в рамках нашего проекта.",
            "В один момент мы подумали: «А что, если разработать пользовательский интерфейс, который позволит самостоятельно решать задачи, минуя аналитика, разработчика и тестировщика?» То есть пользователь сможет сам создать описание для своих действий в админке в виде шаблона. Затем наш магический механизм обработает созданный шаблон. А динамический парсер интерпретирует файлы, соответствующие структуре, описанной в шаблоне, без необходимости дополнительной ручной обработки.",
            "Эту идею мы назвали — шаблон динамического маппинга.",
            "На картинке ниже представлено, как это все можно изобразить с точки зрения пользовательского интерфейса. В этом списке темплейт — наш источник. Мы его назвали шаблоном.",
            "Далее представлено, какие примерно атрибуты могут быть у этого шаблона:",
            "название (name) — на что смотреть в интерфейсе;",
            "внешняя система (external system) — на какой сервер нужно пойти, чтобы достать конкретный файлик (уникальное системное имя шаблона);",
            "имя файлика и директория, в которой он лежит (filepath) — путь к файлу на сервере, с которым связан этот шаблон, и здесь же и имя файла;",
            "системное обозначение для источника (system name) — выбор из внешних систем, куда мы будем подключаться, чтобы туда идти по пути выше.",
            "Дальше мы уже говорим о том, что внутри этого источника. Это находится в отдельной сущности — attributes. Эта сущность включает в себя такие элементы, как:",
            "name — чтобы человеку было на что смотреть в интерфейсе;",
            "system_name — уникальное системное имя поля шаблона, которое мы должны искать в файле;",
            "type — тип данных поля, такие как float, str, str_alpha_numeric, date, int, bool;",
            "field_representations (представления поля) — JSON-структура, представляющая отображение поля на БД;",
            "template (шаблон, Foreign Key) — связь шаблона с общей инфой, к которому относится данное поле.",
            "На каждом этапе присутствует валидация, которая проверяет, например, наличие файлика, полей в файлике, соответствие типу. И, в конечном итоге, это все может записаться в БД.",
            "Сейчас мы построили чисто концептуальное решение. Давайте разберемся, какой результат нам бы принес этот подход — поговорим о его преимуществах.",
            "Элемент продукта закончен",
            "Такой формат реализации сокращает все возможные согласования. Вместо написания отдельного парсера для каждого нового файла и его источника создаем шаблоны, описывающие структуру файла и определяющие соответствие полей самостоятельно. Это существенно сокращает процесс работы, к тому же позволяет сразу же проверить результат. И это будет работать уже завтра. Сегодня написал — завтра это уже готово, сегодня придумал — завтра уже на проде валидируешь новый файл.",
            "Настройка и поддержка, которая оптимизирует время",
            "В случае со статической реализацией каждый день на протяжении полутора лет придется заниматься переводом спецификаций в код. Естественно, это не творческая и скучная задача. Мы все-таки хотим закрыть эту задачу и, конечно, сделать это наиболее интересным для нас способом.",
            "При динамической реализации настройка и поддержка будут намного интереснее, чем просто сконструировать «решение в лоб» и сидеть полтора года переводить спецификации.",
            "Возникает ряд вопросов. А что если где-то что-то отвалится? А почему данные не загружаются? А как вообще это все сохранить в табличку и как это все будет выглядеть? А как эти 20 млн строчек обработать? Явно придется над всем этим поразмыслить. 5 часов подумать — 1 час написать код.",
            "Масштабируемость",
            "Появляются новые требования:",
            "добавить в N-количестве источников проверку на дубликаты и действие, которое надо совершать, если они есть;",
            "а еще в M-источниках добавить проверку на отрицательность.",
            "При этом непонятно, будут ли все эти поля в файле.",
            "Так у нас появились дополнительные атрибуты у поля шаблона:",
            "required (обязательное) — флаг, указывающий, является ли поле обязательным для заполнения;",
            "can_be_negative (может быть отрицательным) — флаг, указывающий, может ли поле содержать отрицательные значения;",
            "contained_in_md_table (содержится в таблице md) — имя таблицы md, в которой содержится это поле (если применимо);",
            "contained_in_md_attribute (содержится в атрибуте md) — имя атрибута md, в котором содержится это поле (если применимо);",
            "duplicates_in_table (дубликаты в таблице) — имя таблицы, в которой разрешены дубликаты этого поля (если применимо);",
            "duplicates_attribute (атрибут дубликатов) — атрибут, определяющий дубликаты этого поля (если применимо);",
            "duplicates_action (действие с дубликатами) — выбор из действий по обработке дубликатов: обновление или пропуск.",
            "И на все это есть 5 дней — вспоминаем сроки разработки одного парсера. В такой системе мы на этапе валидации прописываем новые условия один раз, а дальше клиент уже сам работает с шаблонами и сам отвечает за выбранные им параметры.",
            "admin API (CRUD — в админке) — могут вносить изменения и создавать новые записи;",
            "user API (Read — для всех пользователей) — есть возможность только читать.",
            "Python и Django — это решение удобно для нас, к тому же мы используем его с начала работы над проектом.",
            "Blazingly-Fast Polars — о том, почему мы выбрали именно этот инструмент, рассказывал наш тимлид в статье Битва медведей: Pandas против Polars. Если кратко: этот вариант для наших вариантов использования работает быстрее, чем Pandas.",
            "Paramiko — библиотека для подключения по SFTP. Очень красиво и надежно.",
            "SQLAlchemy — в качестве дополнительной ОRМ. Удобный интерфейс, быстро и красиво.",
            "Здесь мы перевели сущность модели в сущность шаблона, которая была до этого. Из интересного — здесь есть функция temporary_table_name, в ней мы получаем temporary-имя. Это название временной таблицы. О том, для чего нам нужна временная таблица, поговорим чуть позже.",
            "Далее то же самое делаем для TemplateField. Можно просто взять, перевести на другой язык — и все готово.",
            "Как я и говорил, у админов будет полный набор CRUD-операций: чтение, создание, обновление. Но следующий момент более интересный.",
            "Вот эти два поинта ниже нам нужны для того, чтобы у клиента в интерфейсе была возможность посмотреть, какие у нас есть таблички и атрибуты и поля этой таблички.",
            "Классические представления — обычные классы доступа. Здесь идет пагинация по страницам. Далее, когда мы предоставляем список шаблонов, мы передаем только основную информацию о них — название. А когда отдаем только один шаблон, мы его отдаем вместе с атрибутивным составом, чтобы пользователь мог убедиться, что у него все правильно загружается.",
            "На этом этапе мы получаем какой-то путь файла у этого темплейта, то есть не пишем явно — идти на такой-то сервер. А просто говорим — взять у темплейта название файла. И на выходе получается путь файла.",
            "Далее через библиотеку мы подключаемся по SFTP, забираем файл.",
            "У темплейта есть филды — template.fields. Мы их забираем — это и будут наши правила валидации.",
            "django_file = InMemoryUploadedFile",
            "validate_file(django_file, list(template.fields.all()))",
            "BasicFileHeadersValidator. В процессе валидации проверяем наличие данных, дублирование колонок, наличие требуемых колонок. Если какой-то из этих пунктов не проходит, мы отправляем пользователю ошибку, так как нам незачем загружать файл, у которого нет требуемых нам колонок. Мы знаем, что он заведомо сохранит его туда, куда нам не надо.",
            "BasicFileReaderValidator / GeneralValuesFieldsValidator. Базовое чтение (проверка на кодировку, соответствие строк размеру), перевод названий колонок согласно шаблону, нормализация данных и проверка их на соответствие типам, генерация дата-фрейма.",
            "Даже если взять 10 млн строк, с учетом того, что параллельно работает 100 источников, cкорее всего, они упадут по памяти.",
            "Что с этим делать? Классический вариант — разбиение на чанки. В результате этого получается кусок файла, c которым можно продолжать работать. Далее с ним проводим соответствующие этапы валидации и формируем соответствующий дата-фрейм. Но в такой структуре важно, чтобы мы не захотели сделать дополнительную логику — например, агрегацию, дезагрегацию, суммирование, фильтрацию, категоризацию. Для всех этих работ мы используем сохранение во временную таблицу, а потом при необходимости все это сохраняем в основную БД.",
            "Если требование состоит в том, что решение нужно кастомное, то это делает уже другой разработчик в рамках другой задачи. Он идет во временную таблицу, забирает все нужные данные и, соответственно, с ними делает то, что нужно ему. При этом временную таблицу нужно каждый раз чистить перед загрузкой, иначе вы упадете по памяти в БД или будете работать со старыми данными.",
            "Что будем использовать дальше? Во-первых, SQLAlchemy. Собираем из дата-фрейма название колонок, делаем сущности колонок, берем название таблицы и с помощью контекстного менеджера вставляем какое-то количество записей. В нашем случае — 1 млн.",
            "Что такое контекстный менеджер",
            "В Python можно использовать удобный декоратор для этого паттерна. Перед началом мы напишем такой connection_url, где мы вставляем доступ к БД. Здесь конструкция try-finally говорит о том, что мы пытаемся отдать наш движок подключения к БД. Но в любом случае, какие бы ошибки ни были, этот движок будет в конечном итоге закрываться. Это нужно для того, чтобы в БД не висело открытое подключение.",
            "Берем нужную нам модель представления данных, написанную на Django.",
            "Здесь есть специфичный для Django код. Но я почти уверен, что таким образом можно получить табличку из основной БД на любом другом языке.",
            "Дальше мы смотрим поля у этой таблицы и пытаемся найти ее первичный ключ. Для нашего проекта специфично то, что может быть три разных первичных ключа, в зависимости от таблички: обычный ID, External ID либо Name ID.",
            "Итак, мы получили название первичного ключа. Далее мы идем в БД и проверяем, какие записи с этими ID уже есть.",
            "Мы берем их, кладем в оперативную память.",
            "После этого мы идем по дата-фрейму и проверяем, есть ли у нас такая запись.",
            "Если не смогли найти с этим ID объект среди тех, что мы вытащили из БД, то сохраняем ее как новый объект, кладем в какую-то структуру (в нашем случае список) и затем позже создадим их в БД.",
            "Но если мы все-таки смогли найти этот ID, то берем и меняем у найденного объекта все атрибуты.",
            "Условно, в файле одно значение, в БД другое значение для этой строчки — поменяли, сохранили в структуру. И здесь происходит множественное сохранение/обновление. Важно использовать batch_size. Потому что создавать запрос на миллион строчек — невыгодно. Делаем batch_size, разбиваем структуру на множество запросов и делаем с ним.",
            "batch_size=1000 — число объектов в каждом запросе.",
            "Заклинание освоено! А теперь поговорим о недостатках такого решения.",
            "Трудности в отладке и тестировании. Это интересно, но приходится думать, тратить время, пытаться понять, где ошибка, почему не можем распарсить — затратно дебажить.",
            "Строгость в соблюдении принципов. Важно соблюдать всю последовательность действий, про которую мы говорили ранее (этапы реализации динамического решения). Мы не должны запихнуть весь файл в систему, а на выходе говорить, что сделаем полностью кастомную работу.",
            "Ограничение в функциональности. Здесь как раз идет речь об отсутствии некоторых кастомных полей, все динамическое. И если вы тронете хотя бы одну строчку, то для всех остальных это изменение автоматически применится. Один файл, но его тяжело поддерживать.",
            "Дополнительное время на валидацию. Многие знают, что функции в Python вызывать довольно дорого, поэтому вы можете столкнуться с тем, что валидация может занимать много времени. Ни в коем случае не пытайтесь запустить код, который кажется примерно рабочим. Посмотрите, какие есть практики использования функции, почему нельзя использовать лямбду-функцию в цикле и т. д.",
            "Дополнительное время на сохранение во временную БД и/или основную БД. Мы говорим, что у нас есть временная таблица, и после нее вы можете делать некоторые кастомные операции, которые вы хотите. Но в то же время мы тратим минуту-две на то, чтобы сохранить эти данные, и, плюс ко всему, они занимают какую-то дополнительную память на диске.",
            "Как мы можем обойти эти минусы:",
            "Расширение поддерживаемых форматов файлов. Здесь имеются в виду Excel и все подобные структуры. Нам не потребуется писать отдельный парсер для каждого такого формата. Можно просто добавить новый вариант ридера, добавить его в общую структуру и смотреть на расширение файла перед загрузкой.",
            "Оптимизация производительности. Пытаемся ускорить валидацию, оптимизировав все возможные куски, меняя пайплайн вызова валидаторов.",
            "Развитие интерфейсов и возможных конфигураций. Админу нужно дать возможность делать batch_size для источника. То есть он знает, что, например, у него будет 100 валидированных колонок, и здесь batch_size в 100 тыс. — это слишком много, а 1,5 тыс. — уже нормально. Пусть у него будет возможность заменить на любое значение, которое он сам посчитает нужным. Дальше, если говорить о других возможных интерфейсах, у нас есть проверка на отрицательность на случай дублирования (мы меняем эту строчку или пропускаем ее и говорим, что нам все равно, есть ли она).",
            "И на этом все! В статье я поделился своим опытом, основанным на решении конкретной задачи. Надеюсь, что материал поможет вам выбрать правильное решение в аналогичной ситуации и покажет, как можно творчески подходить к задачам. А если у вас появятся вопросы — буду рад на них ответить в комментариях к этой статье!",
            "Статья подготовлена по мотивам доклада Никиты Ильина, Backend-разработчика в Spectr, на митапе #DevTalks. Ссылка на запись доклада:"
        ]
    },
    {
        "Название статьи": "Получение списка людей, посещающих определенные места",
        "Дата публикации": "2024-06-05, 15:12",
        "Автор статьи": "fire64 ",
        "Статья целиком": [
            "Представьте: вы ведете Telegram-канал о животных и хотите пригласить в него посетителей зоопарка. Или вам нужно собрать контакты потенциальных клиентов, посещающих определенный торговый центр. Как это сделать?",
            "Полиция может легко получить такую информацию от мобильных операторов, но что делать обычному человеку?",
            "Ответ – использовать Telegram и его функцию \"Люди рядом\" в сочетании с Python-скриптом.",
            "\"Люди рядом\": эта функция Telegram показывает контакты пользователей, находящихся поблизости, с примерным расстоянием до них (500 м, 1 км, 2 км и 3 км). Отображаются первые 100 ближайших контактов.",
            "Python-скрипт: с помощью библиотеки telethon можно получить доступ к этой информации и автоматизировать процесс сбора контактов.",
            "Установка:",
            "Скачайте и установите Python с официального сайта: https://www.python.org/downloads/",
            "Установите необходимые модули:",
            "Регистрация приложения Telegram:",
            "Зарегистрируйте свое приложение на сайте Telegram: https://core.telegram.org/api/obtaining_api_id",
            "Важно: используйте свой реальный номер телефона, привязанный к Telegram-аккаунту, а не бота.",
            "Создание скрипта:",
            "Создайте файл с расширением .py и вставьте код скрипта (https://pastebin.com/pYPA8PF0).",
            "Замените следующие значения:",
            "api_id = (ваш API ID)",
            "api_hash = (ваш API Hash)",
            "phone_number = '' (ваш номер телефона)",
            "Запустите скрипт.",
            "Выберите на карте нужное местоположение.",
            "Укажите радиус поиска (500, 1000, 2000 или 3000 метров).",
            "Нажмите кнопку \"Начать поиск\".",
            "Скрипт автоматически получит список пользователей Telegram, находящихся в заданном радиусе, и добавит их в ваши контакты.",
            "Данные обновляются Telegram каждые 30 минут.",
            "Отображаются только первые 100 пользователей.",
            "Поиск работает только в регионе, к которому привязан ваш номер телефона.",
            "Отображаются только пользователи со включенной видимостью. В среднем это 10-15%",
            "Важно использовать этот метод этично и уважительно по отношению к другим пользователям. Не рассылайте спам и не используйте полученную информацию в незаконных целях.",
            "Помните: эта информация предназначена только для ознакомления. Перед использованием подобных скриптов убедитесь, что вы не нарушаете правила Telegram и законодательство вашей страны."
        ]
    },
    {
        "Название статьи": "Как в Tele2 автоматизировали тестирование SAP ERP с помощью Python",
        "Дата публикации": "2024-06-05, 13:34",
        "Автор статьи": "a_valeeva ",
        "Статья целиком": [
            "Привет, Хабр! Меня зовут Анастасия Валеева, я – руководитель группы обеспечения качества в Tele2. Наша команда работает в большинстве своём с SAP ERP, и мы не понаслышке знаем, что автоматизация данной платформы — дело далеко не тривиальное. В этой статье я хочу поделиться с вами, как и зачем мы автоматизировали тестирование с помощью Python.",
            "SAP ERP – гибкий инструмент в руках нашей команды. Мы дорабатываем функциональность системы под потребности конкретного бизнес сегмента. Эти изменения производятся по запросу бизнес-пользователей. Объём и влияние доработок могут быть различными, но одно остаётся неизменным – каждая доработка является уникальной. Таким образом, это не простое устранение багов и улучшения текущего функционала, не изменение версионности продукта после оптимизации, а, как правило, абсолютно новый «продукт» в системе. В случае автоматизации функционального тестирования нам потребуется писать автотест на каждую доработку/разработку, что занимает больше времени, чем ручное тестирование (написание автотеста, отладка, оптимизация) + данный автотест с каждой новой разработкой будет уже неактуален, и нужно будет создавать новые и новые из раза в раз. Делаем выводы, что автоматизировать функциональные тексты для нас нерелевантно. А вот регрессионные тесты, которые мы проводим после каждого изменения системы, представляют собой более шаблонные варианты, шаги повторяются, и от их автоматизации есть профит.",
            "Сейчас мы работаем с SAP ERP и интегрированными продуктами (FileNet, BW, Fiori), однако, импортозамещение идёт полным ходом, и мы проводим пилотный проект по миграции на новую платформу. Так или иначе, созданный нами инструмент для автотестов универсален и может быть применён в работе с новой системой.",
            "Из множества инструментов автоматизации мы выбрали для ознакомления четыре наиболее совместимых с SAP ERP:",
            "SAP Scripting;",
            "Tricentis Tosca;",
            "eCatt;",
            "CBTA.",
            "Анализируя, мы исходили из трёх основных для нас факторов: скорость освоения, простота и гибкость, а также бюджет. По каждому из инструментов мы отметили свои плюсы и минусы, собрали информацию в единую таблицу. И вот что у нас получилось.",
            "По количеству зелёных блоков мы увидели, что нашим критериям в большей степени соответствует SAP Scripting.",
            "Принцип работы данного инструмента состоит в том, что он записывает все действия пользователя в системе, на выходе формирует файл в формате .vbs, который в последующем можно запускать в SAP. Соответственно, при запуске этого файла система будет повторять ваши предварительно записанные шаги. Кроме того, данный файл можно корректировать: удалять лишнее, дописывать недостающее или даже полностью переписать. Для этого необходимо открыть файл либо в блокноте, либо в любом другом редакторе, работающем с кодом.",
            "В процессе пилотирования SAP Scripting помимо технических вопросов мы решали несколько административных задач: удобство использования, гибкость, кастомизация, универсальность, прозрачность.",
            "Мы хотели внедрить такой инструмент, который будет полезен не только группе тестирования, но и другим смежным группам нашего подразделения. И поскольку мы говорим об автоматизации, одним из основополагающих факторов для нас было минимальное участие человека в этом процессе. Согласитесь, часто хочется просто нажать на волшебную кнопку \"РАБОТАТЬ\", чтобы оно всё само заработало :)",
            "Добиться данного магического эффекта «работает само» нам помог Python. За это отвечала коллега из моей команды — она написала скрипт для робота, который сейчас работает буквально по одному клику.",
            "Что касается прозрачности, то мы пошли по пути, доступному для любого пользователя. Для этого «прикрутили» Python к файлу Excel. Это означает, что сейчас провести регресс может любой сотрудник — достаточно зайти в файл автотеста и нажать кнопку «СТАРТ».",
            "Бизнес-процесс состоит из набора бизнес-операций. Например, создание логистического заказа состоит из заведения заказа, смены статуса подписания договора, деблокирования заказа и создания счёта-фактуры. Для обеспечения полного регрессионного тестирования мы автоматизируем всю цепочку шагов. На выходе получаем Excel-документ со скриншотами и подробной информацией по каждому шагу тестирования. Причём регресс может запустить любой пользователь, не только тестировщик, это доступно в том числе для менеджеров со стороны бизнеса. А полученные данные (скрипты) можно использовать также для генерации тестовых данных.",
            "Существует несколько способов выполнения автотестов.",
            "1. Отдельно по каждому бизнес-процессу. По каждому модулю финансовой системы SAP ERP создан файл Excel, в котором есть кнопка вызова макроса. По вызову этой кнопки запускается Visual Basic for Applications. VBA обращается к системе SAP и вызывает на выполнение ранее записанный скрипт vbs.Таким образом, мы можем выполнять тестирование по отдельному модулю или бизнес-операции.",
            "2. По всему модулю или нескольким модулям.Для этих нужд как раз используется Python. Наш робот обращается к SAP, открывая рабочее окно. Далее вызывает необходимые файлы Excel, которые работают по описанному принципу макросов на VBA. Таким образом, мы получаем следующую цепочку:",
            "При этом пользователю необходимо только единожды нажать кнопку ВЫПОЛНИТЬ.",
            "Запуск SAP GUI",
            "Заведение функции для чтения файла Excel",
            "Подключение к Excel",
            "На каждом листе в Excel есть подробная входная и выходная информация, при этом входную информацию можно корректировать. Большая часть листов связана между собой, чтобы можно было провести всю цепочку на одних данных, а последующие шаги не зависели от дополнительных действий пользователя.",
            "Все скриншоты, которые создаются в процессе регресса, генерируются вместе с документами и проводками. Лишние скриншоты можно удалить прямо на странице в Excel. При необходимости сотрудник может по номеру документа найти нужную проводку или операцию в SAP. Это является прозрачным и удобным способом анализа логов тестирования.",
            "Рядом с каждым шагом в файле появляется текстовое описание, статус «успешно» или «не успешно» пройден шаг и цветовой индикатор — зеленый означает успешно пройденный этап, красный сигнализирует об ошибках.",
            "Если ошибка является блокирующей для системы и дальнейшее прохождение шагов невозможно, то скрипт остановится, выдаст информационное сообщение и сохранит изменения в файл. Если ошибка не влияет на последующие шаги, то скрипт продолжит работу, а в конце выдаст лог в Excel с отображением корректных и некорректных шагов. При таком раскладе у нас появляется возможность увидеть проблему в моменте и исправить её.",
            "Также, завершение работы скрипта сопровождается звуковым оповещением.",
            "Дополнительно мы настроили автоматическое удаление листов из общей папки через три дня после их создания.",
            "Что в итоге",
            "Мы посчитали, сколько рабочего времени ручных тестировщиков мы экономим при использовании инструмента автоматизации. Получилось, что на один кейс при использовании SAP Scripting мы тратим 31 секунду против 148 секунд при ручном тестировании. Таким образом, 80% времени инженеров высвободилось на другие задачи, и мы смогли повысить эффективность тестирования.",
            "Данный вариант автоматизации является гибким к изменениям. В случае переезда на другую финансовую систему мы перенаправим нашего робота на Python на вызов нужной нам программы. Сейчас одна из наших основных задач – обеспечить качество работы текущего функционала и уже на этой надёжной основе реализовывать улучшения и внедрять новые фичи. Для нашей команды автоматизация тестирования SAP ERP стала интересным и полезным опытом, а бизнесу предоставила доступную, понятную и безотказную систему проверки рабочих процессов."
        ]
    },
    {
        "Название статьи": "Быстрый интерфейс, быстрый деплой",
        "Дата публикации": "2024-06-05, 11:01",
        "Автор статьи": "funtastick ",
        "Статья целиком": [
            "Салют! Не так давно создатели знаменитого pydantic выпустили новый фреймворк — FastUI, который позволяет создавать пользовательские интерфейсы с помощью декларативного кода на Python. В этой статье рассмотрим создание простого приложения и деплой его в Cloud Apps. ❯ Обзор По заявлению авторов фреймворка, фронтенду не нужно (и не следует) знать ничего о приложении, которое вы создаете, вместо этого он должен просто предоставить все компоненты, необходимые для создания интерфейса, а затем бэкенд может предоставить необходимые данные и параметры компонентов. Реализовано это таким образом, что FastUI инкапсулирует описание компонентов интерфейса и в виде классов, затем запускается простое React приложение, которое обращается к эндпоинтам за данными и компонентами. ❯ Пример Для примера давайте напишем простое приложение, предоставляющее информацию о городах из списка с возможностью пагинации. Данные для экспериментов любезно предоставили создатели фреймворка. Для начала опишем pydantic модель и функцию для чтения данных. from pydantic import BaseModel, Field, TypeAdapter import json from pathlib import Path class City(BaseModel): id: int city: str = Field(title=\"Name\") city_ascii: str = Field(title=\"City Ascii\") lat: float = Field(title=\"Latitude\") lng: float = Field(title=\"Longitude\") country: str = Field(title=\"Country\") iso2: str = Field(title=\"ISO2\") iso3: str = Field(title=\"ISO3\") admin_name: str = Field(title=\"Admin Name\") capital: str = Field(title=\"Capital\") population: float = Field(title=\"Population\") def cities_list() -> list[City]: cities_file = Path(__file__).parent / \"cities.json\" with open(cities_file, \"r\", encoding=\"utf-8\") as f: data = json.load(f) cities = [City(**city) for city in data] return cities Далее напишем каркас для нашего примера, с помощью FastAPI. Опишем два роута, первый возвращает необходимые компоненты и данные, а второй — простое React приложение, которое отвечает за запрос и отображение компонентов, полученных из предыдущего. from fastapi import FastAPI from fastapi.responses import HTMLResponse from fastui import AnyComponent, FastUI from fastui import components as c, prebuilt_html from fastui.components.display import DisplayLookup, DisplayMode from fastui.events import BackEvent, GoToEvent app = FastAPI() @app.get(\"/api/cities\", response_model=FastUI, response_model_exclude_none=True) def cities_view(page: int = 1, country=None): cities = cities_list() page_size = 10 # Количество записей в таблице, отображаемых на странице filter_form_initial = {} return c.Page( # Page - базовый контейнер для остальных компонентов components=[ c.Table( # Table - базовая разметка таблицы data=cities[(page - 1) * page_size : page * page_size], #Создаём срез данных для заполнения таблицы data_model=City, #Передаём модель данных columns=[ # Описываем столбцы таблицы DisplayLookup( #Указываем содержимое и размер столбца в процентах field=\"city\", table_width_percent=33 ), DisplayLookup(field=\"country\", table_width_percent=33), DisplayLookup(field=\"population\", table_width_percent=33), ], ), c.Pagination(page=page, page_size=page_size, total=len(cities)), #Кнопки для пагинации ] ) @app.get(\"/{path:path}\") async def html_landing() -> HTMLResponse: \"\"\"Простое React приложение, идёт последним, т.к. соответствует всем маршрутам\"\"\" return HTMLResponse(prebuilt_html(title=\"Большие города\")) Результат работы представлен на рисунке ниже: ❯ Деплой Для деплоя приложений на FastUI можно воспользоваться сервисом Apps, к сожалению рассмотренный фреймворк только набирает популярность, поэтому мы воспользуемся опцией: «деплой из Dockerfile». Для этого достаточно создать Dockerfile и разместить его в корне репозитория. FROM python:3.11 COPY . /app WORKDIR /app RUN pip install -r requirements.txt CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] EXPOSE 8000 Обратите внимание, что при отсутствии в Dockerfile параметра EXPOSE, APPS будет слушать порт 8080 контейнера. Далее достаточно предоставить сервису доступ к аккаунту на github. Затем остаётся следить за логами деплоя: В случае успешного развёртывания появиться удобный дашборд с графиками нагрузки на виртуальную машину: ❯ Заключение В данной статье мы рассмотрели лишь малую часть возможностей фреймворка, однако можно отметить, что FastUI предоставляет новый подход к созданию веб-приложений и позволяет существенно ускорить разработку. Возможно, захочется почитать и это: ➤ Timeweb Cloud CLI ➤ Бесплатный прокси к Docker Hub ➤ Фантастически быстрый деплой веб-приложения ➤ Учимся летать: симуляция эволюции на Rust ➤ Age of Empires – культовая попытка сделать Цивилизацию в реал-тайме Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩"
        ]
    },
    {
        "Название статьи": "Как я создавал аудиоплеер на python с FFmpeg",
        "Дата публикации": "2024-06-04, 18:10",
        "Автор статьи": "Niamorro ",
        "Статья целиком": [
            "Всех приветствую. Сегодня хочу поделиться опытом создания своего первого проекта на Python. Мой проект — это простой аудиоплеер, и я хочу рассказать, как я его создавал, с какими сложностями столкнулся и что из этого вышло.",
            "Выбор языка для первого проекта — это всегда непросто. Я выбрал Python по нескольким причинам:",
            "Простота синтаксиса. Python очень читабельный и понятный, что идеально подходит для новичков.",
            "Богатая стандартная библиотека и сообщество. Множество готовых решений и библиотек, которые можно использовать в своих проектах.",
            "Популярность в разработке. Python — один из самых популярных языков программирования, и навыки работы с ним будут полезны в будущем.",
            "Моя цель была написать простой аудиоплеер, который мог бы играть основные аудиоформаты. Я хотел, чтобы пользователь мог выбирать треки, ставить их на паузу и останавливать, так же изменять скорость проигрывания.",
            "Выбор библиотек занял действительно много времени, так как нужно было выбрать библиотеки которые обновляются, и имеют необходимый мне функционал. Я использовал несколько библиотек:",
            "PySide6: библиотека для создания интерфейсов созданная разработчиками Qt, имеет хорошую поддержку сообщества и регулярные обновления, в дополнение к ней использовал qdarktheme для стилизации интерфейса.",
            "FFmpeg: Универсальный инструмент для обработки видео и аудио.",
            "Sounddevice: Библиотека для воспроизведения и записи звука в Python.",
            "Mutagen: Библиотека для извлечения данных из аудиофайлов.",
            "Выбор файла:",
            "Пользователь выбирает аудиофайл из меню \"Файл\". Поддерживаемые форматы включают MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Выбранный файл передаётся в FFmpeg через подпроцесс для извлечения необработанных аудиоданных. Используемая команда:",
            "Чтение аудиоданных:Аудиоданные считываются блоками и сохраняются в массив NumPy для эффективной обработки.",
            "Регулировка громкости:Регулировка громкости осуществляется путём умножения аудиомассива на коэффициент громкости.",
            "Регулировка скорости воспроизведения:Скорость воспроизведения (например, 2x) управляется через библиотеку sounddevice путём изменения частоты дискретизации.",
            "Поток вывода:Обработанные аудиоданные передаются на аудиовыход через библиотеку sounddevice.",
            "Управление воспроизведением:Элементы управления, такие как воспроизведение/пауза, следующий/предыдущий трек и перемотка, обрабатываются через класс AudioTrigger.",
            "Воспроизведение/Пауза:Использует класс AudioTrigger для начала/остановки аудиопотока.",
            "Следующий/Предыдущий трек:Обновляет текущий индекс трека и загружает следующий/предыдущий трек в плейлисте.",
            "Перемотка:Регулирует позицию воспроизведения, пересчитывая индекс позиции на основе значения ползунка.",
            "Виджет очереди треков:Отображает добавленные ранее папки.",
            "Виджет плейлиста:Отображает содержимое папки.",
            "Виджет информации о треке:Показывает метаданные и обложку для воспроизводимого трека.",
            "Если хотите ознакомиться с исходным кодом или внести свой вклад в проект,",
            "приглашаю вас посетить страницу GitHub проекта. Там вы найдёте весь исходный код аудиоплеера.",
            "Также у проекта есть веб-сайт, где вы можете скачать готовые .exe и .deb пакеты для Windows и Linux. Здесь же доступна подробная документация по установке и использованию программы.",
            "Работа с FFmpeg требовала правильной организации буферизации аудиоданных, чтобы избежать прерываний и задержек при воспроизведении.",
            "Решение: Буферизация данных в массив NumPy.",
            "Треки воспроизводились с неправильной скоростью из-за некорректной частоты дискретизации.",
            "Решение: Я считываю частоту дискретизации трека и открываю аудиопоток с настройками именно для того трека, который в данный момент должен воспроизводиться.",
            "В результате я создал аудиоплеер с основными функциональными возможностями:",
            "Проигрывание аудиофайлов: Поддерживаются популярные форматы MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Управление воспроизведением: Воспроизведение, пауза, остановка, перемотка, следующий/предыдущий трек.",
            "Регулировка скорости воспроизведения: Возможность воспроизводить треки быстрее или медленнее.",
            "Плейлист: Добавление папок с треками.",
            "Информация о треках: Отображение метаданных и обложек альбомов.",
            "Тёмная тема: Благодаря qdarktheme, аудиоплеер имеет современный и стильный интерфейс.",
            "Добавить поддержку потокового аудио: Возможность воспроизводить музыку из интернет-радиостанций и стриминговых сервисов.",
            "Расширить функциональность плейлиста: Добавить возможность создания и сохранения пользовательских плейлистов.",
            "Поддержка эквалайзера: Добавить эквалайзер для настройки звука.",
            "Создание аудиоплеера на Python оказалось полезным опытом. Я научился работать с аудио на низком уровне, обрабатывать потоки и создавать пользовательский интерфейс.",
            "Буду рад любым отзывам и предложениям по улучшению плеера. Спасибо за внимание!",
            "Ссылки:",
            "Исходный код на GitHub",
            "Сайт проекта с загрузкой пакетов"
        ]
    },
    {
        "Название статьи": "Как мониторинг связан с тестированием. Преимущества мониторинга для бизнеса: как экономить время и деньги",
        "Дата публикации": "2024-06-04, 15:59",
        "Автор статьи": "luffity ",
        "Статья целиком": [
            "Привет! Проходя множество собеседований, я не раз слышал вопросы по типу: «Что такое мониторинг?», «Как это связано с тестированием?», «Зачем это нужно?». Для меня, волей случая ставшего специалистом по мониторингу чуть больше года назад, это тривиальные вопросы, однако многие компании либо не знают, что это такое, либо не видят в этом пользы. На одном из последних интервью я услышал интересное мнение от QA Lead о том, что assert должен быть в каждом тесте. Смелое заявление, подумал я. Поэтому, собственно, вы и читаете эту статью.Разберёмся, что такое мониторинг и с чем его едят. А главное, зачем он нужен вообще.",
            "Думаю, начать с небольшого введения обо мне будет наиболее верным для погружения в тему. Сейчас я занимаю должность middle SDET в ООО «МИТ» (если проще, то DIXY Group). Попал я туда как AQA, причём единственным. Я находился в группе по мониторингу корпоративных сервисов, соответственно, кроме меня там были только специалист по мониторингу (мой Lead), DevOps и системный админ. Похоже на стандартное начало анекдота…",
            "Моим заданием на испытательный срок стало написание сервиса по мониторингу интернет-соединения на торговых точках компании. И тут я подумал: какой, блин, мониторинг? С другой стороны, работа на дороге не валяется, тем более настолько приятная. Дело было вечером, делать было нечего…",
            "По итогу этого задания ко мне пришло осознание того, как тесты могут трансформироваться в нечто большее, чем проверки, прогоняемые раз в релиз. Они могут быть целой экосистемой, даже можно сказать: «Глазом Бога» (кто понял отсылку к Форсажу, спасибо).",
            "Начнём мы, конечно же, с поверхности и определим для себя, что такое мониторинг и как он связан с тестированием. В самом распространённом смысле под тестированием понимают сверку ожидаемого и фактического результатов на конечном наборе тестов. Если мы берём стандартные парадигмы тестирования, то такие проверки могут выполняться при добавлении новой фичи, раз в спринт, при релизе и много когда ещё. Однако все эти тестовые прогоны не преследуют цель отслеживать состояние продукта (программного или бизнесового) постоянно.",
            "Как раз здесь и вступает в игру мониторинг. Если о-о-очень грубо сказать, то это, по сути, те же наборы тестов, только несущие цель постоянного наблюдения за состоянием продукта. Но тут стоит уточнить, что это не тестовые кейсы в их привычном понимании.",
            "Думаю, будет проще понять наглядно.",
            "Это всем понятный простой тест. Мы переходим на страницу и ожидаем заголовок. Как можно заметить, для того чтобы исполнить свою главную функцию – сопоставить ожидаемый и фактический результаты, тест содержит в себе assert. Это несомненно верный подход к написанию тестов, так как это позволит более точно валидировать ошибку, а также правильно отобразить её в отчётах, например Allure.",
            "А теперь взглянем на код скрипта мониторинга, который проверяет доступность ресурса.",
            "Сразу бросается в глаза отсутствие assert. Но в таком случае, как такой скрипт вообще можно считать информативным, если он не выводит ошибки? Именно поэтому мы добавим дополнительное действие. Например, найдём какую-то кнопку и нажмём на неё. Теперь, если ресурс не прогрузился или сломался, мы получим TimeoutException и сообщение о том, что именно скрипт не смог сделать.",
            "Возникает вопрос: почему бы тогда точно так же не поставить assert и не ждать лишнее время для выпадения TimeoutException ? Справедливо! Однако возьмём во внимание, что данный скрипт не нацелен на то, чтобы просто проверить доступность ресурса и отследить ошибку в отчёте. Если мы предполагаем, что скрипт гоняется бесконечно, пока смерть сервера не разлучит вас, то отчётом в данном случае будет не Allure, например (хотя я и его прикрутил к скриптам для Project Manager’а), а сервисы для графического отображения типа Grafana или сервисы мониторинга типа Prometheus. Да и сам скрипт, помимо успеха или провала теста, должен собирать ещё кучу полезных данных. В данном примере это может быть время прохождения скрипта, что может дать нам представление о том, в каком состоянии находится сервис. Особенно если учесть, что всегда можно настроить параметры интернет-соединения или любые другие моменты, имитирующие пользователя. И тут мы плавно перейдём к другому вопросу.",
            "Теперь стоит сказать и о том, что мониторинг может быть как на микро-, так и на макроуровне. Под микроуровнем обычно понимают низкоуровневый мониторинг, например, физического оборудования, элемента большого сервиса или что-то подобное. На макроуровне мониторинг предстает как UX-тестирование или тестирование пользовательских путей.",
            "Немного про микроуровень. Вернёмся к проекту по мониторингу интернет-соединения на торговых точках компании. По сути, скрипт достаёт из базы данных ID магазинов, конвертирует их в IP-адреса маршрутизаторов в магазинах и пингует их несколько раз. Помимо того, что в таблице в Grafana этот скрипт отображает «Up» или «Down» в зависимости от доступности каналов, он также собирает время отклика, хранит историю падений и содержит в себе данные об операторе SIM-карты, номере телефона и многое другое. Не очень-то похоже на тест.",
            "Теперь про макроуровень. Высокоуровневый мониторинг уже больше похож на UI/UX-тестирование. В его основе лежит постоянное отслеживание пользовательского пути через UI. Например, для сайта доставки продуктов — от захода пользователя на сайт и выбора товаров до оплаты. Помимо прочего, такой скрипт также собирает множество данных.",
            "В чём, собственно, разница? Основными критериями, отличающими мониторинг от тестирования, являются:",
            "Цель в постоянном наблюдении. Мониторинг — большой брат для ваших сервисов, который безустанно следит за ними;",
            "Сбор данных. Помимо отчётов о тестировании, мониторинг собирает ещё кучу данных;",
            "Быстрое реагирование. Думаю, тут и пояснять не надо. Если у вас есть тестовый сервер или синтетика, то критической баге будет сложно пролезть в прод;",
            "Имитация пользователя. Хоть UX-тесты и тесты пользовательских путей позволяют имитировать действия пользователя, но пишут их далеко не в первую очередь (информация со 100+ собеседований. Всем API подавай, а на пользователей мы кладём...).",
            "Что в итоге польза? Ну, тут я расскажу лучше пару «До и После» примеров.",
            "До разработки мною сервиса мониторинга интернет-соединения на валидацию проблем и выезд оперативной группы на точку уходило два-три рабочих дня. Более того, часто это были ложные вызовы, так как при неработающем основном канале включался резервный. Мониторинг позволил проходить весь процесс за два часа. Процент ложных вызовов за год его работы составляет не более 0,2%. А уж сколько денег это экономит компании, говорить не приходится, если учитывать, что к этому мониторингу подключена вся первая линия поддержки. Во всех магазинах Дикси. По всей России. Даже не думал, что час простоя торговой точки может стоить так много…",
            "А как вам такая новость в ленте: «Основной сайт и сайт доставки магазина Дикси не работают!»? Именно такую новость прочло руководство компании, заваривая утренний кофе. Да, узнавать о падении основных сервисов компании из новостей — это, видимо, не весело. Мне кажется, кофе точно не полезет после такого. Стоит ли говорить, что после этого случая мониторинг был внедрён на все сервисы?",
            "Забавно, правда?",
            "Но остался ещё один вопрос. Специалист по мониторингу и специалист по тестированию — это один и тот же профессионал? Мне кажется, специалист по мониторингу ближе к SDET, чем к AQA. Всё-таки я считаю, что автоматизатор тестирования должен знать и уметь меньше. AQA как бы и должен иметь представление о контейнеризации, но как бы и просто собрать контейнер в Docker достаточно. Специалист по мониторингу должен бы и под каждый свой мониторинг собрать контейнер, и доставить его, и обслужить если что, и k8s знать бы по-хорошему, ноды и воркеры – лучшие друзья. И опять-таки, ты же не знаешь, что может быть важно для бизнеса. Возможно, придётся выйти за рамки PyTest, Selenium и Appium. Уметь разобраться в различных библиотеках, знать асинхронные подходы, парадигмы проектирования, сильные и слабые стороны рабочего языка программирования — всё это важные навыки специалиста по мониторингу. Так что да, SDET более подходящее описание для его деятельности.",
            "Ссылочка на телегу"
        ]
    },
    {
        "Название статьи": "Кратко про Seq2Seq-модели",
        "Дата публикации": "2024-06-04, 09:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Seq2Seq модели — это архитектуры ML, предназначенные для задач, связанных с последовательными данными, типо машинного перевода, суммирования текста, создания описаний к пикчам и прочие задачи, где требуется преобразование одной последовательности в другую.",
            "В этой статье в общих деталях рассмотрим то, как реализуются Seq2Seq модели.",
            "Seq2Seq модели состоят из двух основных частей: энкодера и декодера.",
            "Энкодер преобразует входную последовательность в контекстный вектор, содержащий обобщённое представление всей входной информации. Этот вектор затем используется декодером для генерации выходной последовательности, о декодере чуть ниже.",
            "Перед тем, как подать данные в энкодер, текстовые данные преобразуются в числовые представления с помощью эмбеддинга. Это делается с помощью слоя Embedding, который преобразует каждый токен во входной последовательности в вектор фиксированной размерности. Например, слово milk может быть представлено как вектор размерности 300.",
            "Основу энкодера RNN, обычно реализованные с использованием LSTM или GRU. Эти сети обрабатывают входную последовательность пошагово:",
            "На каждом шаге RNN принимает эмбеддинговое представление текущего токена и скрытое состояние от предыдущего шага.",
            "Выход каждого шага включает новое скрытое состояние, которое передаётся на следующий шаг вместе со следующим токеном.",
            "В конце последовательности RNN генерирует контекстный вектор, который является финальным скрытым состоянием. Этот вектор обобщает всю информацию из входной последовательности и передаётся в декодер для дальнейшей генерации выходной последовательности. Контекстный вектор — это своего рода сжатая версия входной последовательности, включающая в себя её смысл.",
            "Для улучшения качества представления входной последовательности часто используются двунаправленные RNN. В этом случае два RNN работают параллельно: один — слева направо, другой — справа налево. Их состояния объединяются на каждом шаге, что позволяет учитывать как предшествующие, так и последующие слова для каждого токена в последовательности.",
            "Пример реализации энкодера на Keras с LSTM:",
            "Здесь входные данные сначала проходят через эмбеддинговый слой, который преобразует их в векторы фиксированной размерности. Затем эти векторы подаются в LSTM, который на выходе даёт финальные скрытые состояния, использующиеся в качестве контекстного вектора.",
            "В отличие от энкодера, декодер генерирует данные на основе предыдущих предсказаний и контекстного вектора, предоставленного энкодером.",
            "Подобно энкодеру, декодер принимает токены, которые сначала преобразуются в числовые представления с помощью эмбеддинга. Однако, в случае декодера на вход подаются не только реальные данные, но и предсказанные токены на предыдущих шагах.",
            "Декодер также реализован с использованием RNN, как LSTM или GRU. На каждом шаге декодер принимает:",
            "Контекстный вектор от энкодера.",
            "Предыдущий предсказанный токен (или начальный токен для первого шага).",
            "Скрытое состояние от предыдущего шага декодера. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности.",
            "На каждом шаге RNN декодера производит новое скрытое состояние и выходной вектор. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности",
            "Для улучшения качества генерации используется механизм внимания, который позволяет декодеру фокусироваться на различных частях входной последовательности на каждом шаге генерации выходной последовательности. Механизм внимания вычисляет веса для каждого состояния энкодера, определяя важность каждого токена входной последовательности в текущий момент времени.",
            "Пример реализации декодера на Keras с LSTM и механизмом внимания:",
            "Декодер принимает начальные состояния от энкодера и генерирует выходную последовательность.",
            "Машинный перевод — это одна из наиболее базовых задач для Seq2Seq моделей. Реализуем Seq2Seq модельку для перевода с английского на французский язык с использованием Keras.",
            "Для этой задачи будем использовать датасеты французских и английских предложений (они есть на kaggle):",
            "Следующий пример - текстовое суммирование. Это задача генерации краткого представления текста. Реализуем Seq2Seq модель с использованием механизма внимания.",
            "Для этой задачи будем использовать датасет новостей, где заголовок является суммарным представлением статьи:",
            "Реализуем генерацию описаний к изображениям — это задача, где Seq2Seq модели используются для генерации текста, описывающего содержание изображения. Будем использовать предобученную модель InceptionV3 для экстракции признаков изображения и Seq2Seq модельку для генерации текста:",
            "Seq2Seq модели - это очень мощный инструмент для решения задач, связанных с последовательными данными. Они позволяют преобразовывать входные последовательности в выходные с высокой точностью, в особенности при использовании механизмов внимания (об этом не забываем).",
            "В завершение хочу порекомендовать бесплатные вебинары курса ML Advanced:",
            "Современные модели прогнозирования типа TimesNet и TimeGPT",
            "H2O, TPOT, Autokeras - а вы что, за меня и модели строить будете?"
        ]
    },
    {
        "Название статьи": "Как подружить Llama-3 и YouTube имея всего 40 строк кода",
        "Дата публикации": "2024-06-03, 21:57",
        "Автор статьи": "evg_dc ",
        "Статья целиком": [
            "Сделаем Телеграм бота которому можно кинуть ссылку на YouTube видео и поговорить с ним о содержимом этого видео.",
            "За основу возьмем бота работающего на Llama 3-70b из моей прошлой статьи. Можно использовать и любую другую языковую модель включая ChatGPT или локальную запущенную на Ollama.",
            "Создать Телеграм бота и получить его токен (как это сделать, смотрите инструкции на просторах интернета, например здесь).",
            "Зарегистрироваться на Groq и получить api key (нужен VPN).Заходим по этой ссылке, регистрируем аккаунт, генерим ключ. Платежная карта не требуется.",
            "Замените в коде GROQ_API_KEY на api ключ полученный в Groq и TELEGRAM_BOT_TOKEN на токен телеграм бота, все должно быть в кавычках.",
            "После получения сообщения от пользователя ищем в тексте сообщения ссылку на YouTube видео. Делаем это перебирая все слова и проверяя их на наличие URL. Если ссылка на видео найдена, используя библиотеку youtube-transcript-api забираем транскрипцию.",
            "Далее, полученную транскрипцию подставляем языковой модели в виде сообщения от функции. Здесь мы немного обманываем модель, потому что такой функции нет, но лучше делать так чем ставить транскрипцию в системное сообщение. Модель заточена под работу с функциями и все правильно поймет.",
            "Как и в предыдущей версии, бот будет запоминать последние 6 сообщений и поддерживать диалог.",
            "Запускаем скрипт и в Телеграм боте задаем вопрос со ссылкой на видео:",
            "Посмотреть как это работает можно в YouTubeGPT.",
            "Еще есть НашGPT - это как ChatGPT только модель Llama 3-70b."
        ]
    },
    {
        "Название статьи": "Python в Excel жив?",
        "Дата публикации": "2024-06-03, 17:24",
        "Автор статьи": "Gonchar_POTT ",
        "Статья целиком": [
            "Уже больше месяца экспериментирую, исследую, как разные схемы (паттерны) осознанного дыхания влияют на вариабельность сердечного ритма (Heart Rate Variability, HRV на чуждом языке). В скромной, но не совсем уж крошечной Excel-таблице со столбцами “Паттерн”, “HRV”, “Пульс” набралось 258 записей и мне понадобилось выбрать победителя -- дыхательный паттерн, дающий на выходе максимальное значение HRV. Не вручную же сортировать эти записи!",
            "Да, я знаю: есть Pivot Table & Power Query. Но Pivot Table мне не по душе необходимостью после каждого изменения таблицы-источника делать REFRESH, во-первых, избыточной сложностью создания, во-вторых. И просто так не нравятся Pivot Table, что главное. Что же касается Power Query, то сочетание слов вызывает у меня трепет и учащенное сердцебиение: не понимаю, что это за зверь такой и насколько он страшный или полезный.",
            "Поэтому для подсчета результатов -- и выбора победителя -- применил относительно недавно появившуюся в Excel функцию GROUPBY в связке с XLOOKUP. И, раз уж пришлось группировать данные, решил сравнить нативные функции Excel с GROUPBY от Pandas (мы ведь помним, что сейчас Python можно запустить внутри Excel).",
            "Написал простой код:",
            "Поместил код через =PY( в ячейку А1 Excel",
            "И он прекрасно справился с задачей и выдал таблицу с результатами:",
            "breathing_pattern",
            "HRV",
            "HR",
            "8",
            "physiological sighs moderate",
            "59",
            "65",
            "7",
            "physiological sighs light",
            "57",
            "62",
            "1",
            "4.4-6.6",
            "56",
            "59",
            "6",
            "following pulse",
            "55",
            "61",
            "0",
            "4.2-0-6.4-0",
            "53",
            "62",
            "3",
            "6-6",
            "53",
            "61",
            "4",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "61",
            "2",
            "5-5",
            "52",
            "63",
            "5",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "Комбинация функций GROUPBY и XLOOKUP тоже отработала без изъянов:",
            "breathing_pattern",
            "HRV",
            "HR",
            "physiological sighs moderate",
            "59",
            "65",
            "physiological sighs light",
            "57",
            "63",
            "4.4-6.6",
            "56",
            "60",
            "following pulse",
            "55",
            "61",
            "4.2-0-6.4-0",
            "54",
            "63",
            "6-6",
            "54",
            "61",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "62",
            "5-5",
            "53",
            "63",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "* Для внимательных: разница в данных между двумя таблицами -- плод Python-овского округления до целых чисел.",
            "* В “нативном” подходе нет отсечки паттернов с количеством замеров менее шести.",
            "Выводы и наблюдения по теме:",
            "В Python итоговая таблица сама автоматически изменяет размеры при добавлении новых паттернов. GROUPBY by Excel ведет себя так же, а вот связка GROUPBY&XLOOKUP уже потребует редактирования формул: нужно изменять адреса диапазонов ячеек, к которым обращается XLOOKUP.",
            "Вопреки большему размеру Python-код мне кажется проще и для написания, и для чтения-понимания. Хотя писать код в ячейке Excel -- весьма извращенное удовольствие.",
            "Исполнение кода Python требует интернет-соединения.",
            "Выводы и наблюдения не совсем по теме:",
            "Для меня лично схема дыхания “physiological sighs light” (легкие физиогические вздохи) -- оптимальный выбор.",
            "Более шести месяцев я придумывал, зачем мне может понадобиться живущий в Excel Python и наконец нашел.",
            "Буду благодарен за советы и критику. Постараюсь ответить на вопросы."
        ]
    },
    {
        "Название статьи": "Майним крипто-коины с помощью Python и компьютерного зрения",
        "Дата публикации": "2024-06-03, 13:58",
        "Автор статьи": "temabed ",
        "Статья целиком": [
            "После внезапного обогащения энтузиастов, которые поиграли в начале года в приложение Notcoin в телеграм, подобные проекты стали расти как грибы. Да и грибников заметно поприбавилось. Но в данной статье мы не будем касаться тем блокчейна или финансов, а рассмотрим простой пример применения компьютерного зрения для фарма поинтов в самом популярном, после Notcoin, проекте - хомяке комбате. Название явно на что-то намекает, но да ладно.",
            "Это не первый проект, который я автоматизирую, и не самый нуждающийся в этом. Да и без компьютерного зрения с автоматизацией хомяка можно спокойно обойтись. Но с ним, во-первых, интереснее, а во-вторых - это просто хороший пример с минимумом строк кода для демонстрации возможностей библиотеки cv2. Статья, соответственно, предназначена для энтузиастов и начинающих специалистов.",
            "Начнем с того, что мы установим все необходимые зависимости и импортируем их в свой проект. Вот они, слева направо.",
            "С помощью pyautogui наш бот будет управлять мышью. Keyboard пригодится для назначения горячих клавиш, чтобы управлять работой бота. cv2 наградит бота зрением, пусть и компьютерным, с помощью которого тот будет находить совпадения с искомым изображением. А numpy пригодится для работы с большими массивами, но тут он почти для галочки, не бойтесь. Модуль time тоже понадобится, чтобы ставить таймауты в работе программы.",
            "Далее напишем небольшую конструкцию- переключатель.",
            "Функция change при вызове всего лишь меняет значение переменной work, которая будет использована в бесконечном цикле. И если work будет False, работа нашего кода будет останавливаться. И наоборот запускаться, в противоположном случае.",
            "Кстати, забыл упомянуть, что разработчики проекта, над которым мы сейчас проводим эксперимент, большие молодцы, и убрали возможность пользоваться приложением на десктопных устройствах. Поэтому для его запуска понадобится эмулятор Android.",
            "Теперь определим основную логику работы бота:",
            "Он ищет совпадение с изображением полностью заполненной энергии.",
            "Если находит совпадение, ищет изображение монеты и кликает на неё энное количество раз.",
            "И всё это работает в бесконечном цикле.",
            "Значит со скриншота приложения необходимо вырезать две области, которые помечены красным, и разместить их в отдельные файлы, конечно же.",
            "Теперь напишем функцию для кликов по монетке. Она будет принимать путь к исходному изображению, а так же порог чувствительности для компьютерного зрения и интервал (таймаут) в секундах.",
            "Переменной template будет присвоено исходное изображение монетки, но в оттенках серого, так как мы указали в параметрах 0. Это необходимость, так как в оттенках серого компьютер зрит лучше. Сразу вычисляем высоту и ширину исходника, и присваиваем переменным. А далее по ходу исполнения кода он делает скриншот, сравнивает с исходником, получает координаты области с совпадением, и делает 260 даблкликов по ней. Координаты я ищу немного кривовато и в итоге loc содержит большой массив, из которого я использую лишь самые первые координаты, после чего цикл прерываю. Но лучше сделать не смог, извините.",
            "А теперь напишем аналогичную функцию, но с задачей искать совпадение с картинкой полной энергии, после чего вызывать функцию click.",
            "В целом всё аналогично. Добавил лишь ожидание горячей клавиши, чтобы можно было остановить программу в любое время нажатием на тильду (Ё). Ну и для красоты заключил в try-except.",
            "И это всё. Пишем последние строки и запускаем скрипт (не забыв нажать на Ё для запуска логики в цикле).",
            "По сути, этот код многофункционален, и его без труда, с минимальными изменениями, можно переделать под любые другие задачи. На всякий случай оставлю и полную версию кода:",
            "Не судите меня строго по этому скрипту, большую часть жизни я вообще бегал с пистолетиком, и пристрастился к разработке сравнительно недавно, поэтому всего лишь юный падаван в возрасте. Хотя могу писать и большие скучные штуки, но вот писать о них получится дольше, чем сам код. А если будут вопросы- добро пожаловать в телеграм. У меня там небольшой клуб по интересам."
        ]
    },
    {
        "Название статьи": "Сравниваем популярные алгоритмы кластеризации DBSCAN и OPTICS",
        "Дата публикации": "2024-06-03, 13:51",
        "Автор статьи": "evaclick ",
        "Статья целиком": [
            "Привет, Хабр)",
            "Поговорим сегодня о 2 популярных алгоритмах кластеризации — DBSCAN и OPTICS, посмотрим их особенности и сравним",
            "Поехали!",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Ну, DBSCAN в особом в представлении не нуждается, всё-таки один из самых популярных алгоритмов кластеризации. Поэтому по минимуму теории.",
            "DBSCAN (Density-based spatial clustering of applications with noise, плотностной алгоритм пространственной кластеризации с присутствием шума), как следует из названия, оперирует плотностью данных. На вход он просит матрицу близости точек и два параметра — радиус -окрестности и количество соседей .",
            "Эпсилон-окрестность для любого вектора в метрическом признаковом пространстве определяется как множество точек, отстоящих от не более чем на :",
            "где — выбранная метрика (например, евклидовое расстояние).",
            "В общих чертах, алгоритм DBSCAN можно представить как последовательность следующих этапов:",
            "найти новые точки в -окрестности каждой точки и определить основные точки с более чем соседями.",
            "найти связные компоненты основных точек на графе соседей, игнорируя все неосновные точки.",
            "назначить каждую неосновную точку ближайшему кластеру, если кластер является -соседним, в противном случае считаем точку шумом.",
            "Вот так можно использовать DBSCAN из Sci-Kit Learn + с интерактивными ползунками, работает в Colab'е (в Jupyter Notebook какие-то траблы с этим, если кто знает — please, help):",
            "С использованием DBSCAN в Julia и R особых проблем тоже не возникает —",
            "— Julia:",
            "— R:",
            "В идеальном случае DBSCAN может иметь линейную сложность , но не стоит особо на это рассчитывать. Если не пересчитывать каждый раз точек, то ожидаемая сложность — . Худший случай (плохие данные или брутфорс-реализация) — . Наивные реализации DBSCAN любят отъедать памяти под матрицу расстояний — это явно избыточно. Многие версии DBSCAN умеют работать и с более щадящими структурами данных: sklearn и R реализации можно оптимизировать при помощи KD-tree прямо из коробки.",
            "DBSCAN не вычисляет самостоятельно центры кластеров, однако вряд ли это проблема, особенно учитывая произвольную форму кластеров. Зато DBSCAN автоматически определяет выбросы, что довольно здорово.",
            "Соотношение , где — размерность пространства, можно интуитивно рассматривать как пороговую плотность точек данных в области пространства. Ожидаемо, что при одинаковом соотношении , и результаты будут примерно одинаковы. Иногда это действительно так, но есть причина, почему алгоритму нужно задать два параметра, а не один. Во-первых типичное расстояние между точками в разных датасетах разное — явно задавать радиус приходится всегда. Во-вторых, играют роль неоднородности датасета. Чем больше и , тем больше алгоритм склонен «прощать» вариации плотности в кластерах. С одной стороны, это может быть полезно: неприятно увидеть в кластере «дырки», где просто не хватило данных. С другой стороны, это вредно, когда между кластерами нет чёткой границы или шум создаёт «мост» между скоплениями. Тогда DBSCAN запросто соединит две разные группы. В балансе этих параметров и кроется сложность применения DBSCAN: реальные наборы данных содержат кластеры разной плотности с границами разной степени размытости. В условиях, когда плотность некоторых границ между кластерами больше или равна плотности каких-то обособленных кластеров, приходится чем-то жертвовать.",
            "Существуют варианты DBSCAN, способные смягчить эту проблему. Идея состоит в подстраивании в разных областях по ходу работы алгоритма. К сожалению, возрастает количество параметров алгоритма.",
            "Ок, теперь давайте немного поговорим о плюсах и минусах DBSCAN.",
            "Плюсы DBSCAN",
            "• DBSCAN не требует указания числа кластеров в отличие, скажем, от метода k-средних",
            "• DBSCAN может найти кластеры произвольной формы. Он может найти даже кластеры полностью окружённые (но не связанные с) другими кластерами.",
            "• DBSCAN имеет понятие шума и устойчив к выбросам.",
            "• DBSCAN требует лишь двух параметров ( и ) и большей частью нечувствителен к порядку точек в датасете. Однако, точки, находящиеся на границе двух различных кластеров могут оказаться в другом кластере, если изменить порядок точек, а назначение кластеров единственно с точностью до изоморфизма.",
            "Проблемы DBSCAN",
            "• DBSCAN не полностью однозначен — краевые точки, которые могут быть достигнуты из более чем одного кластера, могут принадлежать любому из этих кластеров, что зависит от порядка просмотра точек (тут стоит сказать, что существует DBSCAN❋, который трактует краевые точки как шум и тем самым достигается полностью однозначный результат)",
            "• Качество DBSCAN зависит от способа измерения расстояния. Наиболее часто используемой метрикой расстояний является евклидова метрика. В случае кластеризации данных высокой размерности эта метрика может оказаться почти бесполезной, что делает трудным делом нахождение подходящего значения . Этот эффект, однако, присутствует в любом другом алгоритме, основанном на евклидовом расстоянии.",
            "• DBSCAN не может хорошо разделить на кластеры наборы данных с большой разницей в плотности, поскольку не удается выбрать приемлемую для всех кластеров комбинацию и .",
            "Что ж, теперь давайте теперь переключимся на алгоритм OPTICS (Ordering Points To Identify the Clustering Structure).",
            "Основная идея OPTICS похожа на DBSCAN, но алгоритм предназначен для избавления от одной из главных слабостей алгоритма DBSCAN — проблемы обнаружения кластеров в данных, имеющих различные плотности. Для этого используется граф достижимости, который определяет достижимое расстояние для каждой точки, которая в дальнейшем будет относиться к ближайшему кластеру. Такой подход позволяет ещё лучше определять кластеры разной плотности, особенно если они расположены близко друг к другу, однако это увеличивает время работы алгоритма.",
            "Реализация OPTICS есть в библиотеке Sci-Kit Learn; вот как можно её импортировать и использовать:",
            "С R тоже проблем нет:",
            "Хорошо, давайте немного об особенностях OPTICS",
            "Плюсы OPTICS:",
            "Устойчивость к шуму (впрочем как и у DBSCAN): OPTICS способен обрабатывать данные с шумом и выбросами.",
            "Способность обнаруживать кластеры любой формы",
            "Не требует заранее заданного числа кластеров",
            "Проблемы OPTICS:",
            "Не всегда эффективен для плотных кластеров: OPTICS может иметь проблемы с эффективным обнаружением плотных кластеров, особенно если они имеют сложные формы.",
            "А вот несколько сфер, где регулярно используется OPTICS:",
            "Анализ сетей и обнаружение аномалий: OPTICS используется для анализа социальных сетей, транспортных сетей и других сетевых структур для выявления кластеров и аномалий.",
            "Биоинформатика: OPTICS применяется в биоинформатике для кластеризации геномных данных, выявления генных паттернов и классификации биологических образцов.",
            "Медицинская диагностика: OPTICS может быть применен для кластеризации медицинских данных, таких как результаты тестов, симптомы пациентов и история заболеваний, с целью выявления паттернов заболеваний или групп пациентов схожего профиля. .",
            "Итак, пришло время сравнить DBSCAN и OPTICS",
            "Вот DBSCAN:",
            "...а вот и OPTICS:",
            "И давайте возьмём для начала , , потом поменяем.",
            "Что мы видим? Для данного датасета DBSCAN выделяет кластеры более логичным и понятным способов, но в кластеризации OPTICS тоже есть пара интересных моментов. Как можно увидеть, точки вокруг главных кластеров DBSCAN безнадёжно отмечает как шум, в то время как OPTICS пытается нащупать кластеры и среди этих точек тоже. Это одна из главных фишек OPTICS — метод способен видеть кластеры разной плотности одновременно за счёт того, что он менее чувствителен к параметру .",
            "Вот довольно показательный пример — и тут OPTICS тоже выделил кластер в точках, которые забраковал DBSCAN:",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Описание алгоритма DBSCAN от Sci-Kit Learn",
            "Описание алгоритма OPTICS от Sci-Kit Learn",
            "Наглядная визуализация DBSCAN",
            "Что ж, надеюсь, статья была полезной)",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)"
        ]
    },
    {
        "Название статьи": "Реализация принципа единственной ответственности на Python",
        "Дата публикации": "2024-06-03, 07:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Сегодня мы рассмотрим одну из основополагающих концепций SOLID-принципов — принцип единственной ответственности или сокращенно - SRP. Разберем, что такое SRP и как правильно его применять в Python.",
            "Принцип единственной ответственности гласит, что каждый класс, метод или модуль должен иметь только одну причину для изменения. Проще говоря, каждый компонент вашей системы должен отвечать только за одну функциональность. Т.е если вам нужно внести изменение, связанное с этой функциональностью, вам придется изменить только один компонент.",
            "Когда каждый класс или модуль выполняет одну четко определенную задачу, становится гораздо проще понять его назначение и взаимодействие с другими частями системы.",
            "Что будет, если не соблюдать SRP?",
            "Если класс или модуль берет на себя несколько обязанностей, это приводит к увеличению сложности кода. Такой код сложнее читать, понимать и поддерживать. Также, когда один класс выполняет несколько задач, изменение в одной из них может непредсказуемо повлиять на другие.",
            "Классы, которые нарушают SRP, обычно плохо масштабируются и трудно переиспользуются. Их невозможно легко адаптировать для других целей или проектов.",
            "Для начала рассмотрим класс, который нарушает принцип единственной ответственности. Представим себе класс UserManager, который одновременно отвечает за создание юзера, валидацию данных и сохранение юзера в БД:",
            "Класс нарушает SRP, т.к выполняет несколько задач: валидацию email, создание пользователя и сохранение его в базу данных.",
            "Для исправления нарушения SRP нужно разделить обязанности на отдельные классы: User, UserValidator, UserDatabase, и UserCreator. Каждый класс будет отвечать только за одну задачу:",
            "Теперь каждый класс отвечает за одну конкретную задачу, что соответствует принципу единственной ответственности.",
            "Рассмотрим другой пример, обработку заказов в интернет-магазине. Изначально есть класс, который нарушает SRP, т.к он одновременно обрабатывает заказ, валидирует данные и отправляет уведомления:",
            "Рефакторинг этого класса для соответствия SRP:",
            "Фасадный паттерн помогает упростить взаимодействие между сложными подсистемами, предоставляя простой интерфейс для клиента. С фасадом можно скрыть сложность подсистем и предоставлять единый интерфейс для взаимодействия с ними.",
            "Предположим, есть система обработки заказов, включающая несколько классов для управления заказами, оплатами и уведомлениями. Без фасадного паттерна клиенту пришлось бы взаимодействовать с каждым из этих классов напрямую:",
            "А с использованием фасадного паттерна все будет выглядеть так:",
            "Интерфейсы и абстрактные классы помогают разделить обязанности и четко определить контракт, который должен реализовать класс.",
            "Создание интерфейсов для валидации, сохранения и уведомления:",
            "Разделяем обязанности на интерфейсы, что позволяет каждому классу реализовывать только свои специфические методы, соответствующие SRP.",
            "Для поддержки SRP и других принципов SOLID в Python можно использовать различные библиотеки.",
            "Pylint помогает анализировать код на наличие ошибок и несоответствий стилю, а также выявляет нарушения принципов SOLID, включая SRP.",
            "Mypy - статический анализатор типов для Python, который помогает обнаруживать типовые ошибки и улучшать структуру кода.",
            "Pytest помогает создавать модульные тесты для каждого отдельного компонента.",
            "Dataclasses модуль позволяет создавать классы данных, которые следуют SRP, отделяя логику данных от поведения.",
            "Про другие архитектурные принципы и инструменты коллеги из OTUS рассказывают в рамках практических онлайн-курсов. Также хочу напомнить о том, что в календаре мероприятий вы можете зарегистрироваться на ряд интересных и абсолютно бесплатных вебинаров."
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 12: Дата и время (издание 2024)",
        "Дата публикации": "2024-06-02, 19:47",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это двенадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как работать с датами и временем таким образом, чтобы это работало для всех ваших пользователей, независимо от того, где они проживают.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения",
            "Глава 12: Дата и время (Эта статья)",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Один из аспектов моего приложения для ведения микроблогов, который я долгое время игнорировал, - это отображение дат и времени. До сих пор я просто позволял Python отображать объект datetime в модели User и даже не потрудился отобразить его в модели Post. В этой главе вы узнаете, как работать с этими временными метками.",
            "Ссылки на GitHub для этой главы: Browse, Zip, Diff.",
            "Использование Python на сервере для отображения дат и времени, которые отображаются пользователям в их веб-браузерах, на самом деле не очень хорошая идея, потому что то, что сервер считает своим местным временем, не будет иметь смысла для пользователей, которые живут в другом часовом поясе.",
            "Совершенно ясно, что сервер должен управлять временем, которое является согласованным и независимым от его собственного местоположения и местоположения пользователей. Если это приложение разрастется до такой степени, что потребуется несколько производственных серверов в разных регионах мира, я бы не хотел, чтобы каждый сервер записывал временные метки в базу данных в разных часовых поясах, потому что это сделало бы невозможной работу с этими временами. Поскольку UTC является наиболее используемым единым часовым поясом и поддерживается в классе datetime, именно его я и собираюсь использовать.",
            "В главе 4 вы видели, как создавать временные метки UTC для записей в блоге. В качестве напоминания, вот краткий пример, показывающий, как это было сделано.:",
            "Но с этим подходом связана важная проблема. Пользователям из разных мест будет ужасно сложно определить, когда была сделана публикация, если они будут видеть время в часовом поясе UTC. Им нужно было бы заранее знать, что время указано в UTC, чтобы они могли мысленно подогнать его к своему собственному часовому поясу. Представьте пользователя, скажем, в часовом поясе PDT на Западном побережье США, который публикует что-то в 15: 00 и сразу видит, что сообщение появляется в 10: 00 по времени UTC, или, если быть более точным, в 22: 00. Это будет очень запутанно.",
            "Хотя стандартизация временных меток в соответствии с UTC имеет большой смысл с точки зрения сервера, это создает проблему удобства использования для пользователей. Цель этой главы - представить решение, которое сохраняет все временные метки, управляемые сервером, в часовом поясе UTC, не отталкивая пользователей.",
            "Очевидным решением проблемы является преобразование всех временных меток из сохраненных единиц UTC в местное время каждого пользователя при их отображении. Это позволяет серверу продолжать использовать UTC для обеспечения согласованности, в то время как преобразование \"на лету\", адаптированное к каждому пользователю, решает проблему удобства использования. Сложная часть этого решения - знать местоположение каждого пользователя.",
            "На многих веб-сайтах есть страница конфигурации, где пользователи могут указывать свой часовой пояс. Для этого мне потребуется добавить новую страницу с формой, в которой я представляю пользователям раскрывающийся список часовых поясов. При первом входе на сайт пользователей могут попросить ввести их часовой пояс в рамках регистрации.",
            "Хотя это достойное решение, решающее проблему, немного странно просить пользователей вводить часть информации, которую они уже настроили в своей операционной системе. Кажется, было бы эффективнее, если бы я мог просто получить настройки часового пояса с их компьютеров.",
            "Как выясняется, веб-браузер знает часовой пояс пользователя и предоставляет его через стандартные API JavaScript даты и времени. На самом деле есть два способа воспользоваться информацией о часовом поясе, доступной через JavaScript:",
            "Подход \"старой школы\" заключался бы в том, чтобы веб-браузер каким-то образом отправлял информацию о часовом поясе на сервер, когда пользователь впервые входит в приложение. Это можно было бы сделать с помощью вызова Ajax или гораздо проще с помощью мета-тега обновления. Как только сервер узнает часовой пояс, он может сохранить его в сеансе пользователя или записать в таблицу users в базе данных, и с этого момента корректировать с его помощью все временные метки во время отображения шаблонов.",
            "Подход \"новой школы\" заключается в том, чтобы ничего не менять на сервере и позволить преобразованию UTC в местный часовой пояс происходить в браузере с использованием JavaScript.",
            "Оба варианта допустимы, но второй имеет большое преимущество. Знания часового пояса пользователя не всегда достаточно для представления дат и времени в формате, ожидаемом пользователем. Браузер также имеет доступ к конфигурации языкового стандарта системы, которая определяет такие параметры, как время утра / вечера в сравнении с 24-часовыми часами, формат отображения даты DD / MM / ГГГГ в сравнении с MM / DD / ГГГГ и многие другие культурные или региональные стили.",
            "И если этого недостаточно, у подхода новой школы есть еще одно преимущество. Есть библиотека с открытым исходным кодом, которая выполняет всю эту работу!",
            "Moment.js это небольшая библиотека JavaScript с открытым исходным кодом, которая выводит отображение даты и времени на новый уровень, поскольку предоставляет все мыслимые варианты форматирования, а затем и некоторые другие. Некоторое время назад я создал Flask-Moment, небольшое расширение Flask, которое позволяет очень легко интегрировать moment.js в ваше приложение.",
            "Итак, давайте начнем с установки Flask-Moment:",
            "Это расширение добавляется в приложение Flask обычным способом:",
            "app/__init__.py: Пример Flask-Moment.",
            "В отличие от других расширений, Flask-Moment работает вместе с moment.js, поэтому все шаблоны приложения должны включать эту библиотеку. Чтобы гарантировать, что эта библиотека всегда доступна, я собираюсь добавить ее в базовый шаблон. Это можно сделать двумя способами. Самый прямой способ - явно добавить тег <script>, который импортирует библиотеку, но Flask-Moment упрощает задачу, предоставляя функцию moment.include_moment(), которая генерирует тег <script>:",
            "app/templates/base.html: Включить moment.js в базовый шаблон.",
            "В большинстве случаев библиотеки JavaScript, используемые приложением, включены в конец содержимого <body>, где находится загрузочный JavaScript-код.",
            "Moment.js делает класс moment доступным для браузера. Первым шагом для отображения временной метки является создание объекта этого класса, передающего желаемую временную метку в формате ISO 8601. Вот пример, запущенный в консоли JavaScript браузера:",
            "Если вы не знакомы со стандартным форматом даты и времени ISO 8601, этот формат выглядит следующим образом:",
            "Я уже решил, что буду работать только с часовыми поясами UTC, поэтому последней частью всегда будет +00:00 или в некоторых случаях эквивалент Z, который представляет UTC в стандарте ISO 8601.",
            "В объекте moment предусмотрено несколько методов для различных вариантов рендеринга. Ниже приведены некоторые из наиболее распространенных вариантов:",
            "В этом примере создается объект moment, инициализированный 28 июня 2021 года в 21:45 по Гринвичу. Вы можете видеть, что все параметры, которые я пробовал выше, отображаются в UTC + 1, который является часовым поясом, настроенным на моем компьютере. Вы можете ввести вышеуказанные команды в консоли вашего браузера, убедившись, что в странице, на которой вы открываете консоль, включена moment.js. Вы можете сделать это в микроблоге, при условии, что вы внесли вышеуказанные изменения для включения moment.js, или также на https://momentjs.com/.",
            "Обратите внимание, как разные методы создают разные представления. С помощью метода format() вы управляете форматом выходных данных с помощью строки формата. Метод fromNow() интересен тем, что он отображает временную метку по отношению к текущему времени, поэтому вы получаете выходные данные, такие как \"минуту назад\" или \"через два часа\" и т.д.",
            "Если вы работали непосредственно в JavaScript, приведенные выше вызовы возвращают строку с отображаемой временной меткой. Затем вам предстоит вставить этот текст в нужное место на странице, что, к сожалению, требует работы с DOM. Расширение Flask-Moment значительно упрощает использование moment.js за счет включения в ваши шаблоны объекта moment, аналогичного объекту JavaScript.",
            "Давайте посмотрим на временную метку, которая отображается на странице профиля. Текущий шаблон user.html позволяет Python генерировать строковое представление времени. Теперь я могу отобразить эту временную метку с помощью Flask-Moment следующим образом:",
            "app/templates/user.html: Отрисовка временной метки с помощью moment.js.",
            "Итак, как вы можете видеть, Flask-Moment использует синтаксис, аналогичный синтаксису библиотеки JavaScript, с одним отличием, заключающимся в том, что аргументом для moment() теперь является объект Python datetime, а не строка ISO 8601. Вызов moment(), выполняемый из шаблона, автоматически генерирует необходимый код JavaScript для вставки отображаемой временной метки в нужное место DOM.",
            "Второе место, где я могу воспользоваться преимуществами Flask-Moment, находится во вложенном шаблоне _post.html, который вызывается с главной страницы и страницы пользователя. В текущей версии шаблона каждому сообщению предшествует строка \"username says:\". Теперь я могу добавить временную метку, отображаемую с помощью fromNow():",
            "app/templates/_post.html: Отрисовка временной метки во вложенном шаблоне post.",
            "Ниже вы можете увидеть, как выглядят обе эти временные метки при рендеринге с помощью Flask-Moment и moment.js:"
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 11: Дизайн приложения (издание 2024)",
        "Дата публикации": "2024-06-02, 19:46",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это одиннадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как заменить базовые HTML-шаблоны новым набором, основанным на платформе пользовательского интерфейса Bootstrap.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения (Эта статья)",
            "Глава 12: Даты и время",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Вы уже некоторое время играете с моим приложением для ведения микроблогов, поэтому, я уверен, вы заметили, что я не потратил слишком много времени на то, чтобы оно выглядело хорошо, или, лучше сказать, я вообще не тратил на это времени. Шаблоны, которые я собрал, довольно простые, без какого-либо пользовательского оформления. Мне было полезно сосредоточиться на реальной логике приложения, не отвлекаясь на написание красивых HTML и CSS.",
            "Но я уже долго сосредоточен на серверной части этого приложения. Итак, в этой главе я сделаю перерыв и потрачу некоторое время на то, чтобы показать вам, что можно сделать, чтобы приложение выглядело немного более отточенным и профессиональным.",
            "Эта глава будет немного отличаться от предыдущих, потому что я не собираюсь так подробно, как обычно, описывать сторону Python, которая, в конце концов, является основной темой этого туториала. Создание красивых веб-страниц - обширная тема, которая в значительной степени не связана с веб-разработкой на Python, но я расскажу о некоторых основных рекомендациях и идеях о том, как подойти к этой задаче, и у вас также будет приложение с измененным дизайном, которое можно изучить и перенять опыт.",
            "Ссылки на GitHub для этой главы: Обзор, Zip, Diff.",
            "Хотя мы можем утверждать, что программирование - это сложно, наши усилия ничто по сравнению с усилиями веб-дизайнеров, которым приходится создавать веб-страницы, которые красиво и единообразно выглядят в списке веб-браузеров. За последние годы они стали лучше, но в некоторых браузерах все еще есть непонятные ошибки или причуды, которые сильно усложняют задачу создания веб-страниц, которые везде выглядят красиво. Это еще сложнее, если вам также нужно настроить для браузеров планшетов и смартфонов с ограниченным количеством ресурсов и экранов.",
            "Если вы, как и я, разработчик, который просто хочет создавать прилично выглядящие веб-страницы, но у вас нет времени или интереса изучать низкоуровневые механизмы для эффективного достижения этой цели путем написания необработанного HTML и CSS, то единственным практическим решением является использование CSS фреймворка для упрощения задачи. Выбрав этот путь, вы потеряете некоторую творческую свободу, но, с другой стороны, ваши веб-страницы будут хорошо выглядеть во всех браузерах без особых усилий. Фреймворк CSS предоставляет коллекцию высокоуровневых классов CSS с готовыми стилями для распространенных типов элементов пользовательского интерфейса. Большинство этих фреймворков также предоставляют дополнения JavaScript для вещей, которые нельзя выполнить строго с помощью HTML и CSS.",
            "Одним из самых популярных CSS-фреймворков является Bootstrap. Если вы хотите увидеть, какие страницы можно создавать с помощью этого фреймворка, в документации есть несколько примеров.",
            "Вот некоторые преимущества, которые вы получаете при использовании Bootstrap для оформления ваших веб-страниц:",
            "Аналогично выглядит во всех основных веб-браузерах",
            "Настройка размеров для экранов настольных компьютеров, планшетов и телефонов",
            "Настраиваемые макеты",
            "Красиво оформленные панели навигации, формы, кнопки, оповещения, всплывающие окна и т.д.",
            "Самый простой способ использовать Bootstrap - это просто импортировать файл bootstrap.min.css в ваш базовый шаблон. Вы можете либо загрузить копию этого файла и добавить его в свой проект, либо импортировать его непосредственно из CDN. Затем вы можете начать использовать CSS-классы общего назначения, которые он предоставляет, согласно документации, что довольно неплохо. Возможно, вы также захотите импортировать JavaScript-код фреймворка, чтобы использовать самые продвинутые функции.",
            "Как и большинство проектов с открытым исходным кодом, Bootstrap постоянно развивается. Оригинальная версия мега-учебника Flask была создана для Bootstrap 3. Редакция, которую вы сейчас читаете, создана для Bootstrap 5.3. Текущий подход к интеграции Bootstrap является довольно общим и может быть адаптирован к более новым версиям Bootstrap.",
            "Первым шагом в интеграции Bootstrap с Microblog является добавление его файлов CSS и JavaScript в базовый шаблон. На странице быстрого запуска Bootstrap в качестве примера приведена короткая, но полная HTML-страница, которую я копирую ниже для вашего удобства:",
            "Подход, который я могу применить, чтобы объединить это с моим шаблоном base.html, заключается в том, чтобы использовать приведенный выше в качестве нового базового шаблона, заменив теги <title> и <h1> заголовком и основным содержимым исходного базового шаблона соответственно.",
            "Следующий шаг - заменить базовую панель навигации на более удобную из Bootstrap. На странице документации по панели навигации Bootstrap вверху показан хороший пример. Используя этот пример в качестве руководства, я создал панель навигации со ссылками \"Index\", \"Explore\", \"Profile\", \"Login\" и \"Logout\" из микроблога. Для удобства я настроил профиль, а также ссылки для входа и выхода так, чтобы они отображались в крайнем правом углу.",
            "При использовании Bootstrap полезно знать о некоторых базовых примитивах компоновки. Одним из наиболее важных является контейнер, который определяет область содержимого страницы. Два основных контейнера называются container и container-fluid. В первом случае страница настраивается на использование одной из пяти предопределенных ширин страницы и центрирует содержимое в окне браузера. С другой стороны обтекающий контейнер дает вам доступ ко всей ширине страницы. Для этого приложения я решил использовать контейнер по умолчанию, потому что он предотвращает слишком широкое расширение страницы независимо от размера экрана, поэтому часть содержимого страницы будет заключена в один из этих контейнеров следующим образом:",
            "Последняя часть HTML-разметки в шаблоне base.html, которую необходимо адаптировать, - это раздел, отображающий отображаемые сообщения. Компонент Alert от Bootstrap прекрасно подходит для этой задачи.",
            "Вы можете получить полностью переработанный шаблон base.html из репозитория Github для этой главы. Ниже вы можете увидеть упрощенную структуру, если хотите иметь представление о том, как она выглядит.:",
            "app/templates/base.html: Переработанный базовый шаблон.",
            "Благодаря обновленному базовому шаблону внешний вид приложения уже заметно улучшен без необходимости изменять строки кода Python. Если вы хотите убедиться в этом сами, загрузите копию base.html из репозитория GitHub по ссылкам, приведенным в начале этой главы.",
            "Область, в которой Bootstrap проделывает фантастическую работу, заключается в рендеринге полей формы, которые выглядят намного приятнее и чище, чем поля по умолчанию, предоставляемые браузером. В документации по Bootstrap также есть раздел о формах. В начале этого раздела приведен пример формы входа в систему, который показывает базовую структуру HTML.",
            "HTML-код, необходимый для каждого поля, довольно длинный. Ниже вы можете увидеть одно из текстовых полей из примера формы в документации:",
            "Но это слишком просто для нужд Microblog, который включает проверку полей и, возможно, потребуется показывать пользователю ошибки проверки. На странице документации есть раздел о проверке на стороне сервера, в котором показано, как оформить поля с сообщением об ошибке. Вот пример.:",
            "К сожалению, о необходимости вводить такое количество шаблонов для каждого поля в каждой форме не может быть и речи. Это заняло бы слишком много времени и чревато ошибками. Одним из решений является использование макросов Jinja, которые позволяют вам определять повторно используемые фрагменты HTML, а затем вызывать их из ваших шаблонов, как если бы они были функциями.",
            "Например, макрос Jinja для текстового поля, подобного показанному выше, будет иметь вид:",
            "Обратите внимание, как используются условные обозначения для выборочного добавления стиля ошибки, если поле содержит одно или несколько сообщений об ошибках.",
            "Поскольку макрос определен в файле с именем bootstrap_wtf.html, который расположен в каталоге templates, он может быть вызван, когда потребуется отобразить поле. Например:",
            "Макрос отображения полей можно расширить, чтобы он также поддерживал отображение флажков, раскрывающихся списков выбора, кнопок отправки и других типов полей. Он также может принимать второй аргумент с логическим значением, указывающим, следует ли автоматически переводить поле в фокус страницы, что должно быть сделано для первого поля формы. Для еще большего удобства можно создать другой макрос для рендеринга всей формы, просто перебрав поля формы и вызвав form_field() макрос для каждого из них.",
            "Полный bootstrap_wtf.html файл доступен в репозитории GitHub, ссылка на который приведена в начале этой главы. Он включает в себя более полную версию макроса form_field(), показанного выше, и второй макрос с именем quick_form(), который принимает объект формы и отображает все его поля с помощью первого макроса.",
            "Как это выглядит, когда реализовано в реальной форме? Ниже вы можете увидеть переработанный шаблон register.html в качестве примера:",
            "app/templates/register.html: Шаблон регистрации пользователя.",
            "Разве это не здорово? Оператор import вверху работает аналогично импорту Python на стороне шаблона. Который добавляет макрос wtf.quick_form(), который в одной строке кода отображает полную форму, включая ошибки проверки, и все оформлено в соответствии с фреймворком Bootstrap.",
            "Еще раз, я не собираюсь показывать вам все изменения, которые я сделал для других форм в приложении, но все эти изменения внесены в шаблоны, которые вы можете загрузить или просмотреть на GitHub.",
            "Логика представления, которая отображает отдельные записи в блоге, была абстрагирована в подшаблон под названием _post.html. Все, что мне нужно сделать с этим шаблоном, это внести некоторые незначительные корректировки, чтобы он хорошо выглядел в Bootstrap.",
            "app/templates/_post.html: Переработанный подшаблон публикации.",
            "Ссылки на страницы - это еще одна область, в которой Bootstrap предоставляет поддержку. Для этого я просто еще раз обратился к документации Bootstrap и адаптировал один из их примеров. Вот как это выглядит на странице index.html:",
            "app/templates/index.html: Переработаны ссылки на страницы.",
            "Обратите внимание, что в этой реализации вместо скрытия следующей или предыдущей ссылки, когда в этом направлении больше нет содержимого, я применяю отключенное состояние, из-за которого ссылка будет отображаться серым цветом.",
            "Я не собираюсь показывать это здесь, но аналогичное изменение необходимо применить к шаблону user.html. Пакет для загрузки этой главы включает эти изменения.",
            "Чтобы внести в ваше приложение эти изменения, пожалуйста, загрузите zip-файл для этой главы и соответствующим образом обновите свои шаблоны.",
            "Ниже вы можете увидеть несколько фотографий до и после, чтобы увидеть трансформацию. Имейте в виду, что это изменение было достигнуто не затрагивая ни одной строки кода приложения!",
            "Следующая глава =>"
        ]
    },
    {
        "Название статьи": "Расширяем возможности Keras с помощью кастомных слоев",
        "Дата публикации": "2024-06-02, 18:07",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Keras предоставляет мощные инструменты для создания сложных нейронных сетей. Однако иногда стандартного набора слоев недостаточно для решения некоторых задач. В таких случаях на помощь приходят кастомные слои.",
            "Кастомные слои позволяют адаптировать архитектуру модели под особенности данных, улучшая тем самым производительность и точность моделек.",
            "Каждый кастомный слой начинается с определения нового класса, наследующего от tf.keras.layers.Layer. В __init__ происходит инициализация слоя, где можно задать параметры, необходимые для работы слоя:",
            "Тут units определяет количество нейронов, а activation указывает функцию активации. super(CustomLayer, self).__init__(**kwargs) вызывает конструктор базового класса Layer.",
            "Метод build вызывается Keras при первом использовании слоя. Его юзают для создания параметров слоя, которые зависят от размера входных данных:",
            "В методе создаются веса kernel и bias. Функция add_weight создает и регистрирует переменные слоя, которые будут обновляться во время тренировки.",
            "Метод call содержит основную логику вычислений слоя. Он принимает входные данные и возвращает выходные:",
            "В этом методе выполняется умножение входных данных на веса и добавление смещения. Если определена функция активации, она применяется к выходным данным.",
            "После определения кастомного слоя его можно использовать в моделях Keras как обычный слой:",
            "Другие полезные методы:",
            "add_weight: Добавляет переменную веса в слой.",
            "compute_output_shape: Возвращает форму выходных данных на основе формы входных.",
            "get_config: Возвращает конфигурацию слоя в виде словаря, что полезно для сериализации.",
            "Dense слой выполняет простую линейную операцию: умножение входного вектора на матрицу весов и добавление смещения, а затем применяется функция активации:",
            "Convolutional слои применяют свертку фильтра к входным данным, что позволяет выделять пространственные особенности:",
            "Recurrent слои используются для обработки последовательных данных. Один из наиболее распространнных типов рекуррентных слоев — это LSTM:",
            "Dropout слой используется для регуляризации модели, предотвращая переобучение путем случайного зануления некоторых нейронов во время тренировки:",
            "BatchNormalization слой нормализует активации предыдущего слоя, улучшая скорость обучения модельки:",
            "Больше практических инструментов и кейсов коллеги из OTUS рассматривают в рамках практических онлайн-курсов. Напомню, что с полным каталогом курсов можно ознакомиться по ссылке."
        ]
    },
    {
        "Название статьи": "Enbeddrus — обучение независящей от языка эмбеддинг-модели",
        "Дата публикации": "2024-06-02, 17:31",
        "Автор статьи": "efreelancer ",
        "Статья целиком": [
            "Приветствую, хабровчане!",
            "Сегодня хочу рассказать вам историю о том, как я обучил простую и компактную независящую от языка (language agnostic) модель-эмбеддер, которая умеет работать с техническими текстами о PHP и способна извлекать схожие эмбеддинги для параллельных текстов на английском и русском языках.",
            "Основная причина, по которой я решил заняться этим проектом, заключается в том, что мои заметки, код и документация, накопленные за более чем десять лет практики, представляют собой солянку текстов о разных технологиях, языках программирования, пометки о настройке серверов Linux и т.д. на русском и английском языках. Поэтому мне захотелось сделать Retrieval-Augmented Generation (RAG) помогалку, которая сможет принимать запросы пользователя (меня) и эффективно находить информацию в столь разношерстой базе данных, независимо от того на каком языке я сделал запрос и на каком языке написана документация.",
            "Для достижения этой цели как-раз и необходима независимая от языка модель-эмбеддер, которая будет одинаково хорошо работать с техническими текстами на русском и английском языках.",
            "Ещё одним важным аспектом было то, чтобы модель потребляла как можно меньше ресурсов и, если возможно, чтобы её можно было преобразовать в формат GGUF.",
            "Но прежде чем приступить к созданию своего собственного велосипеда, я решил поискать готовые решения, ведь подобная идея очевидна и, возможно, уже реализована другими.",
            "Спойлер: идея не нова, и подобных решений уже достаточно много.",
            "Для построения системы, которая может извлекать одинаковые эмбеддинги для схожих текстов на русском и английском языках, существует несколько решений, например...",
            "Ссылки: arxiv:1907.04307 , kaggle, github",
            "Это проект разработан инженерами Google и поддерживает 16 языков.",
            "Свойства: ~110m параметров, принимает на вход 128 токенов текста и извлекает из них 512-мерный эмбеддинг.",
            "Плюс: поддерживает русский язык.",
            "Минусы: модель основана на Tensorflow, а так же что с 2019го года не было обновлений.",
            "Ссылки: arxiv:1710.04087, github",
            "Это одна из первых попыток инженеров FB создать модель которая способна выполнять задачи по извлечению независящих от языка эмбеддингов.",
            "Плюс: поддерживает русский язык.",
            "Минусы: в наличии имеются веса для пар языков, навроде en-ru, en-de и т.д., весов нет на HuggingFace, ну и с 2018го года проект не развивается.",
            "Ссылки: arvix:2205.12654, github, pypi",
            "Ещё одна модель разработана инженерами FB и, как сказано в ридми на GitHub, поддерживает более 200 языков (хотя если пройти по ссылочкам и посчитать то получится 147 языков).",
            "Свойства: ~256m параметров, принимает 1024 токенов на вход и извлекает из них 1024-мерный эмбеддинг.",
            "Плюсы: она основана на PyTorch и имеет логику переключения между языками которая явно перекочевала из NLLB (о которой я кстати рассказывал в публикации \"Перевод на разные языки используя модель NLLB\" у себя в блоге на Дзен).",
            "Минусы: весов нет на HuggingFace, а модель несовместима с llama.cpp поэтому её не получится конвертировать в GGUF, чтобы можно было запускать на слабом железе (или же в паре с ollama).",
            "Ссылки: arXiv:1908.10084, сайт",
            "Модели Sentence-BERT представляют собой модифицированную версию предобученной BERT, специально адаптированную для генерации эмбеддингов предложений, multilingual версия позволяет извлекать эмбеддинги из текста на разных языках, а paraphrase модели позволяют извлекать похожие эмбеддинги парафраз на разных языках.",
            "Вот пару примечательных моделей, обученных разными способами:",
            "paraphrase-multilingual-MiniLM-L12-v2 имеет 118m параметров, принимает 256 токенов на вход и возвращает 384-мерный эмбеддинг.",
            "paraphrase-multilingual-mpnet-base-v2 имеет 278m параметров, принимает на вход 512 токенов и возвращает 768-мерный эмбеддинг.",
            "Обе эти модели обучены на комбинации из датасетов:",
            "SNLI о котором говорится в публикации \"A large annotated corpus for learning natural language inference\" (570k примеров)",
            "Multi-Genre NLI, подробнее в работе \"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\" (433k примера)",
            "Плюсы: поддерживает русский язык, можно конвентировать в GGUF.",
            "Минусы: модели не очень хорошо понимают технический текст (особенно русский технический жаргон), нет версии в формате GGUF, и к числу фатальных недостатков могу отнести, что эти модели обучил не я ;)",
            "Пришёл к выводу, что тему обучения подобных модей-эмбеддеров уже достаточно хорошо изучили и что можно без особых сложностей реализовать мою задумку.",
            "В качестве базовой модели решил взять модель google-bert/bert-base-multilingual-uncased, потому что:",
            "У этой крохи всего 168m параметров, что чуть больше чем у paraphrase-multilingual-MiniLM-L12-v2, но меньше чем у paraphrase-multilingual-mpnet-base-v2;",
            "На вход она принимает 512 токенов, а на выходе возвращает 768-мерный эмбеддинг, столько же у paraphrase-multilingual-mpnet-base-v2;",
            "Модель обучена на датасете wikipedia представляющем из себя Text Corpora, а там, сами понимаете, примеров текста больше, чем SNLI и Multi-Genre NLI вместе взятые;",
            "Модель uncased, то есть обучение происходило на регистронезависимых текстах (сиречь всё переводилось в lowercase).",
            "С моделью определились, теперь перейдём к вопросу выбора датасета...",
            "Изначально я хотел собрать больше датасетов, но, собирая датасет по PHP, я понял, какой это трудоёмкий процесс, и решил уменьшить свои амбиции.",
            "Итак, после поиска в интернете я нашёл только один подходящий датасет: OPUS PHP v1 на 2k примеров, содержащий пары текстов на русском и английском языках, по теме PHP.",
            "Из указанного датасеста я использовал только английский корпус (так как русский корпус был очень низкого качества), далее задействовал инстанс LibreTranslate для перевода английских текстов на русский и очистил данные от аномалий и шума (сценарий dataset_php_build.ipynb). Затем вручную перевёл кривые места с помощью Google и Yandex Translate и экспортировал результат в CSV формат. Данные отсортировал и удалил дубликаты (сценарий dataset_php_undup.py) после чего осталось 1.6k примеров.",
            "В финале попросил ChatGPT сгенерировать 100 примеров пар технического текста о PHP на русском и английском языках для сплита eval, а очищенные данные использовал для сплита train.",
            "Результат выгрузил (сценарий dataset_php_publish.ipynb) на HuggingFace: evilfreelancer/opus-php-en-ru-cleaned .",
            "Для создания эффективного эмбеддера, способного работать с техническими текстами о PHP на русском и английском языке, я решил провести обучение модели в два этапа, сначала выполнить Domain Adaptation, чтобы модель могла работать с техническими текстами на английском языке, а после этого обучить её на Parallel Corpora из русских и английских текстов.",
            "Для Domain Adaptation я использовал метод Generative Pseudo Labeling (GPL) (arXiv:2112.07577), данный метод позволяет проводить обучение модели на основе неразмеченных данных, генерируя псевдометки и улучшая качество работы модели для специфических доменов.",
            "Библиотека gpl имеет захардкоженный формат входного датасета и читает данные по определённым путям, поэтому пришлось слегка конвертировать тренировочный датасет и положить результат в директорию datasets (сценарий: dataset_php_convert.py).",
            "Для адаптации модели bert-base-multilingual-uncased к домену английских текстов про PHP я использовал в качестве шаблона скрипт, предложенный авторами проекта GPL на их странице на GitHub, получился следующего вида код:",
            "Полный скрипт тренировки train_domain.py можно найти в репозитории проекта на GitHub.",
            "Процесс обучения включает в себя несколько этапов:",
            "Используется генератор запросов, такой как BeIR/query-gen-msmarco-t5-base-v1, для создания синтетических запросов на основе текстов из корпуса;",
            "С помощью ретриверов, таких как msmarco-distilbert-base-v3 и msmarco-MiniLM-L-6-v3, которые работают с косинусным сходством, извлекаются наиболее релевантные документы для сгенерированных запросов;",
            "Кросс-энкодер, такой как cross-encoder/ms-marco-MiniLM-L-6-v2, используется для создания псевдометок, присваивая оценочные метки соответствия между запросами и документами;",
            "Модель обучается с использованием MarginMSELoss, которая позволяет модели лучше адаптироваться к новому домену.",
            "И так, наша модель обучена работать с новым доменом, поэтому переходим к следующему шагу.",
            "Для обучения модели на параллельных корпусах я использовал метод обучения моделей на разных языках, описанный в примере на сайте Sentence Transformers. Этот метод позволяет обучать мультиязычные модели, используя параллельные тексты на разных языках (заготовка скрипта make_multilingual.py).",
            "Для оценки качества модели я написал юпитер-блокнот, который загружает базовую и дообученную модель, прогоняет пары из eval сплита датасета evilfreelancer/opus-php-en-ru-cleaned и анализирует разницу между эмбеддингами, построенными для текстов на разных языках. Результаты визуализируются в виде графиков. Скрипт можно найти здесь.",
            "На графике видно, что базовая модель bert-base-multilingual-uncased распределяет русские и английские тексты в изолированные кластеры точек, ну а наша задача сделать так, чтобы эти точки были расположены как можно ближе друг к другу.",
            "Подобную задачу позволяет решать MSELoss, так как она минимизирует разницу между эмбеддингом, сгенерированным моделью-учителем (на английском языке) и эмбеддингом, сгенерированным моделью-учеником (на русском языке).",
            "Теперь пару слов про датасеты, решил остановиться на следующем наборе:",
            "evilfreelancer/opus-php-en-ru-cleaned (1.6k) - ранее созданный датасет параллельных текстов на английском и русском языках;",
            "Helsinki-NLP/opus_books (17.5k) - датасет OPUS параллельных текстов из книг.",
            "Выбрал я их потому, что мои первые эксперименты с обучением модели на только PHP датасете показали, что у модели происходит overfitting в результате чего падала общее качество работы модели, поэтому самым логичным решением было добавить ещё один Parallel Corpora общего назначения.",
            "Помимо этого в скрипт обучения я хотел сразу заложить возможность обучать на множестве разных датасетов (имеющих разные форматы данных), в результате чего получилась функция:",
            "В дальнейшем планирую добавить в неё больше датасетов на разные технические темы, но на этапе прототипирования того что есть более чем достаточно.",
            "Двигаемся дальше.",
            "Полный скрипт тренировки train_parallel.py можно найти в репозитории проекта на GitHub, в качестве модели-учителя возьмём google-bert/bert-base-multilingual-uncased, а в качестве модели-ученика ту, что мы обучили ранее на шаге Domain Adaptation.",
            "Обучение происходит в несколько этапов:",
            "Сначала мы загружаем датасеты (функция read_datasets);",
            "Далее выполняем их преобразование в нужный формат, после чего сохраняем на диске (функциия prepare_datasets)",
            "Инициализируем модель-учитель и модель-ученик (тут)",
            "Инициализируем MSELoss, передав ей на вход указатель на модель-ученика (тут)",
            "Запускаем обучение модели-ученика",
            "По завершению обучению давайте попробуем протестировать модель и понять стала ли на лучше извлекать эмбеддинги.",
            "Как видно на графике эмбеддинги извлечённые из русских и английских текстов где-то наложились друг на друга, точность похожести поднялась с 0.83 до 0.94, при этом модель также хорошо разделяет фразы различающиеся по смыслу.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1-domain",
            "Посмотрел я на этот график и пришла в голову мысль, а что если попробовать обучить базовую модель сразу на Parallel Corpora, пропустив шаг с Domain Adaptation?",
            "Правим скрипт тренировки, меняем модель-ученика, получается вот так:",
            "Опять запускаем тренировку и ждём некоторое время, по завершению прогоняем тесты и смотрим что получилось.",
            "Как видно на графиках если обучать сразу на Parallel Corpora модель быстрее, так как не нужно выполнять Domain Adaptation, и лучше обучается извлекать эмбеддинги из параллельных текстов, ведь косинусное расстояние в таком случае между близкими по смыслу фразами на разных языках в среднем в районе 0.97, что выше чем у модели изначально обученной на домене текстов про PHP.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1",
            "Отсюда можно сделать вывод, что дообучение мультиязыковой модели bert-base-multilingual-cased через Domain Adaptation с последующем обучением на Parallel Corpora не имеет особого смысла и проще сразу дообучать её на Parallel Corpora.",
            "Осталось выполнить самую малость, для начала я хочу конвертировать модель в формат GGUF, чтобы можно было использовать обученные модели через llama.cpp, но на этом моменте сильно не будем заострять внимание, сошлюсь на мою публикацию \"Как конвертировать модель BERT в формат GGUF?\" в моём блоге и PR который я создал в проекте llama.cpp.",
            "Но если кратко команды конвертации нужно выполнять и корня проекта llama.cpp и выглядят они следующим образом:",
            "По её завершению в директории models появятся файлы: enbeddrus-v0.1-f16.gguf и enbeddrus-v0.1-domain-f16.gguf.",
            "Полученные модели я выгрузил на серверы Ollama следующим образом:",
            "Выгруженные модели находятся тут и скачать их можно следующей командой:",
            "Содержимое Modelfile'ов можно найти в директории models проекта на GitHub.",
            "https://github.com/EvilFreelancer/enbeddrus",
            "https://huggingface.co/datasets/evilfreelancer/opus-php-en-ru-cleaned",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1-domain",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1",
            "https://ollama.com/evilfreelancer/enbeddrus",
            "Благодаря работе над проектом enbeddrus были достигнуты следующие цели:",
            "Удалось разобрался с тем как подобные модели устроены и как они работают, а так же с тем как их можно обучать;",
            "Был собран датасет с Parallel Corpora тематических текстов о PHP на русском и английском;",
            "Удалось разобраться с методами оценки моделей, а также с тем как эту оценку красиво визуализировать;",
            "Была обучена модель, которая эффективно работает с текстами на двух языках и может быть использована в RAG-системе для поиска и анализа информации.",
            "Полученные результаты подтверждают, что обучение мультиязычных эмбеддеров на основе параллельных корпусов является эффективным подходом для создания моделей, способных работать с текстами на разных языках.",
            "Спасибо за внимание и за что дочитал публикацию до конца! Если у вас есть вопросы или вы хотите связаться со мной, ссылки на мои контакты в социальных сетях можно найти в моём профиле на Хабре."
        ]
    },
    {
        "Название статьи": "Я научу вас неправильно играть в Hearts of iron. Оптимизация довоенной экономики: часть 2",
        "Дата публикации": "2024-06-02, 17:02",
        "Автор статьи": "Glasssparrow ",
        "Статья целиком": [
            "В прошлой части мы создали инструментарий, настало время им воспользоваться.",
            "За долю секунды мы можем провести симуляцию нескольких внутриигровых лет, что позволяет нам применить простейший метод исследования - метод перебора. И, раз уж мы всё равно будем перебирать, стоит также построить графики.",
            "В качестве испытуемой страны мы выберем, конечно, Советский Союз, условия будем выбирать близкие к реальному прохождению.",
            "Во-первых, рассмотрим торговлю. Торговля в игре зависит от многих факторов и не может быть оценена в симуляции, т.к. мы не работаем со всем миром (это потребует больших вычислительных мощностей). Таким образом, торговлю можно взять только из игры, что я и сделал, прокрутив 5 внутриигровых лет и записав количество фабрик получаемых от торговли. При этом закупки брались равными нулю (закупки сильно зависят от того что производит игрок, потому для общего случая я их просто игнорировал).",
            "1 установить_торговлю 18 120 установить_торговлю 10 210 установить_торговлю 7 365 установить_торговлю 5 730 установить_торговлю 9 1100 установить_торговлю 13 1450 установить_торговлю 25 1800 установить_торговлю 18",
            "Во-вторых, рассмотрим технологии. Нам важны технология индустрии и технология строительства. Моменты их развития также были получены из игры следующим образом: технологии исследовались без опережения по времени (с некоторыми погрешностями, конечно), в первую очередь строительство и индустрия, во вторую электроника, всё остальное исследовалось лишь для того чтобы сбить накапливающиеся во время простоя дополнительные 30 дней исследований. Никаких фокусов, решений и политик на исследования использовано не было.",
            "188 construction_tech # технология строительства 1 328 industry_tech # технология индустрии 1 511 construction_tech # технология строительства 2 511 industry_tech # технология индустрии 2 1285 construction_tech # технология строительства 3 1285 industry_tech # технология индустрии 3 1950 construction_tech # технология строительства 4 1950 industry_tech # технология индустрии 4",
            "В-третьих, обратим внимание на фокусы. Рассмотрим 4 варианта: 1) Стандартное быстрое закрытие паранойи с советником на гражданское строительство и частичной мобилизацией2) Оно же, но дополнительно поставим свободную торговлю, когда будет политка3) Также быстро закрываем паранойю, но частичную мобилизацию берем до советника на гражданское строительство (это будет стоить нам 30 политки)4) Вообще никаких фокусов не берем, только советник и частичная мобилизация.",
            "sov # тэг страны.140 добавить_гражданского_советника # если идти по пути Сталина то политки как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранойи 350 добавить_лимит_фабрик 0.1 # фокус ветки паранойи 350 добавить_бонус_строительства 0.05 # фокус ветки паранойи 350 добавить_фабрики 2 # фокус ветки паранойи 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov140 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная #320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная",
            "В общем и целом, получаем достаточно неплохие бонусы на промышленность, но отнюдь не максимальные (есть еще плакаты на -15% товаров народного потребления, есть множество других бонусов на промышленность в фокусах), но для общего случая этого и не нужно, мы (пока) не собираемся ограничивать игрока в свободе выбора фокусов.",
            "Рассмотрим получившиеся графики зависимости максимума количества военных заводов от количества фабрик которые мы строим (это важно, фабрики от фокусов в счетчик не идут) до переключения на военную промышленность.",
            "Видим что 5% от свободной торговли дают нам лишь 4 завода, при том что 2 завода мы можем получить за 30 политки, просто взяв частичную мобилизацию до советника. Скупают ресурсы у СССР или не так активно чтобы свободная торговля была в плюс, или слишком поздно, когда ресурсы уже нужны для производства.",
            "Также видим что бонусы от фокусов в ветке паранойи дают большое преимущество: 116 заводов против 145 (если мы не берем скидку, то нет смысла брать советника до мобилизации экономики). Честно говоря, не ожидал такой разницы, бонусы не выглядели для меня настолько сильными (+5% к скорости строительства, -2% тнп, 10% к лимиту зданий в провинции, 2 фабрики, 2 завода). Товаров народного потребления дают не так много, +5% это как бонус от свободной торговли, места под строительство у СССР и так хватает. Но в сумме разница выходит почти в 30 заводов.",
            "Ну и самое важное: видим малые производные на достаточно широком участке. В сущности, при строительстве от 28 до 50 фабрик, количество военных заводов на 1 января 1941 года остаётся +/- стабильным.",
            "Как было сказано выше, количество заводов относительно постоянно на участке от 28 фабрик до 50 фабрик, рассмотрим это две точки подробнее:",
            "Если начинать строить заводы раньше, то снаряжения к итоговой дате будет больше (см. площадь под графиками военных заводов. Также стоит учесть что эффективность производства будет нарастать со временем, значит, соотношение снаряжение будет больше соотношения площадей), однако я не уверен, насколько следует переходить к максимизации снаряжения. В игру добавили новую систему снабжения (да, для меня она всё еще новая), теперь она требует строительства пунктов снабжения и железных дорог. Таким образом, нельзя считать экономику эффективной если мы можем произвести снаряжение, но не можем доставить его до фронта. Портальные технологии еще более увеличивают важность логистики, т.к. вся произведенная техника сначала телепортируется в столицу и уже после этого отправляется на фронт вместе с пряниками из карамельной страны.",
            "Помимо оптимизации момента перехода с фабрик на военные заводы, можно также оптимизировать количество инфраструктуры. Можно оценить выгодность инфраструктуры при помощи следующего несложного скрипта (нужен только базовый python, никаких сторонних библиотек):",
            "Отдельно отмечу, что приведенный выше код не учитывает что построенные в процессе фабрики тоже будут строить.",
            "Строительство инфраструктуры, это совсем не то же самое что переход с фабрик на военные заводы. Наиболее выгодное количество инфраструктуры разнится в зависимости от количества слотов под строительство. Да и выгода не всегда велика. Скажем так, оптимальное строительство инфраструктуры потребует от игрока определенных усилий.",
            "Наиболее универсальная оптимизация удалась, но что делать с частными случаями пока не понятно. Баланс между логистикой и количеством снаряжения для разных государств будет разным. Понимание того как этот баланс устроен требует большего понимания самой игры (что требует играть в игру правильно, а это не наш случай).",
            "Оптимизация инфраструктуры выглядит многообещающе, но оценка желания игроков применять это оптимизацию - нет. Также сам процесс нахождения оптимального алгоритма строительства с инфраструктурой выглядит достаточно сложно.",
            "В текущей программе упущен такой момент как возможность аннексии других государств. Этот процесс не обязательно сопровождается войной (а тем более серьезной войной), так что под концепцию довоенной экономики вполне подходит. В программе уже реализован алгоритм реализации контроля провинций, остаётся лишь добавить механизм добавления провинций по тэгу страны владельца или по принципу выделяемых стран (например возвращение польских территорий можно реализовать через добавление к СССР всех провинций Белоруссии и Украины, которые еще не входят в состав Советского Союза). Подобное может потребовать расчета строительства для аннексируемых стран (они же тоже развивают свою экономику параллельно вам), что замедлит расчет, но можно упростить его просто до строительства военных заводов, думаю это не должно добавить много погрешности.",
            "С интерфейсом пока всё совсем не здорово. Идеи приходящие мне в голову или звучат очень сложно или звучат еще менее удобно чем редактирование текстовых файлов. Так что gui пока застопорился.",
            "readme.txt будет обновляться по мере внесения изменений в программу. Если вы считаете что в нём чего-то не хватает, можете мне написать в дискорде или прямо в комментарии под этой статьей. В целом, планирую постепенно работать над читаемостью кода и документацией проекта.",
            "Весь код репозитория распространяется по лицензии MIT, которая гласит следующее: \"делайте с кодом что хотите, но не надо из-за него со мной судиться\". В общем, свободное ПО и всё такое, развлекайтесь, если хотите."
        ]
    }
][
    {
        "Название статьи": "Машинное обучение с Python и TensorFlow на Windows. Быстрый старт",
        "Дата публикации": "2024-06-06, 10:10",
        "Автор статьи": "Sber ",
        "Статья целиком": [
            "Словосочетание «машинное обучение» становится всё более значимым с каждым годом и проникает во все возможные сферы жизни, а с появлением в открытом доступе таких нейронных сетей как Chat GPT [1] интерес к машинному обучению стал высок как никогда. Но при этом многих отпугивает сложность создания своих систем на основе машинного обучения, потому что нужно одновременного использовать и настраивать много разных инструментов разработки.",
            "Поэтому я хочу представить вашему вниманию максимально простую инструкцию для быстрого погружения в мир машинного обучения. Инструкция ориентирована в первую очередь на начинающих программистов, мы будем применять Python 3 [2] с библиотекой TensorFlow [3]. Это лучший выбор для начинающих из-за простоты языка и большого сообщества разработчиков, использующих TensorFlow.",
            "Ваш процессор должен поддерживать AVX-инструкции [4], а видеокарта должна поддерживать архитектуру CUDA начиная с версии 3.5 (список подходящих видеокарт [5]):",
            "AVX (Advanced Vector Extensions) — это расширение системы команд x86-архитектуры для микропроцессоров Intel и AMD, предложенное Intel в марте 2008 года.",
            "CUDA (Compute Unified Device Architecture) — это программно-аппаратная архитектура параллельных вычислений, разработанная компанией NVIDIA. Она позволяет существенно увеличить вычислительную производительность благодаря использованию графических процессоров (GPU).",
            "Далее я буду говорить о TensorFlow 2.10, так как последующие версии требуют установки Windows Subsystem for Linux (использование WSL усложнит установку и ограничит количество подходящих версий Windows). TensorFlow 2.10 будет работать на Windows 7 и более новых версиях операционной системы. Версия Python должна быть от 3.9 до 3.11.",
            "Нужно начать с пакета Microsoft Visual C++ для Visual Studio 2015, 2017 и 2019. Далее установите Python с минимальным набором библиотек и менеджером пакетов — лучше всего для этого подходит Anaconda [6]. Она содержит в себе дистрибутив языка Python, систему управления средой разработки Anaconda Navigator (визуальный менеджер пакетов, управление дополнительным софтом и т. д.) и базовый набор библиотек. Но если вы не фанат визуальных сред разработки или не хотите ставить лишние программы на компьютер, то можно ограничиться Miniconda (это то же самое, что и Anaconda, но без лишних программ, только библиотеки и консольный менеджер пакетов Conda) [7].",
            "После установки Anaconda нужно запустить Anaconda Powerschell Prompt и выполнить в консоли команду:",
            "Она создаст новую среду conda (в рамках одной среды можно использовать определённый набор библиотек, это полезно, если для разных проектов нужны разные версии библиотек) и установит для неё Python 3.9.",
            "Затем с помощью команды activate активируем созданную выше среду (deactivate возвращает базовую среду):",
            "Теперь нужно установить свежие драйверы для вашей видеокарты [8], а если они у вас установлены, то можно переходить к установке пакетов CUDA и cuDNN. Это можно сделать выполнив одну команду:",
            "Указанные выше версии пакетов лучше не менять, так как могут быть проблемы с совместимостью библиотек. Выполнение этой команды выглядит так:",
            "Для корректной работы TensorFlow требует свежая версия pip (это менеджер пакетов). Обновите его:",
            "TensorFlow можно установить и через conda (или через Anaconda Navigator), но разработчики TensorFlow рекомендуют использовать именно pip. Устанавливаем:",
            "Нужно удостовериться, правильно ли всё настроено и установлено. Для этого выполните в консоли или вашей среде разработки код:",
            "Очень удобно для разработки использовать IDE Spyder, если её установить с помощью Anaconda Navigator для нужной среды. Выглядит установка так (выбираем среду и нажимаем установить):",
            "Обратите внимание на текущую среду, ибо именно в неё будет установлено выбранное ПО. После установки вы можете запустить Spyder из Навигатора от имени выбранной среды (это значит, что будут доступны библиотеки, которые загружены именно для среды tf).",
            "Теперь выполняем код в консоли, который я приводил выше:",
            "Если всё правильно сделано, то система должна вывести список доступных для TensorFlow устройств (в моем случае это видеокарта RTX 3060).",
            "Пришло время создать первый проект, использующий библиотеки машинного обучения. Для теста возьмём стандартный проект классификации ирисов [9]. Создайте скрипт с расширением .py и таким кодом:",
            "По ссылке [9] скачайте файл IRIS.csv из раздела «Входные данные» и разместите его в одной папке со скриптом. Этот файл содержит исходные данные для обучения модели. Затем запустите выполнение скрипта и дождитесь его завершения:",
            "Если скрипт отработал корректно, в конце вы увидите график обучения модели в разделе Plots.",
            "Примечание: при первой попытке выполнения этого скрипта интерпретатор выдаст ошибку об отсутствии необходимых библиотек. При реализации своих проектов вам придётся использовать и устанавливать различные библиотеки. Устанавливать их можно разными способами консольными менеджерами пакетов (pip или conda) или визуальным менеджером пакетов Anaconda Navigator. Старайтесь использовать только один способ, иначе может возникнуть путаница и конфликты между разными версиями библиотек. Я обычно использую визуальную установку через Anaconda Navigator, это позволяет более наглядно контролировать набор установленных пакетов и их версии. Пример визуальной установки пакета для программы классификации ирисов:",
            "Для установки пакета активируем вашу среду (tf в моём случае), выбираем раздел «Неустановленные», в поле поиска (справа) вводим название требуемой библиотеки, ставим на ней галочку и нажимаем Apply. После установки всех необходимых библиотек, которые импортируются в начале скрипта, программа классификации ирисов должна работать.",
            "На этом этапе можно поздравить вас с первым запуском проекта на основе TensorFlow. Теперь вы точно готовы к реализации своих проектов с применением технологий машинного обучения.",
            "Желаю вам успехов во всех ваших будущих проектах и спасибо за внимание. До новых встреч.",
            "https://openai.com/",
            "https://www.python.org/",
            "https://www.tensorflow.org/",
            "https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX",
            "https://developer.nvidia.com/cuda-gpus",
            "https://docs.anaconda.com/free/anaconda/install/windows/",
            "https://docs.anaconda.com/free/miniconda/",
            "https://www.nvidia.com/download/index.aspx",
            "https://www.kaggle.com/code/venkatkrishnan/iris-data-tensorflow-neural-network/notebook"
        ]
    },
    {
        "Название статьи": "Введение в gRPC: Основы, применение, плюсы и минусы. Часть I",
        "Дата публикации": "2024-06-05, 23:59",
        "Автор статьи": "0xN1ck ",
        "Статья целиком": [
            "gRPC (gRPC Remote Procedure Call) — это современная высокопроизводительная фреймворк для удаленных вызовов процедур, разработанная Google. gRPC позволяет клиентам и серверам общаться напрямую, используя протокол HTTP/2 и Protocol Buffers (protobuf) в качестве языка описания интерфейсов (IDL). Эта технология предоставляет возможность эффективного взаимодействия между различными компонентами распределенных систем, независимо от языка программирования.",
            "gRPC основывается на архитектуре клиент-сервер и поддерживает множество языков программирования, включая C++, Java, Python, Go, Ruby и многие другие. В основе gRPC лежат следующие ключевые компоненты:",
            "Protocol Buffers (protobuf): Это язык описания данных и инструмент сериализации, который используется для определения сервисов и их методов, а также для обмена данными между клиентом и сервером. Protobuf позволяет описывать структуру данных в специальном .proto файле, который затем компилируется в исходный код для выбранного языка программирования.",
            "HTTP/2: Протокол транспортного уровня, который обеспечивает мультиплексирование запросов, сжатие заголовков, и другие улучшения производительности по сравнению с HTTP/1.x. HTTP/2 позволяет отправлять несколько запросов через одно соединение, что значительно уменьшает задержки и улучшает пропускную способность.",
            "Stub-генерация: gRPC автоматически генерирует клиентские и серверные stub'ы на основе protobuf-файлов, что упрощает процесс интеграции и уменьшает количество шаблонного кода. Клиенты используют сгенерированные stub'ы для вызова методов на сервере так, как если бы они вызывали локальные функции.",
            "gRPC широко используется для построения распределенных систем и микросервисных архитектур. Вот несколько типичных сценариев его применения:",
            "Микросервисы: В больших системах, где микросервисы взаимодействуют друг с другом, gRPC обеспечивает эффективное и надежное общение с низкой задержкой. Это особенно полезно в системах, где важно минимизировать время отклика и обеспечить высокую пропускную способность.",
            "Взаимодействие между разными языками: Благодаря поддержке множества языков, gRPC позволяет разрабатывать системы, где компоненты написаны на разных языках программирования, легко взаимодействуя между собой. Это упрощает интеграцию различных технологий и позволяет выбирать наиболее подходящий язык для каждого компонента системы.",
            "Реализация API: gRPC идеально подходит для создания высокопроизводительных API, где критичны низкие задержки и высокая пропускная способность. API, реализованные с помощью gRPC, могут использоваться как внутри организации, так и предоставляться внешним пользователям.",
            "Мобильные и IoT приложения: gRPC отлично подходит для мобильных и IoT приложений благодаря своей эффективности и низкому потреблению ресурсов. HTTP/2 обеспечивает минимальное использование сетевых ресурсов, что особенно важно для устройств с ограниченными возможностями.",
            "Высокая производительность: Благодаря использованию HTTP/2 и protobuf, gRPC обеспечивает низкие задержки и высокую пропускную способность. Это делает его идеальным выбором для высоконагруженных систем.",
            "Ясно определенные интерфейсы: Использование protobuf для описания сервисов и сообщений обеспечивает четкую контрактность и минимизацию ошибок на этапе компиляции. Это упрощает процесс разработки и интеграции различных компонентов системы.",
            "Поддержка различных языков: gRPC поддерживает множество языков программирования, что позволяет интегрировать компоненты, написанные на разных языках, в единую систему. Это упрощает использование существующего кода и технологий.",
            "Би-ди стриминг: gRPC поддерживает не только однонаправленные и двунаправленные потоки, но и полный дуплекс, что позволяет реализовать сложные сценарии взаимодействия. Это особенно полезно для приложений, требующих постоянного обмена данными в реальном времени, таких как чаты или системы мониторинга.",
            "Автоматическая генерация кода: gRPC генерирует клиентские и серверные stub'ы, что упрощает разработку и снижает количество шаблонного кода. Это сокращает время разработки и уменьшает количество ошибок, связанных с ручным написанием кода.",
            "Крутая кривая обучения: Для новичков gRPC может показаться сложным из-за необходимости освоения protobuf и специфических особенностей HTTP/2. Однако, с практикой и доступными ресурсами, обучение становится легче.",
            "Ограниченная поддержка браузеров: gRPC не поддерживается большинством браузеров напрямую, что требует использования дополнительных прокси-серверов или gRPC-Web. Это добавляет дополнительную сложность при создании веб-приложений, использующих gRPC.",
            "Зависимость от protobuf: Использование Protocol Buffers как основного формата сериализации может быть ограничением для тех, кто предпочитает другие форматы, такие как JSON или XML. Хотя protobuf предлагает высокую производительность и компактность, он требует дополнительных шагов для сериализации и десериализации данных.",
            "Инфраструктурные требования: Для эффективного использования gRPC необходимо обеспечить поддержку HTTP/2 на уровне сетевой инфраструктуры, что может потребовать дополнительных настроек и ресурсов. Это может стать препятствием для некоторых организаций, особенно если их существующая инфраструктура не поддерживает HTTP/2.",
            "gRPC — это мощный инструмент для построения высокопроизводительных распределенных систем и микросервисов. Он обеспечивает эффективное общение между сервисами, поддерживает множество языков программирования и предлагает ясные и контрактно-ориентированные интерфейсы. Однако, как и любая технология, gRPC имеет свои недостатки и требует определенных усилий для освоения и интеграции.",
            "Если вы строите сложную распределенную систему или ищете способ улучшить взаимодействие между микросервисами, gRPC может стать отличным выбором, обеспечивая высокую производительность и надежность. Важно тщательно взвесить плюсы и минусы этой технологии и оценить ее применимость к вашему проекту, чтобы получить максимальную пользу от использования gRPC."
        ]
    },
    {
        "Название статьи": "Майним крипто-пойнты с помощью цветового автокликера на Python",
        "Дата публикации": "2024-06-05, 20:22",
        "Автор статьи": "temabed ",
        "Статья целиком": [
            "Привет, Хабр! Я продолжаю цикл небольших статей для энтузиастов и начинающих программистов о том, как интересно, а иногда и с выгодой, можно применять свои навыки.",
            "В последнее время широкое распространение получили разные крипто-проекты, которые обещают пользователям материальные вознаграждения за определенную активность. Особенно актуальны они стали после успеха Notcoin, который очень неплохо отблагодарил своих пользователей. Не буду углубляться как это всё работает, откуда там деньги, и много ли среди таких проектов скама (да), нас интересует лишь тот момент, что большинство этих проектов можно автоматизировать, а значит не терять драгоценное время.",
            "Да, большинство из таких проектов имеют достаточно примитивную механику, но при этом отнимают слишком много времени, обещая лишь туманную перспективу возможного заработка. А нас, взрослых людей, такой расклад не устраивает, поэтому за нас должен работать хотя бы робот.",
            "Сегодня на операционном столе будет приложение в телеграм, которое в будущем должно стать полноценной децентрализованной биржей, но в настоящее время позволяет новым пользователям лишь зарабатывать внутренние очки. Хочу сразу сказать, что даже руководитель проекта честно заявляет, что обмен этих очков на что-то более материальное в будущем вовсе на обязателен, имейте это ввиду. Я его тоже не рекламирую, и уж тем более не даю никаких финансовых рекомендаций, он нам нужен лишь в качестве примера.",
            "Механика мини игры внутри этого приложения такая: в течение небольшого времени сверху падают цветочки, которые нужно ловить простым нажатием. За них и начисляют очки. Однако, недавно в игре появились еще и бомбочки, при нажатии на которые счет обнуляется.",
            "В связи с нововведением в виде бомбочек бездумный автокликер использовать стало невозможно. Поэтому я делал попытку использования для убийства цветков компьютерного зрения. Но эксперимент вышел так себе, терминатор получился достаточно убогим, и мир ему захватывать еще не скоро.",
            "А сегодня рассмотрим механику всё того же самого простого автокликера, но определяющего цвет пикселя перед нажатием. Который делает клик только при условии, что пиксель в нужном диапазоне. Нам понадобятся всего три библиотеки.",
            "Pyautogui будет управлять мышью и определять цвет пикселя, на который наведен курсор. Keyboard нужен для назначения горячих клавиш, а time для временной задержки.",
            "Назначим три переменные.",
            "Work будет отвечать за работу кода в бесконечном цикле. Если True- выполняется следующая функция, если False - всё останавливается. Points содержит координаты двадцати точек, изначально равные нулю, так как мы их будем назначать после запуска программы. А index нужен, чтобы демонстрировать пользователю, какую по счету точку он назначает. И сразу напишем функцию присваивания координат точкам для кликов.",
            "Эта функция будет срабатывать при нажатии на горячую клавишу и последовательно присваивать координаты каждой из двадцати точек. Координаты будут браться с позиции, на которую в этот момент наведен курсор мыши.",
            "Следующая функция будем изменять значение переменной work и срабатывать так же при нажатии горячей клавиши. Она будет ставить на паузу работу автокликера.",
            "Теперь функция самого кликера.",
            "В цикле for программа пробегает по всем координатам для клика мышью, но на каждой итерации делается снимок экрана, и перед нажатием определяется цвет пикселя, на который указывает курсор. Важно, что метод getpixel возвращает кортеж с тремя числами, которые характеризую цвет по системе RGB. Методом тыка я пришел к выводу, что для определения цветка нам достаточно, чтобы первое число в картеже было в диапазоне от 134 до 218, а вот если второе и третье число равны, это уже свидетельствует о сером оттенке, в которые выкрашены бомбы, и на такое кликать не стоит. Соответствующие условия я и передал в конструкцию if.",
            "Ну и основная функция, которая будет записывать координаты для кликов нажатием на горячую клавишу \"=\" и запускать кликер нажатием на \"~\".",
            "Вот такая простенькая, но многофункциональная программа получилась. Код целиком:",
            "Применить такой код сможет любой, даже не обладая навыками программирования, и не только по назначению, указанному в статье. Вариантов масса, включая самые обычные компьютерные игры.",
            "Поэкспериментировать же с озвученным проектом возможно только лишь в случае, если у вас есть инвайт (пригласительная ссылка). К сожалению, у меня они уже закончились, поэтому поспрашивайте у знакомых либо в моем телеграм-канале под соответствующими постами.",
            "И последнее, эффективность описанного кода сейчас зависит только от стратегии, по которой вы разместили точки для кликов. У меня результат был так себе, но я нашел самый эффективный способ для фарма в таких простых играх - это дети. Хотя это уже читерство, согласен."
        ]
    },
    {
        "Название статьи": "Магия динамического маппинга. Реализация универсальной обработки файлов нефиксированной структуры на Python",
        "Дата публикации": "2024-06-05, 15:58",
        "Автор статьи": "spectr_dev ",
        "Статья целиком": [
            "Привет! На связи Никита Ильин из Spectr, Backend-разработчик с опытом более 5 лет.",
            "Один из проектов, с которым мы работаем, — IBP-платформа для планирования и прогнозирования спроса и продаж в ритейле. В статье поговорим о конкретной реализации для одной из задач в рамках этой платформы на Python и Django. При этом сама концепция может быть реализована абсолютно на любом фреймворке или платформе: Spring, .NET, Laravel.",
            "Мы разрабатываем IBP-платформу для крупной корпорации, где на основе данных, которые поступают из смежных систем, строятся прогнозы и аналитика. И одна из областей — работа с огромным количеством файлов из внешних источников: чтение, обработка, загрузка и запись всех этих данных в БД. При этом существует большое количество различных источников и форматов этих файлов.",
            "Глобально задача состоит в том, чтобы осуществить загрузку из внешних источников в единую внутреннюю систему для последующего анализа и прогноза.",
            "Информация об источниках.",
            "Сейчас в системе около 350 источников. При этом одновременно может поступать до 100 штук новых.",
            "Важное уточнение: один источник — это файл уникальной структуры. Если названия столбцов различаются, то это уже новый файл, новый источник данных.",
            "Информация о файле. Это обычный классический csv-формат. Разделителем может быть либо «;» либо «,». Число колонок — от 5 до 200, но распарсить нужно только количество, которое обозначено в техническом задании. В нашем случае доходило до 40 колонок, все остальное — системные поля. Число строк всегда большое — от 5 млн до 20 млн.",
            "Ниже представлен классический пример файла с условными обозначениями.",
            "Отлично, вся информация о данных у нас на руках! Что же делаем? Тут возможны два пути.",
            "Первая идея, которая у нас возникла, — статическая реализация или, по-другому, «решение в лоб». При такой реализации мы для каждого источника пишем свой парсер и применяем его — эффективно и быстро. Поговорим о преимуществах чуть подробнее.",
            "Быстрая разработка. Раньше уже существовало какое-то количество источников и мы уже знали, как работать по этому пайплайну: без лишних оптимизаций и размышлений. То есть мы приоритезируем источники, отдаем наиболее важные по значимости в первую очередь, а сами занимаемся другими.",
            "Работает надежно, как швейцарские часы. Когда мы пишем парсер на конкретный источник, мы можем написать на него тест. И перед тем как выкатывать, надо посмотреть — а точно ли поведение такое же, как ожидается? Тут мы говорим про стабильность и проверенность того, что наш парсер работает.",
            "Без непредвиденных side-эффектов. Все парсеры строго императивны и изолированы и, благодаря этому, более стабильны. Ведь когда мы говорим про enterprise-разработку, мы точно не хотим, чтобы один наш парсер сломал всю систему.",
            "Сходить на нужный внешний сервер по SFTP.",
            "Именно SFTP, потому что это спецификация нашей задачи, мы так договорились, нам так комфортно общаться между серверами.",
            "Забрать файл.",
            "Применить парсер.",
            "Сохранить в БД.",
            "Работа аналитика. Сверка с локальной структурой БД и проектирование перевода из внешнего имени во внутреннее. Нам нужно узнать, что, где и как хранится, какая между всем этим связь. Данные формируются в виде спецификации, требований и отдаются разработчику.",
            "Работа Backend-разработчика.",
            "Написание кода для валидации файла и его перевода. То есть это сам код, который осуществляет разбор данных в соответствии с ожидаемой структурой.",
            "Добавление конфигурации в общую структуру парсеров: на какой сервер идем, куда и как данные сохраняем. Мы обобщили все на уровне кода, а теперь надо это явно прописать, куда мы идем, чтобы получить данные о конкретном источнике.",
            "Например, когда у нас SFTP-сервер, я не думаю, что вы будете писать одни и те же четыре строчки кода в каждом парсере. Скорее всего, это будет какая-то функция или класс, которому передаем имя файла и доступы для SFTP-сервера, чтобы он пустил в систему.",
            "Работа тестировщика. Отладка, тесты на деве, стейдже на обрезанных данных. На этом этапе мы выявляем и устраняем ошибки в коде парсера. То есть тестировщик должен проверить две вещи:",
            "работает ли код. То есть он берет маленький кусочек файла, например 100–1000 строчек, и смотрит, встали ли данные в БД, в интерфейс, не свалилось ли что-то и работает ли функционал в целом;",
            "какая скорость. Нужно понять, удовлетворяем ли мы скорости загрузки и нет ли каких-то проблем.",
            "Работа тестировщика и DevOps-инженера. Пуско-наладка на реальных данных. Далее мы все выкатываем на прод, делаем первые итерации, проверяем, все ли работает: встали ли данные в интерфейс, ничего ли не потерялось, выдерживаем ли по ресурсам.",
            "Время разработки пропорционально количеству парсеров. Для разработки одного парсера нам нужно 5 дней. Сверка со структурой БД занимает 1 день, написание парсера — 2 дня, на отладку/тесты и пуско-наладку закладываем еще по одномуу дню.",
            "На один парсер — 5 дней, а на 100 — целых 1,5 года. Процесс, конечно, можно оптимизировать и вести параллельно: аналитику не ждать разработчика, а разработчику не ждать тестировщика. Но тем не менее все это в сумме — большой объем очень рутинной работы.",
            "Время доработок пропорционально количеству парсеров. Классический пример: есть 200 источников, при этом появилось требование о том, что столбец — это не целое число, а число с плавающей запятой. И теперь нужно это отвалидировать, чтобы все было в порядке, иначе данные не будут сходиться. А в случае переиспользования кода (DRY) нужно еще приложить усилия к тому, чтобы подумать, как это сделать. К тому же нужно заново пройти по пайплайну. Мы сделали изменения — значит, нам нужно все заново проверить, посмотреть, выкатить и замерить все парсеры.",
            "По итогу формула будет такой: время множественного изменения = время одного изменения * число парсеров. То есть время на расширение числа парсеров или валидируемых ими полей эквивалентно времени разработки. Не получится делать быстрее дублирующуюся работу, придется делать с такой же скоростью.",
            "Расширение числа парсеров или валидируемых ими полей запускает пайплайн заново. Подобная ситуация случилась лично с нами. Изначально было N парсеров, далее убрали 10 и потом добавили еще 15 сверху. А после этого в 20 имеющихся парсерах изменился состав файла и добавился еще один внешний сервер. Приходится начинать все сначала: аналитика –> разработка –> тестирование.",
            "Ключевой вопрос в этой всей ситуации — как преодолеть проблемы «решения в лоб» и сделать результат нашей работы максимально самодостаточным? Мы подумали об этом и пришли к другой идее — динамической реализации.",
            "Давайте вспомним No-code-приложения, например Tilda. Или такой конструктор мобильных приложений, где вы тащите формы, а затем система сама выполняет работу. Код генерируется — вы наслаждаетесь. Примерно то же самое мы сделали в рамках нашего проекта.",
            "В один момент мы подумали: «А что, если разработать пользовательский интерфейс, который позволит самостоятельно решать задачи, минуя аналитика, разработчика и тестировщика?» То есть пользователь сможет сам создать описание для своих действий в админке в виде шаблона. Затем наш магический механизм обработает созданный шаблон. А динамический парсер интерпретирует файлы, соответствующие структуре, описанной в шаблоне, без необходимости дополнительной ручной обработки.",
            "Эту идею мы назвали — шаблон динамического маппинга.",
            "На картинке ниже представлено, как это все можно изобразить с точки зрения пользовательского интерфейса. В этом списке темплейт — наш источник. Мы его назвали шаблоном.",
            "Далее представлено, какие примерно атрибуты могут быть у этого шаблона:",
            "название (name) — на что смотреть в интерфейсе;",
            "внешняя система (external system) — на какой сервер нужно пойти, чтобы достать конкретный файлик (уникальное системное имя шаблона);",
            "имя файлика и директория, в которой он лежит (filepath) — путь к файлу на сервере, с которым связан этот шаблон, и здесь же и имя файла;",
            "системное обозначение для источника (system name) — выбор из внешних систем, куда мы будем подключаться, чтобы туда идти по пути выше.",
            "Дальше мы уже говорим о том, что внутри этого источника. Это находится в отдельной сущности — attributes. Эта сущность включает в себя такие элементы, как:",
            "name — чтобы человеку было на что смотреть в интерфейсе;",
            "system_name — уникальное системное имя поля шаблона, которое мы должны искать в файле;",
            "type — тип данных поля, такие как float, str, str_alpha_numeric, date, int, bool;",
            "field_representations (представления поля) — JSON-структура, представляющая отображение поля на БД;",
            "template (шаблон, Foreign Key) — связь шаблона с общей инфой, к которому относится данное поле.",
            "На каждом этапе присутствует валидация, которая проверяет, например, наличие файлика, полей в файлике, соответствие типу. И, в конечном итоге, это все может записаться в БД.",
            "Сейчас мы построили чисто концептуальное решение. Давайте разберемся, какой результат нам бы принес этот подход — поговорим о его преимуществах.",
            "Элемент продукта закончен",
            "Такой формат реализации сокращает все возможные согласования. Вместо написания отдельного парсера для каждого нового файла и его источника создаем шаблоны, описывающие структуру файла и определяющие соответствие полей самостоятельно. Это существенно сокращает процесс работы, к тому же позволяет сразу же проверить результат. И это будет работать уже завтра. Сегодня написал — завтра это уже готово, сегодня придумал — завтра уже на проде валидируешь новый файл.",
            "Настройка и поддержка, которая оптимизирует время",
            "В случае со статической реализацией каждый день на протяжении полутора лет придется заниматься переводом спецификаций в код. Естественно, это не творческая и скучная задача. Мы все-таки хотим закрыть эту задачу и, конечно, сделать это наиболее интересным для нас способом.",
            "При динамической реализации настройка и поддержка будут намного интереснее, чем просто сконструировать «решение в лоб» и сидеть полтора года переводить спецификации.",
            "Возникает ряд вопросов. А что если где-то что-то отвалится? А почему данные не загружаются? А как вообще это все сохранить в табличку и как это все будет выглядеть? А как эти 20 млн строчек обработать? Явно придется над всем этим поразмыслить. 5 часов подумать — 1 час написать код.",
            "Масштабируемость",
            "Появляются новые требования:",
            "добавить в N-количестве источников проверку на дубликаты и действие, которое надо совершать, если они есть;",
            "а еще в M-источниках добавить проверку на отрицательность.",
            "При этом непонятно, будут ли все эти поля в файле.",
            "Так у нас появились дополнительные атрибуты у поля шаблона:",
            "required (обязательное) — флаг, указывающий, является ли поле обязательным для заполнения;",
            "can_be_negative (может быть отрицательным) — флаг, указывающий, может ли поле содержать отрицательные значения;",
            "contained_in_md_table (содержится в таблице md) — имя таблицы md, в которой содержится это поле (если применимо);",
            "contained_in_md_attribute (содержится в атрибуте md) — имя атрибута md, в котором содержится это поле (если применимо);",
            "duplicates_in_table (дубликаты в таблице) — имя таблицы, в которой разрешены дубликаты этого поля (если применимо);",
            "duplicates_attribute (атрибут дубликатов) — атрибут, определяющий дубликаты этого поля (если применимо);",
            "duplicates_action (действие с дубликатами) — выбор из действий по обработке дубликатов: обновление или пропуск.",
            "И на все это есть 5 дней — вспоминаем сроки разработки одного парсера. В такой системе мы на этапе валидации прописываем новые условия один раз, а дальше клиент уже сам работает с шаблонами и сам отвечает за выбранные им параметры.",
            "admin API (CRUD — в админке) — могут вносить изменения и создавать новые записи;",
            "user API (Read — для всех пользователей) — есть возможность только читать.",
            "Python и Django — это решение удобно для нас, к тому же мы используем его с начала работы над проектом.",
            "Blazingly-Fast Polars — о том, почему мы выбрали именно этот инструмент, рассказывал наш тимлид в статье Битва медведей: Pandas против Polars. Если кратко: этот вариант для наших вариантов использования работает быстрее, чем Pandas.",
            "Paramiko — библиотека для подключения по SFTP. Очень красиво и надежно.",
            "SQLAlchemy — в качестве дополнительной ОRМ. Удобный интерфейс, быстро и красиво.",
            "Здесь мы перевели сущность модели в сущность шаблона, которая была до этого. Из интересного — здесь есть функция temporary_table_name, в ней мы получаем temporary-имя. Это название временной таблицы. О том, для чего нам нужна временная таблица, поговорим чуть позже.",
            "Далее то же самое делаем для TemplateField. Можно просто взять, перевести на другой язык — и все готово.",
            "Как я и говорил, у админов будет полный набор CRUD-операций: чтение, создание, обновление. Но следующий момент более интересный.",
            "Вот эти два поинта ниже нам нужны для того, чтобы у клиента в интерфейсе была возможность посмотреть, какие у нас есть таблички и атрибуты и поля этой таблички.",
            "Классические представления — обычные классы доступа. Здесь идет пагинация по страницам. Далее, когда мы предоставляем список шаблонов, мы передаем только основную информацию о них — название. А когда отдаем только один шаблон, мы его отдаем вместе с атрибутивным составом, чтобы пользователь мог убедиться, что у него все правильно загружается.",
            "На этом этапе мы получаем какой-то путь файла у этого темплейта, то есть не пишем явно — идти на такой-то сервер. А просто говорим — взять у темплейта название файла. И на выходе получается путь файла.",
            "Далее через библиотеку мы подключаемся по SFTP, забираем файл.",
            "У темплейта есть филды — template.fields. Мы их забираем — это и будут наши правила валидации.",
            "django_file = InMemoryUploadedFile",
            "validate_file(django_file, list(template.fields.all()))",
            "BasicFileHeadersValidator. В процессе валидации проверяем наличие данных, дублирование колонок, наличие требуемых колонок. Если какой-то из этих пунктов не проходит, мы отправляем пользователю ошибку, так как нам незачем загружать файл, у которого нет требуемых нам колонок. Мы знаем, что он заведомо сохранит его туда, куда нам не надо.",
            "BasicFileReaderValidator / GeneralValuesFieldsValidator. Базовое чтение (проверка на кодировку, соответствие строк размеру), перевод названий колонок согласно шаблону, нормализация данных и проверка их на соответствие типам, генерация дата-фрейма.",
            "Даже если взять 10 млн строк, с учетом того, что параллельно работает 100 источников, cкорее всего, они упадут по памяти.",
            "Что с этим делать? Классический вариант — разбиение на чанки. В результате этого получается кусок файла, c которым можно продолжать работать. Далее с ним проводим соответствующие этапы валидации и формируем соответствующий дата-фрейм. Но в такой структуре важно, чтобы мы не захотели сделать дополнительную логику — например, агрегацию, дезагрегацию, суммирование, фильтрацию, категоризацию. Для всех этих работ мы используем сохранение во временную таблицу, а потом при необходимости все это сохраняем в основную БД.",
            "Если требование состоит в том, что решение нужно кастомное, то это делает уже другой разработчик в рамках другой задачи. Он идет во временную таблицу, забирает все нужные данные и, соответственно, с ними делает то, что нужно ему. При этом временную таблицу нужно каждый раз чистить перед загрузкой, иначе вы упадете по памяти в БД или будете работать со старыми данными.",
            "Что будем использовать дальше? Во-первых, SQLAlchemy. Собираем из дата-фрейма название колонок, делаем сущности колонок, берем название таблицы и с помощью контекстного менеджера вставляем какое-то количество записей. В нашем случае — 1 млн.",
            "Что такое контекстный менеджер",
            "В Python можно использовать удобный декоратор для этого паттерна. Перед началом мы напишем такой connection_url, где мы вставляем доступ к БД. Здесь конструкция try-finally говорит о том, что мы пытаемся отдать наш движок подключения к БД. Но в любом случае, какие бы ошибки ни были, этот движок будет в конечном итоге закрываться. Это нужно для того, чтобы в БД не висело открытое подключение.",
            "Берем нужную нам модель представления данных, написанную на Django.",
            "Здесь есть специфичный для Django код. Но я почти уверен, что таким образом можно получить табличку из основной БД на любом другом языке.",
            "Дальше мы смотрим поля у этой таблицы и пытаемся найти ее первичный ключ. Для нашего проекта специфично то, что может быть три разных первичных ключа, в зависимости от таблички: обычный ID, External ID либо Name ID.",
            "Итак, мы получили название первичного ключа. Далее мы идем в БД и проверяем, какие записи с этими ID уже есть.",
            "Мы берем их, кладем в оперативную память.",
            "После этого мы идем по дата-фрейму и проверяем, есть ли у нас такая запись.",
            "Если не смогли найти с этим ID объект среди тех, что мы вытащили из БД, то сохраняем ее как новый объект, кладем в какую-то структуру (в нашем случае список) и затем позже создадим их в БД.",
            "Но если мы все-таки смогли найти этот ID, то берем и меняем у найденного объекта все атрибуты.",
            "Условно, в файле одно значение, в БД другое значение для этой строчки — поменяли, сохранили в структуру. И здесь происходит множественное сохранение/обновление. Важно использовать batch_size. Потому что создавать запрос на миллион строчек — невыгодно. Делаем batch_size, разбиваем структуру на множество запросов и делаем с ним.",
            "batch_size=1000 — число объектов в каждом запросе.",
            "Заклинание освоено! А теперь поговорим о недостатках такого решения.",
            "Трудности в отладке и тестировании. Это интересно, но приходится думать, тратить время, пытаться понять, где ошибка, почему не можем распарсить — затратно дебажить.",
            "Строгость в соблюдении принципов. Важно соблюдать всю последовательность действий, про которую мы говорили ранее (этапы реализации динамического решения). Мы не должны запихнуть весь файл в систему, а на выходе говорить, что сделаем полностью кастомную работу.",
            "Ограничение в функциональности. Здесь как раз идет речь об отсутствии некоторых кастомных полей, все динамическое. И если вы тронете хотя бы одну строчку, то для всех остальных это изменение автоматически применится. Один файл, но его тяжело поддерживать.",
            "Дополнительное время на валидацию. Многие знают, что функции в Python вызывать довольно дорого, поэтому вы можете столкнуться с тем, что валидация может занимать много времени. Ни в коем случае не пытайтесь запустить код, который кажется примерно рабочим. Посмотрите, какие есть практики использования функции, почему нельзя использовать лямбду-функцию в цикле и т. д.",
            "Дополнительное время на сохранение во временную БД и/или основную БД. Мы говорим, что у нас есть временная таблица, и после нее вы можете делать некоторые кастомные операции, которые вы хотите. Но в то же время мы тратим минуту-две на то, чтобы сохранить эти данные, и, плюс ко всему, они занимают какую-то дополнительную память на диске.",
            "Как мы можем обойти эти минусы:",
            "Расширение поддерживаемых форматов файлов. Здесь имеются в виду Excel и все подобные структуры. Нам не потребуется писать отдельный парсер для каждого такого формата. Можно просто добавить новый вариант ридера, добавить его в общую структуру и смотреть на расширение файла перед загрузкой.",
            "Оптимизация производительности. Пытаемся ускорить валидацию, оптимизировав все возможные куски, меняя пайплайн вызова валидаторов.",
            "Развитие интерфейсов и возможных конфигураций. Админу нужно дать возможность делать batch_size для источника. То есть он знает, что, например, у него будет 100 валидированных колонок, и здесь batch_size в 100 тыс. — это слишком много, а 1,5 тыс. — уже нормально. Пусть у него будет возможность заменить на любое значение, которое он сам посчитает нужным. Дальше, если говорить о других возможных интерфейсах, у нас есть проверка на отрицательность на случай дублирования (мы меняем эту строчку или пропускаем ее и говорим, что нам все равно, есть ли она).",
            "И на этом все! В статье я поделился своим опытом, основанным на решении конкретной задачи. Надеюсь, что материал поможет вам выбрать правильное решение в аналогичной ситуации и покажет, как можно творчески подходить к задачам. А если у вас появятся вопросы — буду рад на них ответить в комментариях к этой статье!",
            "Статья подготовлена по мотивам доклада Никиты Ильина, Backend-разработчика в Spectr, на митапе #DevTalks. Ссылка на запись доклада:"
        ]
    },
    {
        "Название статьи": "Получение списка людей, посещающих определенные места",
        "Дата публикации": "2024-06-05, 15:12",
        "Автор статьи": "fire64 ",
        "Статья целиком": [
            "Представьте: вы ведете Telegram-канал о животных и хотите пригласить в него посетителей зоопарка. Или вам нужно собрать контакты потенциальных клиентов, посещающих определенный торговый центр. Как это сделать?",
            "Полиция может легко получить такую информацию от мобильных операторов, но что делать обычному человеку?",
            "Ответ – использовать Telegram и его функцию \"Люди рядом\" в сочетании с Python-скриптом.",
            "\"Люди рядом\": эта функция Telegram показывает контакты пользователей, находящихся поблизости, с примерным расстоянием до них (500 м, 1 км, 2 км и 3 км). Отображаются первые 100 ближайших контактов.",
            "Python-скрипт: с помощью библиотеки telethon можно получить доступ к этой информации и автоматизировать процесс сбора контактов.",
            "Установка:",
            "Скачайте и установите Python с официального сайта: https://www.python.org/downloads/",
            "Установите необходимые модули:",
            "Регистрация приложения Telegram:",
            "Зарегистрируйте свое приложение на сайте Telegram: https://core.telegram.org/api/obtaining_api_id",
            "Важно: используйте свой реальный номер телефона, привязанный к Telegram-аккаунту, а не бота.",
            "Создание скрипта:",
            "Создайте файл с расширением .py и вставьте код скрипта (https://pastebin.com/pYPA8PF0).",
            "Замените следующие значения:",
            "api_id = (ваш API ID)",
            "api_hash = (ваш API Hash)",
            "phone_number = '' (ваш номер телефона)",
            "Запустите скрипт.",
            "Выберите на карте нужное местоположение.",
            "Укажите радиус поиска (500, 1000, 2000 или 3000 метров).",
            "Нажмите кнопку \"Начать поиск\".",
            "Скрипт автоматически получит список пользователей Telegram, находящихся в заданном радиусе, и добавит их в ваши контакты.",
            "Данные обновляются Telegram каждые 30 минут.",
            "Отображаются только первые 100 пользователей.",
            "Поиск работает только в регионе, к которому привязан ваш номер телефона.",
            "Отображаются только пользователи со включенной видимостью. В среднем это 10-15%",
            "Важно использовать этот метод этично и уважительно по отношению к другим пользователям. Не рассылайте спам и не используйте полученную информацию в незаконных целях.",
            "Помните: эта информация предназначена только для ознакомления. Перед использованием подобных скриптов убедитесь, что вы не нарушаете правила Telegram и законодательство вашей страны."
        ]
    },
    {
        "Название статьи": "Как в Tele2 автоматизировали тестирование SAP ERP с помощью Python",
        "Дата публикации": "2024-06-05, 13:34",
        "Автор статьи": "a_valeeva ",
        "Статья целиком": [
            "Привет, Хабр! Меня зовут Анастасия Валеева, я – руководитель группы обеспечения качества в Tele2. Наша команда работает в большинстве своём с SAP ERP, и мы не понаслышке знаем, что автоматизация данной платформы — дело далеко не тривиальное. В этой статье я хочу поделиться с вами, как и зачем мы автоматизировали тестирование с помощью Python.",
            "SAP ERP – гибкий инструмент в руках нашей команды. Мы дорабатываем функциональность системы под потребности конкретного бизнес сегмента. Эти изменения производятся по запросу бизнес-пользователей. Объём и влияние доработок могут быть различными, но одно остаётся неизменным – каждая доработка является уникальной. Таким образом, это не простое устранение багов и улучшения текущего функционала, не изменение версионности продукта после оптимизации, а, как правило, абсолютно новый «продукт» в системе. В случае автоматизации функционального тестирования нам потребуется писать автотест на каждую доработку/разработку, что занимает больше времени, чем ручное тестирование (написание автотеста, отладка, оптимизация) + данный автотест с каждой новой разработкой будет уже неактуален, и нужно будет создавать новые и новые из раза в раз. Делаем выводы, что автоматизировать функциональные тексты для нас нерелевантно. А вот регрессионные тесты, которые мы проводим после каждого изменения системы, представляют собой более шаблонные варианты, шаги повторяются, и от их автоматизации есть профит.",
            "Сейчас мы работаем с SAP ERP и интегрированными продуктами (FileNet, BW, Fiori), однако, импортозамещение идёт полным ходом, и мы проводим пилотный проект по миграции на новую платформу. Так или иначе, созданный нами инструмент для автотестов универсален и может быть применён в работе с новой системой.",
            "Из множества инструментов автоматизации мы выбрали для ознакомления четыре наиболее совместимых с SAP ERP:",
            "SAP Scripting;",
            "Tricentis Tosca;",
            "eCatt;",
            "CBTA.",
            "Анализируя, мы исходили из трёх основных для нас факторов: скорость освоения, простота и гибкость, а также бюджет. По каждому из инструментов мы отметили свои плюсы и минусы, собрали информацию в единую таблицу. И вот что у нас получилось.",
            "По количеству зелёных блоков мы увидели, что нашим критериям в большей степени соответствует SAP Scripting.",
            "Принцип работы данного инструмента состоит в том, что он записывает все действия пользователя в системе, на выходе формирует файл в формате .vbs, который в последующем можно запускать в SAP. Соответственно, при запуске этого файла система будет повторять ваши предварительно записанные шаги. Кроме того, данный файл можно корректировать: удалять лишнее, дописывать недостающее или даже полностью переписать. Для этого необходимо открыть файл либо в блокноте, либо в любом другом редакторе, работающем с кодом.",
            "В процессе пилотирования SAP Scripting помимо технических вопросов мы решали несколько административных задач: удобство использования, гибкость, кастомизация, универсальность, прозрачность.",
            "Мы хотели внедрить такой инструмент, который будет полезен не только группе тестирования, но и другим смежным группам нашего подразделения. И поскольку мы говорим об автоматизации, одним из основополагающих факторов для нас было минимальное участие человека в этом процессе. Согласитесь, часто хочется просто нажать на волшебную кнопку \"РАБОТАТЬ\", чтобы оно всё само заработало :)",
            "Добиться данного магического эффекта «работает само» нам помог Python. За это отвечала коллега из моей команды — она написала скрипт для робота, который сейчас работает буквально по одному клику.",
            "Что касается прозрачности, то мы пошли по пути, доступному для любого пользователя. Для этого «прикрутили» Python к файлу Excel. Это означает, что сейчас провести регресс может любой сотрудник — достаточно зайти в файл автотеста и нажать кнопку «СТАРТ».",
            "Бизнес-процесс состоит из набора бизнес-операций. Например, создание логистического заказа состоит из заведения заказа, смены статуса подписания договора, деблокирования заказа и создания счёта-фактуры. Для обеспечения полного регрессионного тестирования мы автоматизируем всю цепочку шагов. На выходе получаем Excel-документ со скриншотами и подробной информацией по каждому шагу тестирования. Причём регресс может запустить любой пользователь, не только тестировщик, это доступно в том числе для менеджеров со стороны бизнеса. А полученные данные (скрипты) можно использовать также для генерации тестовых данных.",
            "Существует несколько способов выполнения автотестов.",
            "1. Отдельно по каждому бизнес-процессу. По каждому модулю финансовой системы SAP ERP создан файл Excel, в котором есть кнопка вызова макроса. По вызову этой кнопки запускается Visual Basic for Applications. VBA обращается к системе SAP и вызывает на выполнение ранее записанный скрипт vbs.Таким образом, мы можем выполнять тестирование по отдельному модулю или бизнес-операции.",
            "2. По всему модулю или нескольким модулям.Для этих нужд как раз используется Python. Наш робот обращается к SAP, открывая рабочее окно. Далее вызывает необходимые файлы Excel, которые работают по описанному принципу макросов на VBA. Таким образом, мы получаем следующую цепочку:",
            "При этом пользователю необходимо только единожды нажать кнопку ВЫПОЛНИТЬ.",
            "Запуск SAP GUI",
            "Заведение функции для чтения файла Excel",
            "Подключение к Excel",
            "На каждом листе в Excel есть подробная входная и выходная информация, при этом входную информацию можно корректировать. Большая часть листов связана между собой, чтобы можно было провести всю цепочку на одних данных, а последующие шаги не зависели от дополнительных действий пользователя.",
            "Все скриншоты, которые создаются в процессе регресса, генерируются вместе с документами и проводками. Лишние скриншоты можно удалить прямо на странице в Excel. При необходимости сотрудник может по номеру документа найти нужную проводку или операцию в SAP. Это является прозрачным и удобным способом анализа логов тестирования.",
            "Рядом с каждым шагом в файле появляется текстовое описание, статус «успешно» или «не успешно» пройден шаг и цветовой индикатор — зеленый означает успешно пройденный этап, красный сигнализирует об ошибках.",
            "Если ошибка является блокирующей для системы и дальнейшее прохождение шагов невозможно, то скрипт остановится, выдаст информационное сообщение и сохранит изменения в файл. Если ошибка не влияет на последующие шаги, то скрипт продолжит работу, а в конце выдаст лог в Excel с отображением корректных и некорректных шагов. При таком раскладе у нас появляется возможность увидеть проблему в моменте и исправить её.",
            "Также, завершение работы скрипта сопровождается звуковым оповещением.",
            "Дополнительно мы настроили автоматическое удаление листов из общей папки через три дня после их создания.",
            "Что в итоге",
            "Мы посчитали, сколько рабочего времени ручных тестировщиков мы экономим при использовании инструмента автоматизации. Получилось, что на один кейс при использовании SAP Scripting мы тратим 31 секунду против 148 секунд при ручном тестировании. Таким образом, 80% времени инженеров высвободилось на другие задачи, и мы смогли повысить эффективность тестирования.",
            "Данный вариант автоматизации является гибким к изменениям. В случае переезда на другую финансовую систему мы перенаправим нашего робота на Python на вызов нужной нам программы. Сейчас одна из наших основных задач – обеспечить качество работы текущего функционала и уже на этой надёжной основе реализовывать улучшения и внедрять новые фичи. Для нашей команды автоматизация тестирования SAP ERP стала интересным и полезным опытом, а бизнесу предоставила доступную, понятную и безотказную систему проверки рабочих процессов."
        ]
    },
    {
        "Название статьи": "Быстрый интерфейс, быстрый деплой",
        "Дата публикации": "2024-06-05, 11:01",
        "Автор статьи": "funtastick ",
        "Статья целиком": [
            "Салют! Не так давно создатели знаменитого pydantic выпустили новый фреймворк — FastUI, который позволяет создавать пользовательские интерфейсы с помощью декларативного кода на Python. В этой статье рассмотрим создание простого приложения и деплой его в Cloud Apps. ❯ Обзор По заявлению авторов фреймворка, фронтенду не нужно (и не следует) знать ничего о приложении, которое вы создаете, вместо этого он должен просто предоставить все компоненты, необходимые для создания интерфейса, а затем бэкенд может предоставить необходимые данные и параметры компонентов. Реализовано это таким образом, что FastUI инкапсулирует описание компонентов интерфейса и в виде классов, затем запускается простое React приложение, которое обращается к эндпоинтам за данными и компонентами. ❯ Пример Для примера давайте напишем простое приложение, предоставляющее информацию о городах из списка с возможностью пагинации. Данные для экспериментов любезно предоставили создатели фреймворка. Для начала опишем pydantic модель и функцию для чтения данных. from pydantic import BaseModel, Field, TypeAdapter import json from pathlib import Path class City(BaseModel): id: int city: str = Field(title=\"Name\") city_ascii: str = Field(title=\"City Ascii\") lat: float = Field(title=\"Latitude\") lng: float = Field(title=\"Longitude\") country: str = Field(title=\"Country\") iso2: str = Field(title=\"ISO2\") iso3: str = Field(title=\"ISO3\") admin_name: str = Field(title=\"Admin Name\") capital: str = Field(title=\"Capital\") population: float = Field(title=\"Population\") def cities_list() -> list[City]: cities_file = Path(__file__).parent / \"cities.json\" with open(cities_file, \"r\", encoding=\"utf-8\") as f: data = json.load(f) cities = [City(**city) for city in data] return cities Далее напишем каркас для нашего примера, с помощью FastAPI. Опишем два роута, первый возвращает необходимые компоненты и данные, а второй — простое React приложение, которое отвечает за запрос и отображение компонентов, полученных из предыдущего. from fastapi import FastAPI from fastapi.responses import HTMLResponse from fastui import AnyComponent, FastUI from fastui import components as c, prebuilt_html from fastui.components.display import DisplayLookup, DisplayMode from fastui.events import BackEvent, GoToEvent app = FastAPI() @app.get(\"/api/cities\", response_model=FastUI, response_model_exclude_none=True) def cities_view(page: int = 1, country=None): cities = cities_list() page_size = 10 # Количество записей в таблице, отображаемых на странице filter_form_initial = {} return c.Page( # Page - базовый контейнер для остальных компонентов components=[ c.Table( # Table - базовая разметка таблицы data=cities[(page - 1) * page_size : page * page_size], #Создаём срез данных для заполнения таблицы data_model=City, #Передаём модель данных columns=[ # Описываем столбцы таблицы DisplayLookup( #Указываем содержимое и размер столбца в процентах field=\"city\", table_width_percent=33 ), DisplayLookup(field=\"country\", table_width_percent=33), DisplayLookup(field=\"population\", table_width_percent=33), ], ), c.Pagination(page=page, page_size=page_size, total=len(cities)), #Кнопки для пагинации ] ) @app.get(\"/{path:path}\") async def html_landing() -> HTMLResponse: \"\"\"Простое React приложение, идёт последним, т.к. соответствует всем маршрутам\"\"\" return HTMLResponse(prebuilt_html(title=\"Большие города\")) Результат работы представлен на рисунке ниже: ❯ Деплой Для деплоя приложений на FastUI можно воспользоваться сервисом Apps, к сожалению рассмотренный фреймворк только набирает популярность, поэтому мы воспользуемся опцией: «деплой из Dockerfile». Для этого достаточно создать Dockerfile и разместить его в корне репозитория. FROM python:3.11 COPY . /app WORKDIR /app RUN pip install -r requirements.txt CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] EXPOSE 8000 Обратите внимание, что при отсутствии в Dockerfile параметра EXPOSE, APPS будет слушать порт 8080 контейнера. Далее достаточно предоставить сервису доступ к аккаунту на github. Затем остаётся следить за логами деплоя: В случае успешного развёртывания появиться удобный дашборд с графиками нагрузки на виртуальную машину: ❯ Заключение В данной статье мы рассмотрели лишь малую часть возможностей фреймворка, однако можно отметить, что FastUI предоставляет новый подход к созданию веб-приложений и позволяет существенно ускорить разработку. Возможно, захочется почитать и это: ➤ Timeweb Cloud CLI ➤ Бесплатный прокси к Docker Hub ➤ Фантастически быстрый деплой веб-приложения ➤ Учимся летать: симуляция эволюции на Rust ➤ Age of Empires – культовая попытка сделать Цивилизацию в реал-тайме Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩"
        ]
    },
    {
        "Название статьи": "Как я создавал аудиоплеер на python с FFmpeg",
        "Дата публикации": "2024-06-04, 18:10",
        "Автор статьи": "Niamorro ",
        "Статья целиком": [
            "Всех приветствую. Сегодня хочу поделиться опытом создания своего первого проекта на Python. Мой проект — это простой аудиоплеер, и я хочу рассказать, как я его создавал, с какими сложностями столкнулся и что из этого вышло.",
            "Выбор языка для первого проекта — это всегда непросто. Я выбрал Python по нескольким причинам:",
            "Простота синтаксиса. Python очень читабельный и понятный, что идеально подходит для новичков.",
            "Богатая стандартная библиотека и сообщество. Множество готовых решений и библиотек, которые можно использовать в своих проектах.",
            "Популярность в разработке. Python — один из самых популярных языков программирования, и навыки работы с ним будут полезны в будущем.",
            "Моя цель была написать простой аудиоплеер, который мог бы играть основные аудиоформаты. Я хотел, чтобы пользователь мог выбирать треки, ставить их на паузу и останавливать, так же изменять скорость проигрывания.",
            "Выбор библиотек занял действительно много времени, так как нужно было выбрать библиотеки которые обновляются, и имеют необходимый мне функционал. Я использовал несколько библиотек:",
            "PySide6: библиотека для создания интерфейсов созданная разработчиками Qt, имеет хорошую поддержку сообщества и регулярные обновления, в дополнение к ней использовал qdarktheme для стилизации интерфейса.",
            "FFmpeg: Универсальный инструмент для обработки видео и аудио.",
            "Sounddevice: Библиотека для воспроизведения и записи звука в Python.",
            "Mutagen: Библиотека для извлечения данных из аудиофайлов.",
            "Выбор файла:",
            "Пользователь выбирает аудиофайл из меню \"Файл\". Поддерживаемые форматы включают MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Выбранный файл передаётся в FFmpeg через подпроцесс для извлечения необработанных аудиоданных. Используемая команда:",
            "Чтение аудиоданных:Аудиоданные считываются блоками и сохраняются в массив NumPy для эффективной обработки.",
            "Регулировка громкости:Регулировка громкости осуществляется путём умножения аудиомассива на коэффициент громкости.",
            "Регулировка скорости воспроизведения:Скорость воспроизведения (например, 2x) управляется через библиотеку sounddevice путём изменения частоты дискретизации.",
            "Поток вывода:Обработанные аудиоданные передаются на аудиовыход через библиотеку sounddevice.",
            "Управление воспроизведением:Элементы управления, такие как воспроизведение/пауза, следующий/предыдущий трек и перемотка, обрабатываются через класс AudioTrigger.",
            "Воспроизведение/Пауза:Использует класс AudioTrigger для начала/остановки аудиопотока.",
            "Следующий/Предыдущий трек:Обновляет текущий индекс трека и загружает следующий/предыдущий трек в плейлисте.",
            "Перемотка:Регулирует позицию воспроизведения, пересчитывая индекс позиции на основе значения ползунка.",
            "Виджет очереди треков:Отображает добавленные ранее папки.",
            "Виджет плейлиста:Отображает содержимое папки.",
            "Виджет информации о треке:Показывает метаданные и обложку для воспроизводимого трека.",
            "Если хотите ознакомиться с исходным кодом или внести свой вклад в проект,",
            "приглашаю вас посетить страницу GitHub проекта. Там вы найдёте весь исходный код аудиоплеера.",
            "Также у проекта есть веб-сайт, где вы можете скачать готовые .exe и .deb пакеты для Windows и Linux. Здесь же доступна подробная документация по установке и использованию программы.",
            "Работа с FFmpeg требовала правильной организации буферизации аудиоданных, чтобы избежать прерываний и задержек при воспроизведении.",
            "Решение: Буферизация данных в массив NumPy.",
            "Треки воспроизводились с неправильной скоростью из-за некорректной частоты дискретизации.",
            "Решение: Я считываю частоту дискретизации трека и открываю аудиопоток с настройками именно для того трека, который в данный момент должен воспроизводиться.",
            "В результате я создал аудиоплеер с основными функциональными возможностями:",
            "Проигрывание аудиофайлов: Поддерживаются популярные форматы MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Управление воспроизведением: Воспроизведение, пауза, остановка, перемотка, следующий/предыдущий трек.",
            "Регулировка скорости воспроизведения: Возможность воспроизводить треки быстрее или медленнее.",
            "Плейлист: Добавление папок с треками.",
            "Информация о треках: Отображение метаданных и обложек альбомов.",
            "Тёмная тема: Благодаря qdarktheme, аудиоплеер имеет современный и стильный интерфейс.",
            "Добавить поддержку потокового аудио: Возможность воспроизводить музыку из интернет-радиостанций и стриминговых сервисов.",
            "Расширить функциональность плейлиста: Добавить возможность создания и сохранения пользовательских плейлистов.",
            "Поддержка эквалайзера: Добавить эквалайзер для настройки звука.",
            "Создание аудиоплеера на Python оказалось полезным опытом. Я научился работать с аудио на низком уровне, обрабатывать потоки и создавать пользовательский интерфейс.",
            "Буду рад любым отзывам и предложениям по улучшению плеера. Спасибо за внимание!",
            "Ссылки:",
            "Исходный код на GitHub",
            "Сайт проекта с загрузкой пакетов"
        ]
    },
    {
        "Название статьи": "Как мониторинг связан с тестированием. Преимущества мониторинга для бизнеса: как экономить время и деньги",
        "Дата публикации": "2024-06-04, 15:59",
        "Автор статьи": "luffity ",
        "Статья целиком": [
            "Привет! Проходя множество собеседований, я не раз слышал вопросы по типу: «Что такое мониторинг?», «Как это связано с тестированием?», «Зачем это нужно?». Для меня, волей случая ставшего специалистом по мониторингу чуть больше года назад, это тривиальные вопросы, однако многие компании либо не знают, что это такое, либо не видят в этом пользы. На одном из последних интервью я услышал интересное мнение от QA Lead о том, что assert должен быть в каждом тесте. Смелое заявление, подумал я. Поэтому, собственно, вы и читаете эту статью.Разберёмся, что такое мониторинг и с чем его едят. А главное, зачем он нужен вообще.",
            "Думаю, начать с небольшого введения обо мне будет наиболее верным для погружения в тему. Сейчас я занимаю должность middle SDET в ООО «МИТ» (если проще, то DIXY Group). Попал я туда как AQA, причём единственным. Я находился в группе по мониторингу корпоративных сервисов, соответственно, кроме меня там были только специалист по мониторингу (мой Lead), DevOps и системный админ. Похоже на стандартное начало анекдота…",
            "Моим заданием на испытательный срок стало написание сервиса по мониторингу интернет-соединения на торговых точках компании. И тут я подумал: какой, блин, мониторинг? С другой стороны, работа на дороге не валяется, тем более настолько приятная. Дело было вечером, делать было нечего…",
            "По итогу этого задания ко мне пришло осознание того, как тесты могут трансформироваться в нечто большее, чем проверки, прогоняемые раз в релиз. Они могут быть целой экосистемой, даже можно сказать: «Глазом Бога» (кто понял отсылку к Форсажу, спасибо).",
            "Начнём мы, конечно же, с поверхности и определим для себя, что такое мониторинг и как он связан с тестированием. В самом распространённом смысле под тестированием понимают сверку ожидаемого и фактического результатов на конечном наборе тестов. Если мы берём стандартные парадигмы тестирования, то такие проверки могут выполняться при добавлении новой фичи, раз в спринт, при релизе и много когда ещё. Однако все эти тестовые прогоны не преследуют цель отслеживать состояние продукта (программного или бизнесового) постоянно.",
            "Как раз здесь и вступает в игру мониторинг. Если о-о-очень грубо сказать, то это, по сути, те же наборы тестов, только несущие цель постоянного наблюдения за состоянием продукта. Но тут стоит уточнить, что это не тестовые кейсы в их привычном понимании.",
            "Думаю, будет проще понять наглядно.",
            "Это всем понятный простой тест. Мы переходим на страницу и ожидаем заголовок. Как можно заметить, для того чтобы исполнить свою главную функцию – сопоставить ожидаемый и фактический результаты, тест содержит в себе assert. Это несомненно верный подход к написанию тестов, так как это позволит более точно валидировать ошибку, а также правильно отобразить её в отчётах, например Allure.",
            "А теперь взглянем на код скрипта мониторинга, который проверяет доступность ресурса.",
            "Сразу бросается в глаза отсутствие assert. Но в таком случае, как такой скрипт вообще можно считать информативным, если он не выводит ошибки? Именно поэтому мы добавим дополнительное действие. Например, найдём какую-то кнопку и нажмём на неё. Теперь, если ресурс не прогрузился или сломался, мы получим TimeoutException и сообщение о том, что именно скрипт не смог сделать.",
            "Возникает вопрос: почему бы тогда точно так же не поставить assert и не ждать лишнее время для выпадения TimeoutException ? Справедливо! Однако возьмём во внимание, что данный скрипт не нацелен на то, чтобы просто проверить доступность ресурса и отследить ошибку в отчёте. Если мы предполагаем, что скрипт гоняется бесконечно, пока смерть сервера не разлучит вас, то отчётом в данном случае будет не Allure, например (хотя я и его прикрутил к скриптам для Project Manager’а), а сервисы для графического отображения типа Grafana или сервисы мониторинга типа Prometheus. Да и сам скрипт, помимо успеха или провала теста, должен собирать ещё кучу полезных данных. В данном примере это может быть время прохождения скрипта, что может дать нам представление о том, в каком состоянии находится сервис. Особенно если учесть, что всегда можно настроить параметры интернет-соединения или любые другие моменты, имитирующие пользователя. И тут мы плавно перейдём к другому вопросу.",
            "Теперь стоит сказать и о том, что мониторинг может быть как на микро-, так и на макроуровне. Под микроуровнем обычно понимают низкоуровневый мониторинг, например, физического оборудования, элемента большого сервиса или что-то подобное. На макроуровне мониторинг предстает как UX-тестирование или тестирование пользовательских путей.",
            "Немного про микроуровень. Вернёмся к проекту по мониторингу интернет-соединения на торговых точках компании. По сути, скрипт достаёт из базы данных ID магазинов, конвертирует их в IP-адреса маршрутизаторов в магазинах и пингует их несколько раз. Помимо того, что в таблице в Grafana этот скрипт отображает «Up» или «Down» в зависимости от доступности каналов, он также собирает время отклика, хранит историю падений и содержит в себе данные об операторе SIM-карты, номере телефона и многое другое. Не очень-то похоже на тест.",
            "Теперь про макроуровень. Высокоуровневый мониторинг уже больше похож на UI/UX-тестирование. В его основе лежит постоянное отслеживание пользовательского пути через UI. Например, для сайта доставки продуктов — от захода пользователя на сайт и выбора товаров до оплаты. Помимо прочего, такой скрипт также собирает множество данных.",
            "В чём, собственно, разница? Основными критериями, отличающими мониторинг от тестирования, являются:",
            "Цель в постоянном наблюдении. Мониторинг — большой брат для ваших сервисов, который безустанно следит за ними;",
            "Сбор данных. Помимо отчётов о тестировании, мониторинг собирает ещё кучу данных;",
            "Быстрое реагирование. Думаю, тут и пояснять не надо. Если у вас есть тестовый сервер или синтетика, то критической баге будет сложно пролезть в прод;",
            "Имитация пользователя. Хоть UX-тесты и тесты пользовательских путей позволяют имитировать действия пользователя, но пишут их далеко не в первую очередь (информация со 100+ собеседований. Всем API подавай, а на пользователей мы кладём...).",
            "Что в итоге польза? Ну, тут я расскажу лучше пару «До и После» примеров.",
            "До разработки мною сервиса мониторинга интернет-соединения на валидацию проблем и выезд оперативной группы на точку уходило два-три рабочих дня. Более того, часто это были ложные вызовы, так как при неработающем основном канале включался резервный. Мониторинг позволил проходить весь процесс за два часа. Процент ложных вызовов за год его работы составляет не более 0,2%. А уж сколько денег это экономит компании, говорить не приходится, если учитывать, что к этому мониторингу подключена вся первая линия поддержки. Во всех магазинах Дикси. По всей России. Даже не думал, что час простоя торговой точки может стоить так много…",
            "А как вам такая новость в ленте: «Основной сайт и сайт доставки магазина Дикси не работают!»? Именно такую новость прочло руководство компании, заваривая утренний кофе. Да, узнавать о падении основных сервисов компании из новостей — это, видимо, не весело. Мне кажется, кофе точно не полезет после такого. Стоит ли говорить, что после этого случая мониторинг был внедрён на все сервисы?",
            "Забавно, правда?",
            "Но остался ещё один вопрос. Специалист по мониторингу и специалист по тестированию — это один и тот же профессионал? Мне кажется, специалист по мониторингу ближе к SDET, чем к AQA. Всё-таки я считаю, что автоматизатор тестирования должен знать и уметь меньше. AQA как бы и должен иметь представление о контейнеризации, но как бы и просто собрать контейнер в Docker достаточно. Специалист по мониторингу должен бы и под каждый свой мониторинг собрать контейнер, и доставить его, и обслужить если что, и k8s знать бы по-хорошему, ноды и воркеры – лучшие друзья. И опять-таки, ты же не знаешь, что может быть важно для бизнеса. Возможно, придётся выйти за рамки PyTest, Selenium и Appium. Уметь разобраться в различных библиотеках, знать асинхронные подходы, парадигмы проектирования, сильные и слабые стороны рабочего языка программирования — всё это важные навыки специалиста по мониторингу. Так что да, SDET более подходящее описание для его деятельности.",
            "Ссылочка на телегу"
        ]
    },
    {
        "Название статьи": "Кратко про Seq2Seq-модели",
        "Дата публикации": "2024-06-04, 09:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Seq2Seq модели — это архитектуры ML, предназначенные для задач, связанных с последовательными данными, типо машинного перевода, суммирования текста, создания описаний к пикчам и прочие задачи, где требуется преобразование одной последовательности в другую.",
            "В этой статье в общих деталях рассмотрим то, как реализуются Seq2Seq модели.",
            "Seq2Seq модели состоят из двух основных частей: энкодера и декодера.",
            "Энкодер преобразует входную последовательность в контекстный вектор, содержащий обобщённое представление всей входной информации. Этот вектор затем используется декодером для генерации выходной последовательности, о декодере чуть ниже.",
            "Перед тем, как подать данные в энкодер, текстовые данные преобразуются в числовые представления с помощью эмбеддинга. Это делается с помощью слоя Embedding, который преобразует каждый токен во входной последовательности в вектор фиксированной размерности. Например, слово milk может быть представлено как вектор размерности 300.",
            "Основу энкодера RNN, обычно реализованные с использованием LSTM или GRU. Эти сети обрабатывают входную последовательность пошагово:",
            "На каждом шаге RNN принимает эмбеддинговое представление текущего токена и скрытое состояние от предыдущего шага.",
            "Выход каждого шага включает новое скрытое состояние, которое передаётся на следующий шаг вместе со следующим токеном.",
            "В конце последовательности RNN генерирует контекстный вектор, который является финальным скрытым состоянием. Этот вектор обобщает всю информацию из входной последовательности и передаётся в декодер для дальнейшей генерации выходной последовательности. Контекстный вектор — это своего рода сжатая версия входной последовательности, включающая в себя её смысл.",
            "Для улучшения качества представления входной последовательности часто используются двунаправленные RNN. В этом случае два RNN работают параллельно: один — слева направо, другой — справа налево. Их состояния объединяются на каждом шаге, что позволяет учитывать как предшествующие, так и последующие слова для каждого токена в последовательности.",
            "Пример реализации энкодера на Keras с LSTM:",
            "Здесь входные данные сначала проходят через эмбеддинговый слой, который преобразует их в векторы фиксированной размерности. Затем эти векторы подаются в LSTM, который на выходе даёт финальные скрытые состояния, использующиеся в качестве контекстного вектора.",
            "В отличие от энкодера, декодер генерирует данные на основе предыдущих предсказаний и контекстного вектора, предоставленного энкодером.",
            "Подобно энкодеру, декодер принимает токены, которые сначала преобразуются в числовые представления с помощью эмбеддинга. Однако, в случае декодера на вход подаются не только реальные данные, но и предсказанные токены на предыдущих шагах.",
            "Декодер также реализован с использованием RNN, как LSTM или GRU. На каждом шаге декодер принимает:",
            "Контекстный вектор от энкодера.",
            "Предыдущий предсказанный токен (или начальный токен для первого шага).",
            "Скрытое состояние от предыдущего шага декодера. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности.",
            "На каждом шаге RNN декодера производит новое скрытое состояние и выходной вектор. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности",
            "Для улучшения качества генерации используется механизм внимания, который позволяет декодеру фокусироваться на различных частях входной последовательности на каждом шаге генерации выходной последовательности. Механизм внимания вычисляет веса для каждого состояния энкодера, определяя важность каждого токена входной последовательности в текущий момент времени.",
            "Пример реализации декодера на Keras с LSTM и механизмом внимания:",
            "Декодер принимает начальные состояния от энкодера и генерирует выходную последовательность.",
            "Машинный перевод — это одна из наиболее базовых задач для Seq2Seq моделей. Реализуем Seq2Seq модельку для перевода с английского на французский язык с использованием Keras.",
            "Для этой задачи будем использовать датасеты французских и английских предложений (они есть на kaggle):",
            "Следующий пример - текстовое суммирование. Это задача генерации краткого представления текста. Реализуем Seq2Seq модель с использованием механизма внимания.",
            "Для этой задачи будем использовать датасет новостей, где заголовок является суммарным представлением статьи:",
            "Реализуем генерацию описаний к изображениям — это задача, где Seq2Seq модели используются для генерации текста, описывающего содержание изображения. Будем использовать предобученную модель InceptionV3 для экстракции признаков изображения и Seq2Seq модельку для генерации текста:",
            "Seq2Seq модели - это очень мощный инструмент для решения задач, связанных с последовательными данными. Они позволяют преобразовывать входные последовательности в выходные с высокой точностью, в особенности при использовании механизмов внимания (об этом не забываем).",
            "В завершение хочу порекомендовать бесплатные вебинары курса ML Advanced:",
            "Современные модели прогнозирования типа TimesNet и TimeGPT",
            "H2O, TPOT, Autokeras - а вы что, за меня и модели строить будете?"
        ]
    },
    {
        "Название статьи": "Как подружить Llama-3 и YouTube имея всего 40 строк кода",
        "Дата публикации": "2024-06-03, 21:57",
        "Автор статьи": "evg_dc ",
        "Статья целиком": [
            "Сделаем Телеграм бота которому можно кинуть ссылку на YouTube видео и поговорить с ним о содержимом этого видео.",
            "За основу возьмем бота работающего на Llama 3-70b из моей прошлой статьи. Можно использовать и любую другую языковую модель включая ChatGPT или локальную запущенную на Ollama.",
            "Создать Телеграм бота и получить его токен (как это сделать, смотрите инструкции на просторах интернета, например здесь).",
            "Зарегистрироваться на Groq и получить api key (нужен VPN).Заходим по этой ссылке, регистрируем аккаунт, генерим ключ. Платежная карта не требуется.",
            "Замените в коде GROQ_API_KEY на api ключ полученный в Groq и TELEGRAM_BOT_TOKEN на токен телеграм бота, все должно быть в кавычках.",
            "После получения сообщения от пользователя ищем в тексте сообщения ссылку на YouTube видео. Делаем это перебирая все слова и проверяя их на наличие URL. Если ссылка на видео найдена, используя библиотеку youtube-transcript-api забираем транскрипцию.",
            "Далее, полученную транскрипцию подставляем языковой модели в виде сообщения от функции. Здесь мы немного обманываем модель, потому что такой функции нет, но лучше делать так чем ставить транскрипцию в системное сообщение. Модель заточена под работу с функциями и все правильно поймет.",
            "Как и в предыдущей версии, бот будет запоминать последние 6 сообщений и поддерживать диалог.",
            "Запускаем скрипт и в Телеграм боте задаем вопрос со ссылкой на видео:",
            "Посмотреть как это работает можно в YouTubeGPT.",
            "Еще есть НашGPT - это как ChatGPT только модель Llama 3-70b."
        ]
    },
    {
        "Название статьи": "Python в Excel жив?",
        "Дата публикации": "2024-06-03, 17:24",
        "Автор статьи": "Gonchar_POTT ",
        "Статья целиком": [
            "Уже больше месяца экспериментирую, исследую, как разные схемы (паттерны) осознанного дыхания влияют на вариабельность сердечного ритма (Heart Rate Variability, HRV на чуждом языке). В скромной, но не совсем уж крошечной Excel-таблице со столбцами “Паттерн”, “HRV”, “Пульс” набралось 258 записей и мне понадобилось выбрать победителя -- дыхательный паттерн, дающий на выходе максимальное значение HRV. Не вручную же сортировать эти записи!",
            "Да, я знаю: есть Pivot Table & Power Query. Но Pivot Table мне не по душе необходимостью после каждого изменения таблицы-источника делать REFRESH, во-первых, избыточной сложностью создания, во-вторых. И просто так не нравятся Pivot Table, что главное. Что же касается Power Query, то сочетание слов вызывает у меня трепет и учащенное сердцебиение: не понимаю, что это за зверь такой и насколько он страшный или полезный.",
            "Поэтому для подсчета результатов -- и выбора победителя -- применил относительно недавно появившуюся в Excel функцию GROUPBY в связке с XLOOKUP. И, раз уж пришлось группировать данные, решил сравнить нативные функции Excel с GROUPBY от Pandas (мы ведь помним, что сейчас Python можно запустить внутри Excel).",
            "Написал простой код:",
            "Поместил код через =PY( в ячейку А1 Excel",
            "И он прекрасно справился с задачей и выдал таблицу с результатами:",
            "breathing_pattern",
            "HRV",
            "HR",
            "8",
            "physiological sighs moderate",
            "59",
            "65",
            "7",
            "physiological sighs light",
            "57",
            "62",
            "1",
            "4.4-6.6",
            "56",
            "59",
            "6",
            "following pulse",
            "55",
            "61",
            "0",
            "4.2-0-6.4-0",
            "53",
            "62",
            "3",
            "6-6",
            "53",
            "61",
            "4",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "61",
            "2",
            "5-5",
            "52",
            "63",
            "5",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "Комбинация функций GROUPBY и XLOOKUP тоже отработала без изъянов:",
            "breathing_pattern",
            "HRV",
            "HR",
            "physiological sighs moderate",
            "59",
            "65",
            "physiological sighs light",
            "57",
            "63",
            "4.4-6.6",
            "56",
            "60",
            "following pulse",
            "55",
            "61",
            "4.2-0-6.4-0",
            "54",
            "63",
            "6-6",
            "54",
            "61",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "62",
            "5-5",
            "53",
            "63",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "* Для внимательных: разница в данных между двумя таблицами -- плод Python-овского округления до целых чисел.",
            "* В “нативном” подходе нет отсечки паттернов с количеством замеров менее шести.",
            "Выводы и наблюдения по теме:",
            "В Python итоговая таблица сама автоматически изменяет размеры при добавлении новых паттернов. GROUPBY by Excel ведет себя так же, а вот связка GROUPBY&XLOOKUP уже потребует редактирования формул: нужно изменять адреса диапазонов ячеек, к которым обращается XLOOKUP.",
            "Вопреки большему размеру Python-код мне кажется проще и для написания, и для чтения-понимания. Хотя писать код в ячейке Excel -- весьма извращенное удовольствие.",
            "Исполнение кода Python требует интернет-соединения.",
            "Выводы и наблюдения не совсем по теме:",
            "Для меня лично схема дыхания “physiological sighs light” (легкие физиогические вздохи) -- оптимальный выбор.",
            "Более шести месяцев я придумывал, зачем мне может понадобиться живущий в Excel Python и наконец нашел.",
            "Буду благодарен за советы и критику. Постараюсь ответить на вопросы."
        ]
    },
    {
        "Название статьи": "Майним крипто-коины с помощью Python и компьютерного зрения",
        "Дата публикации": "2024-06-03, 13:58",
        "Автор статьи": "temabed ",
        "Статья целиком": [
            "После внезапного обогащения энтузиастов, которые поиграли в начале года в приложение Notcoin в телеграм, подобные проекты стали расти как грибы. Да и грибников заметно поприбавилось. Но в данной статье мы не будем касаться тем блокчейна или финансов, а рассмотрим простой пример применения компьютерного зрения для фарма поинтов в самом популярном, после Notcoin, проекте - хомяке комбате. Название явно на что-то намекает, но да ладно.",
            "Это не первый проект, который я автоматизирую, и не самый нуждающийся в этом. Да и без компьютерного зрения с автоматизацией хомяка можно спокойно обойтись. Но с ним, во-первых, интереснее, а во-вторых - это просто хороший пример с минимумом строк кода для демонстрации возможностей библиотеки cv2. Статья, соответственно, предназначена для энтузиастов и начинающих специалистов.",
            "Начнем с того, что мы установим все необходимые зависимости и импортируем их в свой проект. Вот они, слева направо.",
            "С помощью pyautogui наш бот будет управлять мышью. Keyboard пригодится для назначения горячих клавиш, чтобы управлять работой бота. cv2 наградит бота зрением, пусть и компьютерным, с помощью которого тот будет находить совпадения с искомым изображением. А numpy пригодится для работы с большими массивами, но тут он почти для галочки, не бойтесь. Модуль time тоже понадобится, чтобы ставить таймауты в работе программы.",
            "Далее напишем небольшую конструкцию- переключатель.",
            "Функция change при вызове всего лишь меняет значение переменной work, которая будет использована в бесконечном цикле. И если work будет False, работа нашего кода будет останавливаться. И наоборот запускаться, в противоположном случае.",
            "Кстати, забыл упомянуть, что разработчики проекта, над которым мы сейчас проводим эксперимент, большие молодцы, и убрали возможность пользоваться приложением на десктопных устройствах. Поэтому для его запуска понадобится эмулятор Android.",
            "Теперь определим основную логику работы бота:",
            "Он ищет совпадение с изображением полностью заполненной энергии.",
            "Если находит совпадение, ищет изображение монеты и кликает на неё энное количество раз.",
            "И всё это работает в бесконечном цикле.",
            "Значит со скриншота приложения необходимо вырезать две области, которые помечены красным, и разместить их в отдельные файлы, конечно же.",
            "Теперь напишем функцию для кликов по монетке. Она будет принимать путь к исходному изображению, а так же порог чувствительности для компьютерного зрения и интервал (таймаут) в секундах.",
            "Переменной template будет присвоено исходное изображение монетки, но в оттенках серого, так как мы указали в параметрах 0. Это необходимость, так как в оттенках серого компьютер зрит лучше. Сразу вычисляем высоту и ширину исходника, и присваиваем переменным. А далее по ходу исполнения кода он делает скриншот, сравнивает с исходником, получает координаты области с совпадением, и делает 260 даблкликов по ней. Координаты я ищу немного кривовато и в итоге loc содержит большой массив, из которого я использую лишь самые первые координаты, после чего цикл прерываю. Но лучше сделать не смог, извините.",
            "А теперь напишем аналогичную функцию, но с задачей искать совпадение с картинкой полной энергии, после чего вызывать функцию click.",
            "В целом всё аналогично. Добавил лишь ожидание горячей клавиши, чтобы можно было остановить программу в любое время нажатием на тильду (Ё). Ну и для красоты заключил в try-except.",
            "И это всё. Пишем последние строки и запускаем скрипт (не забыв нажать на Ё для запуска логики в цикле).",
            "По сути, этот код многофункционален, и его без труда, с минимальными изменениями, можно переделать под любые другие задачи. На всякий случай оставлю и полную версию кода:",
            "Не судите меня строго по этому скрипту, большую часть жизни я вообще бегал с пистолетиком, и пристрастился к разработке сравнительно недавно, поэтому всего лишь юный падаван в возрасте. Хотя могу писать и большие скучные штуки, но вот писать о них получится дольше, чем сам код. А если будут вопросы- добро пожаловать в телеграм. У меня там небольшой клуб по интересам."
        ]
    },
    {
        "Название статьи": "Сравниваем популярные алгоритмы кластеризации DBSCAN и OPTICS",
        "Дата публикации": "2024-06-03, 13:51",
        "Автор статьи": "evaclick ",
        "Статья целиком": [
            "Привет, Хабр)",
            "Поговорим сегодня о 2 популярных алгоритмах кластеризации — DBSCAN и OPTICS, посмотрим их особенности и сравним",
            "Поехали!",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Ну, DBSCAN в особом в представлении не нуждается, всё-таки один из самых популярных алгоритмов кластеризации. Поэтому по минимуму теории.",
            "DBSCAN (Density-based spatial clustering of applications with noise, плотностной алгоритм пространственной кластеризации с присутствием шума), как следует из названия, оперирует плотностью данных. На вход он просит матрицу близости точек и два параметра — радиус -окрестности и количество соседей .",
            "Эпсилон-окрестность для любого вектора в метрическом признаковом пространстве определяется как множество точек, отстоящих от не более чем на :",
            "где — выбранная метрика (например, евклидовое расстояние).",
            "В общих чертах, алгоритм DBSCAN можно представить как последовательность следующих этапов:",
            "найти новые точки в -окрестности каждой точки и определить основные точки с более чем соседями.",
            "найти связные компоненты основных точек на графе соседей, игнорируя все неосновные точки.",
            "назначить каждую неосновную точку ближайшему кластеру, если кластер является -соседним, в противном случае считаем точку шумом.",
            "Вот так можно использовать DBSCAN из Sci-Kit Learn + с интерактивными ползунками, работает в Colab'е (в Jupyter Notebook какие-то траблы с этим, если кто знает — please, help):",
            "С использованием DBSCAN в Julia и R особых проблем тоже не возникает —",
            "— Julia:",
            "— R:",
            "В идеальном случае DBSCAN может иметь линейную сложность , но не стоит особо на это рассчитывать. Если не пересчитывать каждый раз точек, то ожидаемая сложность — . Худший случай (плохие данные или брутфорс-реализация) — . Наивные реализации DBSCAN любят отъедать памяти под матрицу расстояний — это явно избыточно. Многие версии DBSCAN умеют работать и с более щадящими структурами данных: sklearn и R реализации можно оптимизировать при помощи KD-tree прямо из коробки.",
            "DBSCAN не вычисляет самостоятельно центры кластеров, однако вряд ли это проблема, особенно учитывая произвольную форму кластеров. Зато DBSCAN автоматически определяет выбросы, что довольно здорово.",
            "Соотношение , где — размерность пространства, можно интуитивно рассматривать как пороговую плотность точек данных в области пространства. Ожидаемо, что при одинаковом соотношении , и результаты будут примерно одинаковы. Иногда это действительно так, но есть причина, почему алгоритму нужно задать два параметра, а не один. Во-первых типичное расстояние между точками в разных датасетах разное — явно задавать радиус приходится всегда. Во-вторых, играют роль неоднородности датасета. Чем больше и , тем больше алгоритм склонен «прощать» вариации плотности в кластерах. С одной стороны, это может быть полезно: неприятно увидеть в кластере «дырки», где просто не хватило данных. С другой стороны, это вредно, когда между кластерами нет чёткой границы или шум создаёт «мост» между скоплениями. Тогда DBSCAN запросто соединит две разные группы. В балансе этих параметров и кроется сложность применения DBSCAN: реальные наборы данных содержат кластеры разной плотности с границами разной степени размытости. В условиях, когда плотность некоторых границ между кластерами больше или равна плотности каких-то обособленных кластеров, приходится чем-то жертвовать.",
            "Существуют варианты DBSCAN, способные смягчить эту проблему. Идея состоит в подстраивании в разных областях по ходу работы алгоритма. К сожалению, возрастает количество параметров алгоритма.",
            "Ок, теперь давайте немного поговорим о плюсах и минусах DBSCAN.",
            "Плюсы DBSCAN",
            "• DBSCAN не требует указания числа кластеров в отличие, скажем, от метода k-средних",
            "• DBSCAN может найти кластеры произвольной формы. Он может найти даже кластеры полностью окружённые (но не связанные с) другими кластерами.",
            "• DBSCAN имеет понятие шума и устойчив к выбросам.",
            "• DBSCAN требует лишь двух параметров ( и ) и большей частью нечувствителен к порядку точек в датасете. Однако, точки, находящиеся на границе двух различных кластеров могут оказаться в другом кластере, если изменить порядок точек, а назначение кластеров единственно с точностью до изоморфизма.",
            "Проблемы DBSCAN",
            "• DBSCAN не полностью однозначен — краевые точки, которые могут быть достигнуты из более чем одного кластера, могут принадлежать любому из этих кластеров, что зависит от порядка просмотра точек (тут стоит сказать, что существует DBSCAN❋, который трактует краевые точки как шум и тем самым достигается полностью однозначный результат)",
            "• Качество DBSCAN зависит от способа измерения расстояния. Наиболее часто используемой метрикой расстояний является евклидова метрика. В случае кластеризации данных высокой размерности эта метрика может оказаться почти бесполезной, что делает трудным делом нахождение подходящего значения . Этот эффект, однако, присутствует в любом другом алгоритме, основанном на евклидовом расстоянии.",
            "• DBSCAN не может хорошо разделить на кластеры наборы данных с большой разницей в плотности, поскольку не удается выбрать приемлемую для всех кластеров комбинацию и .",
            "Что ж, теперь давайте теперь переключимся на алгоритм OPTICS (Ordering Points To Identify the Clustering Structure).",
            "Основная идея OPTICS похожа на DBSCAN, но алгоритм предназначен для избавления от одной из главных слабостей алгоритма DBSCAN — проблемы обнаружения кластеров в данных, имеющих различные плотности. Для этого используется граф достижимости, который определяет достижимое расстояние для каждой точки, которая в дальнейшем будет относиться к ближайшему кластеру. Такой подход позволяет ещё лучше определять кластеры разной плотности, особенно если они расположены близко друг к другу, однако это увеличивает время работы алгоритма.",
            "Реализация OPTICS есть в библиотеке Sci-Kit Learn; вот как можно её импортировать и использовать:",
            "С R тоже проблем нет:",
            "Хорошо, давайте немного об особенностях OPTICS",
            "Плюсы OPTICS:",
            "Устойчивость к шуму (впрочем как и у DBSCAN): OPTICS способен обрабатывать данные с шумом и выбросами.",
            "Способность обнаруживать кластеры любой формы",
            "Не требует заранее заданного числа кластеров",
            "Проблемы OPTICS:",
            "Не всегда эффективен для плотных кластеров: OPTICS может иметь проблемы с эффективным обнаружением плотных кластеров, особенно если они имеют сложные формы.",
            "А вот несколько сфер, где регулярно используется OPTICS:",
            "Анализ сетей и обнаружение аномалий: OPTICS используется для анализа социальных сетей, транспортных сетей и других сетевых структур для выявления кластеров и аномалий.",
            "Биоинформатика: OPTICS применяется в биоинформатике для кластеризации геномных данных, выявления генных паттернов и классификации биологических образцов.",
            "Медицинская диагностика: OPTICS может быть применен для кластеризации медицинских данных, таких как результаты тестов, симптомы пациентов и история заболеваний, с целью выявления паттернов заболеваний или групп пациентов схожего профиля. .",
            "Итак, пришло время сравнить DBSCAN и OPTICS",
            "Вот DBSCAN:",
            "...а вот и OPTICS:",
            "И давайте возьмём для начала , , потом поменяем.",
            "Что мы видим? Для данного датасета DBSCAN выделяет кластеры более логичным и понятным способов, но в кластеризации OPTICS тоже есть пара интересных моментов. Как можно увидеть, точки вокруг главных кластеров DBSCAN безнадёжно отмечает как шум, в то время как OPTICS пытается нащупать кластеры и среди этих точек тоже. Это одна из главных фишек OPTICS — метод способен видеть кластеры разной плотности одновременно за счёт того, что он менее чувствителен к параметру .",
            "Вот довольно показательный пример — и тут OPTICS тоже выделил кластер в точках, которые забраковал DBSCAN:",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Описание алгоритма DBSCAN от Sci-Kit Learn",
            "Описание алгоритма OPTICS от Sci-Kit Learn",
            "Наглядная визуализация DBSCAN",
            "Что ж, надеюсь, статья была полезной)",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)"
        ]
    },
    {
        "Название статьи": "Реализация принципа единственной ответственности на Python",
        "Дата публикации": "2024-06-03, 07:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Сегодня мы рассмотрим одну из основополагающих концепций SOLID-принципов — принцип единственной ответственности или сокращенно - SRP. Разберем, что такое SRP и как правильно его применять в Python.",
            "Принцип единственной ответственности гласит, что каждый класс, метод или модуль должен иметь только одну причину для изменения. Проще говоря, каждый компонент вашей системы должен отвечать только за одну функциональность. Т.е если вам нужно внести изменение, связанное с этой функциональностью, вам придется изменить только один компонент.",
            "Когда каждый класс или модуль выполняет одну четко определенную задачу, становится гораздо проще понять его назначение и взаимодействие с другими частями системы.",
            "Что будет, если не соблюдать SRP?",
            "Если класс или модуль берет на себя несколько обязанностей, это приводит к увеличению сложности кода. Такой код сложнее читать, понимать и поддерживать. Также, когда один класс выполняет несколько задач, изменение в одной из них может непредсказуемо повлиять на другие.",
            "Классы, которые нарушают SRP, обычно плохо масштабируются и трудно переиспользуются. Их невозможно легко адаптировать для других целей или проектов.",
            "Для начала рассмотрим класс, который нарушает принцип единственной ответственности. Представим себе класс UserManager, который одновременно отвечает за создание юзера, валидацию данных и сохранение юзера в БД:",
            "Класс нарушает SRP, т.к выполняет несколько задач: валидацию email, создание пользователя и сохранение его в базу данных.",
            "Для исправления нарушения SRP нужно разделить обязанности на отдельные классы: User, UserValidator, UserDatabase, и UserCreator. Каждый класс будет отвечать только за одну задачу:",
            "Теперь каждый класс отвечает за одну конкретную задачу, что соответствует принципу единственной ответственности.",
            "Рассмотрим другой пример, обработку заказов в интернет-магазине. Изначально есть класс, который нарушает SRP, т.к он одновременно обрабатывает заказ, валидирует данные и отправляет уведомления:",
            "Рефакторинг этого класса для соответствия SRP:",
            "Фасадный паттерн помогает упростить взаимодействие между сложными подсистемами, предоставляя простой интерфейс для клиента. С фасадом можно скрыть сложность подсистем и предоставлять единый интерфейс для взаимодействия с ними.",
            "Предположим, есть система обработки заказов, включающая несколько классов для управления заказами, оплатами и уведомлениями. Без фасадного паттерна клиенту пришлось бы взаимодействовать с каждым из этих классов напрямую:",
            "А с использованием фасадного паттерна все будет выглядеть так:",
            "Интерфейсы и абстрактные классы помогают разделить обязанности и четко определить контракт, который должен реализовать класс.",
            "Создание интерфейсов для валидации, сохранения и уведомления:",
            "Разделяем обязанности на интерфейсы, что позволяет каждому классу реализовывать только свои специфические методы, соответствующие SRP.",
            "Для поддержки SRP и других принципов SOLID в Python можно использовать различные библиотеки.",
            "Pylint помогает анализировать код на наличие ошибок и несоответствий стилю, а также выявляет нарушения принципов SOLID, включая SRP.",
            "Mypy - статический анализатор типов для Python, который помогает обнаруживать типовые ошибки и улучшать структуру кода.",
            "Pytest помогает создавать модульные тесты для каждого отдельного компонента.",
            "Dataclasses модуль позволяет создавать классы данных, которые следуют SRP, отделяя логику данных от поведения.",
            "Про другие архитектурные принципы и инструменты коллеги из OTUS рассказывают в рамках практических онлайн-курсов. Также хочу напомнить о том, что в календаре мероприятий вы можете зарегистрироваться на ряд интересных и абсолютно бесплатных вебинаров."
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 12: Дата и время (издание 2024)",
        "Дата публикации": "2024-06-02, 19:47",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это двенадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как работать с датами и временем таким образом, чтобы это работало для всех ваших пользователей, независимо от того, где они проживают.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения",
            "Глава 12: Дата и время (Эта статья)",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Один из аспектов моего приложения для ведения микроблогов, который я долгое время игнорировал, - это отображение дат и времени. До сих пор я просто позволял Python отображать объект datetime в модели User и даже не потрудился отобразить его в модели Post. В этой главе вы узнаете, как работать с этими временными метками.",
            "Ссылки на GitHub для этой главы: Browse, Zip, Diff.",
            "Использование Python на сервере для отображения дат и времени, которые отображаются пользователям в их веб-браузерах, на самом деле не очень хорошая идея, потому что то, что сервер считает своим местным временем, не будет иметь смысла для пользователей, которые живут в другом часовом поясе.",
            "Совершенно ясно, что сервер должен управлять временем, которое является согласованным и независимым от его собственного местоположения и местоположения пользователей. Если это приложение разрастется до такой степени, что потребуется несколько производственных серверов в разных регионах мира, я бы не хотел, чтобы каждый сервер записывал временные метки в базу данных в разных часовых поясах, потому что это сделало бы невозможной работу с этими временами. Поскольку UTC является наиболее используемым единым часовым поясом и поддерживается в классе datetime, именно его я и собираюсь использовать.",
            "В главе 4 вы видели, как создавать временные метки UTC для записей в блоге. В качестве напоминания, вот краткий пример, показывающий, как это было сделано.:",
            "Но с этим подходом связана важная проблема. Пользователям из разных мест будет ужасно сложно определить, когда была сделана публикация, если они будут видеть время в часовом поясе UTC. Им нужно было бы заранее знать, что время указано в UTC, чтобы они могли мысленно подогнать его к своему собственному часовому поясу. Представьте пользователя, скажем, в часовом поясе PDT на Западном побережье США, который публикует что-то в 15: 00 и сразу видит, что сообщение появляется в 10: 00 по времени UTC, или, если быть более точным, в 22: 00. Это будет очень запутанно.",
            "Хотя стандартизация временных меток в соответствии с UTC имеет большой смысл с точки зрения сервера, это создает проблему удобства использования для пользователей. Цель этой главы - представить решение, которое сохраняет все временные метки, управляемые сервером, в часовом поясе UTC, не отталкивая пользователей.",
            "Очевидным решением проблемы является преобразование всех временных меток из сохраненных единиц UTC в местное время каждого пользователя при их отображении. Это позволяет серверу продолжать использовать UTC для обеспечения согласованности, в то время как преобразование \"на лету\", адаптированное к каждому пользователю, решает проблему удобства использования. Сложная часть этого решения - знать местоположение каждого пользователя.",
            "На многих веб-сайтах есть страница конфигурации, где пользователи могут указывать свой часовой пояс. Для этого мне потребуется добавить новую страницу с формой, в которой я представляю пользователям раскрывающийся список часовых поясов. При первом входе на сайт пользователей могут попросить ввести их часовой пояс в рамках регистрации.",
            "Хотя это достойное решение, решающее проблему, немного странно просить пользователей вводить часть информации, которую они уже настроили в своей операционной системе. Кажется, было бы эффективнее, если бы я мог просто получить настройки часового пояса с их компьютеров.",
            "Как выясняется, веб-браузер знает часовой пояс пользователя и предоставляет его через стандартные API JavaScript даты и времени. На самом деле есть два способа воспользоваться информацией о часовом поясе, доступной через JavaScript:",
            "Подход \"старой школы\" заключался бы в том, чтобы веб-браузер каким-то образом отправлял информацию о часовом поясе на сервер, когда пользователь впервые входит в приложение. Это можно было бы сделать с помощью вызова Ajax или гораздо проще с помощью мета-тега обновления. Как только сервер узнает часовой пояс, он может сохранить его в сеансе пользователя или записать в таблицу users в базе данных, и с этого момента корректировать с его помощью все временные метки во время отображения шаблонов.",
            "Подход \"новой школы\" заключается в том, чтобы ничего не менять на сервере и позволить преобразованию UTC в местный часовой пояс происходить в браузере с использованием JavaScript.",
            "Оба варианта допустимы, но второй имеет большое преимущество. Знания часового пояса пользователя не всегда достаточно для представления дат и времени в формате, ожидаемом пользователем. Браузер также имеет доступ к конфигурации языкового стандарта системы, которая определяет такие параметры, как время утра / вечера в сравнении с 24-часовыми часами, формат отображения даты DD / MM / ГГГГ в сравнении с MM / DD / ГГГГ и многие другие культурные или региональные стили.",
            "И если этого недостаточно, у подхода новой школы есть еще одно преимущество. Есть библиотека с открытым исходным кодом, которая выполняет всю эту работу!",
            "Moment.js это небольшая библиотека JavaScript с открытым исходным кодом, которая выводит отображение даты и времени на новый уровень, поскольку предоставляет все мыслимые варианты форматирования, а затем и некоторые другие. Некоторое время назад я создал Flask-Moment, небольшое расширение Flask, которое позволяет очень легко интегрировать moment.js в ваше приложение.",
            "Итак, давайте начнем с установки Flask-Moment:",
            "Это расширение добавляется в приложение Flask обычным способом:",
            "app/__init__.py: Пример Flask-Moment.",
            "В отличие от других расширений, Flask-Moment работает вместе с moment.js, поэтому все шаблоны приложения должны включать эту библиотеку. Чтобы гарантировать, что эта библиотека всегда доступна, я собираюсь добавить ее в базовый шаблон. Это можно сделать двумя способами. Самый прямой способ - явно добавить тег <script>, который импортирует библиотеку, но Flask-Moment упрощает задачу, предоставляя функцию moment.include_moment(), которая генерирует тег <script>:",
            "app/templates/base.html: Включить moment.js в базовый шаблон.",
            "В большинстве случаев библиотеки JavaScript, используемые приложением, включены в конец содержимого <body>, где находится загрузочный JavaScript-код.",
            "Moment.js делает класс moment доступным для браузера. Первым шагом для отображения временной метки является создание объекта этого класса, передающего желаемую временную метку в формате ISO 8601. Вот пример, запущенный в консоли JavaScript браузера:",
            "Если вы не знакомы со стандартным форматом даты и времени ISO 8601, этот формат выглядит следующим образом:",
            "Я уже решил, что буду работать только с часовыми поясами UTC, поэтому последней частью всегда будет +00:00 или в некоторых случаях эквивалент Z, который представляет UTC в стандарте ISO 8601.",
            "В объекте moment предусмотрено несколько методов для различных вариантов рендеринга. Ниже приведены некоторые из наиболее распространенных вариантов:",
            "В этом примере создается объект moment, инициализированный 28 июня 2021 года в 21:45 по Гринвичу. Вы можете видеть, что все параметры, которые я пробовал выше, отображаются в UTC + 1, который является часовым поясом, настроенным на моем компьютере. Вы можете ввести вышеуказанные команды в консоли вашего браузера, убедившись, что в странице, на которой вы открываете консоль, включена moment.js. Вы можете сделать это в микроблоге, при условии, что вы внесли вышеуказанные изменения для включения moment.js, или также на https://momentjs.com/.",
            "Обратите внимание, как разные методы создают разные представления. С помощью метода format() вы управляете форматом выходных данных с помощью строки формата. Метод fromNow() интересен тем, что он отображает временную метку по отношению к текущему времени, поэтому вы получаете выходные данные, такие как \"минуту назад\" или \"через два часа\" и т.д.",
            "Если вы работали непосредственно в JavaScript, приведенные выше вызовы возвращают строку с отображаемой временной меткой. Затем вам предстоит вставить этот текст в нужное место на странице, что, к сожалению, требует работы с DOM. Расширение Flask-Moment значительно упрощает использование moment.js за счет включения в ваши шаблоны объекта moment, аналогичного объекту JavaScript.",
            "Давайте посмотрим на временную метку, которая отображается на странице профиля. Текущий шаблон user.html позволяет Python генерировать строковое представление времени. Теперь я могу отобразить эту временную метку с помощью Flask-Moment следующим образом:",
            "app/templates/user.html: Отрисовка временной метки с помощью moment.js.",
            "Итак, как вы можете видеть, Flask-Moment использует синтаксис, аналогичный синтаксису библиотеки JavaScript, с одним отличием, заключающимся в том, что аргументом для moment() теперь является объект Python datetime, а не строка ISO 8601. Вызов moment(), выполняемый из шаблона, автоматически генерирует необходимый код JavaScript для вставки отображаемой временной метки в нужное место DOM.",
            "Второе место, где я могу воспользоваться преимуществами Flask-Moment, находится во вложенном шаблоне _post.html, который вызывается с главной страницы и страницы пользователя. В текущей версии шаблона каждому сообщению предшествует строка \"username says:\". Теперь я могу добавить временную метку, отображаемую с помощью fromNow():",
            "app/templates/_post.html: Отрисовка временной метки во вложенном шаблоне post.",
            "Ниже вы можете увидеть, как выглядят обе эти временные метки при рендеринге с помощью Flask-Moment и moment.js:"
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 11: Дизайн приложения (издание 2024)",
        "Дата публикации": "2024-06-02, 19:46",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это одиннадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как заменить базовые HTML-шаблоны новым набором, основанным на платформе пользовательского интерфейса Bootstrap.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения (Эта статья)",
            "Глава 12: Даты и время",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Вы уже некоторое время играете с моим приложением для ведения микроблогов, поэтому, я уверен, вы заметили, что я не потратил слишком много времени на то, чтобы оно выглядело хорошо, или, лучше сказать, я вообще не тратил на это времени. Шаблоны, которые я собрал, довольно простые, без какого-либо пользовательского оформления. Мне было полезно сосредоточиться на реальной логике приложения, не отвлекаясь на написание красивых HTML и CSS.",
            "Но я уже долго сосредоточен на серверной части этого приложения. Итак, в этой главе я сделаю перерыв и потрачу некоторое время на то, чтобы показать вам, что можно сделать, чтобы приложение выглядело немного более отточенным и профессиональным.",
            "Эта глава будет немного отличаться от предыдущих, потому что я не собираюсь так подробно, как обычно, описывать сторону Python, которая, в конце концов, является основной темой этого туториала. Создание красивых веб-страниц - обширная тема, которая в значительной степени не связана с веб-разработкой на Python, но я расскажу о некоторых основных рекомендациях и идеях о том, как подойти к этой задаче, и у вас также будет приложение с измененным дизайном, которое можно изучить и перенять опыт.",
            "Ссылки на GitHub для этой главы: Обзор, Zip, Diff.",
            "Хотя мы можем утверждать, что программирование - это сложно, наши усилия ничто по сравнению с усилиями веб-дизайнеров, которым приходится создавать веб-страницы, которые красиво и единообразно выглядят в списке веб-браузеров. За последние годы они стали лучше, но в некоторых браузерах все еще есть непонятные ошибки или причуды, которые сильно усложняют задачу создания веб-страниц, которые везде выглядят красиво. Это еще сложнее, если вам также нужно настроить для браузеров планшетов и смартфонов с ограниченным количеством ресурсов и экранов.",
            "Если вы, как и я, разработчик, который просто хочет создавать прилично выглядящие веб-страницы, но у вас нет времени или интереса изучать низкоуровневые механизмы для эффективного достижения этой цели путем написания необработанного HTML и CSS, то единственным практическим решением является использование CSS фреймворка для упрощения задачи. Выбрав этот путь, вы потеряете некоторую творческую свободу, но, с другой стороны, ваши веб-страницы будут хорошо выглядеть во всех браузерах без особых усилий. Фреймворк CSS предоставляет коллекцию высокоуровневых классов CSS с готовыми стилями для распространенных типов элементов пользовательского интерфейса. Большинство этих фреймворков также предоставляют дополнения JavaScript для вещей, которые нельзя выполнить строго с помощью HTML и CSS.",
            "Одним из самых популярных CSS-фреймворков является Bootstrap. Если вы хотите увидеть, какие страницы можно создавать с помощью этого фреймворка, в документации есть несколько примеров.",
            "Вот некоторые преимущества, которые вы получаете при использовании Bootstrap для оформления ваших веб-страниц:",
            "Аналогично выглядит во всех основных веб-браузерах",
            "Настройка размеров для экранов настольных компьютеров, планшетов и телефонов",
            "Настраиваемые макеты",
            "Красиво оформленные панели навигации, формы, кнопки, оповещения, всплывающие окна и т.д.",
            "Самый простой способ использовать Bootstrap - это просто импортировать файл bootstrap.min.css в ваш базовый шаблон. Вы можете либо загрузить копию этого файла и добавить его в свой проект, либо импортировать его непосредственно из CDN. Затем вы можете начать использовать CSS-классы общего назначения, которые он предоставляет, согласно документации, что довольно неплохо. Возможно, вы также захотите импортировать JavaScript-код фреймворка, чтобы использовать самые продвинутые функции.",
            "Как и большинство проектов с открытым исходным кодом, Bootstrap постоянно развивается. Оригинальная версия мега-учебника Flask была создана для Bootstrap 3. Редакция, которую вы сейчас читаете, создана для Bootstrap 5.3. Текущий подход к интеграции Bootstrap является довольно общим и может быть адаптирован к более новым версиям Bootstrap.",
            "Первым шагом в интеграции Bootstrap с Microblog является добавление его файлов CSS и JavaScript в базовый шаблон. На странице быстрого запуска Bootstrap в качестве примера приведена короткая, но полная HTML-страница, которую я копирую ниже для вашего удобства:",
            "Подход, который я могу применить, чтобы объединить это с моим шаблоном base.html, заключается в том, чтобы использовать приведенный выше в качестве нового базового шаблона, заменив теги <title> и <h1> заголовком и основным содержимым исходного базового шаблона соответственно.",
            "Следующий шаг - заменить базовую панель навигации на более удобную из Bootstrap. На странице документации по панели навигации Bootstrap вверху показан хороший пример. Используя этот пример в качестве руководства, я создал панель навигации со ссылками \"Index\", \"Explore\", \"Profile\", \"Login\" и \"Logout\" из микроблога. Для удобства я настроил профиль, а также ссылки для входа и выхода так, чтобы они отображались в крайнем правом углу.",
            "При использовании Bootstrap полезно знать о некоторых базовых примитивах компоновки. Одним из наиболее важных является контейнер, который определяет область содержимого страницы. Два основных контейнера называются container и container-fluid. В первом случае страница настраивается на использование одной из пяти предопределенных ширин страницы и центрирует содержимое в окне браузера. С другой стороны обтекающий контейнер дает вам доступ ко всей ширине страницы. Для этого приложения я решил использовать контейнер по умолчанию, потому что он предотвращает слишком широкое расширение страницы независимо от размера экрана, поэтому часть содержимого страницы будет заключена в один из этих контейнеров следующим образом:",
            "Последняя часть HTML-разметки в шаблоне base.html, которую необходимо адаптировать, - это раздел, отображающий отображаемые сообщения. Компонент Alert от Bootstrap прекрасно подходит для этой задачи.",
            "Вы можете получить полностью переработанный шаблон base.html из репозитория Github для этой главы. Ниже вы можете увидеть упрощенную структуру, если хотите иметь представление о том, как она выглядит.:",
            "app/templates/base.html: Переработанный базовый шаблон.",
            "Благодаря обновленному базовому шаблону внешний вид приложения уже заметно улучшен без необходимости изменять строки кода Python. Если вы хотите убедиться в этом сами, загрузите копию base.html из репозитория GitHub по ссылкам, приведенным в начале этой главы.",
            "Область, в которой Bootstrap проделывает фантастическую работу, заключается в рендеринге полей формы, которые выглядят намного приятнее и чище, чем поля по умолчанию, предоставляемые браузером. В документации по Bootstrap также есть раздел о формах. В начале этого раздела приведен пример формы входа в систему, который показывает базовую структуру HTML.",
            "HTML-код, необходимый для каждого поля, довольно длинный. Ниже вы можете увидеть одно из текстовых полей из примера формы в документации:",
            "Но это слишком просто для нужд Microblog, который включает проверку полей и, возможно, потребуется показывать пользователю ошибки проверки. На странице документации есть раздел о проверке на стороне сервера, в котором показано, как оформить поля с сообщением об ошибке. Вот пример.:",
            "К сожалению, о необходимости вводить такое количество шаблонов для каждого поля в каждой форме не может быть и речи. Это заняло бы слишком много времени и чревато ошибками. Одним из решений является использование макросов Jinja, которые позволяют вам определять повторно используемые фрагменты HTML, а затем вызывать их из ваших шаблонов, как если бы они были функциями.",
            "Например, макрос Jinja для текстового поля, подобного показанному выше, будет иметь вид:",
            "Обратите внимание, как используются условные обозначения для выборочного добавления стиля ошибки, если поле содержит одно или несколько сообщений об ошибках.",
            "Поскольку макрос определен в файле с именем bootstrap_wtf.html, который расположен в каталоге templates, он может быть вызван, когда потребуется отобразить поле. Например:",
            "Макрос отображения полей можно расширить, чтобы он также поддерживал отображение флажков, раскрывающихся списков выбора, кнопок отправки и других типов полей. Он также может принимать второй аргумент с логическим значением, указывающим, следует ли автоматически переводить поле в фокус страницы, что должно быть сделано для первого поля формы. Для еще большего удобства можно создать другой макрос для рендеринга всей формы, просто перебрав поля формы и вызвав form_field() макрос для каждого из них.",
            "Полный bootstrap_wtf.html файл доступен в репозитории GitHub, ссылка на который приведена в начале этой главы. Он включает в себя более полную версию макроса form_field(), показанного выше, и второй макрос с именем quick_form(), который принимает объект формы и отображает все его поля с помощью первого макроса.",
            "Как это выглядит, когда реализовано в реальной форме? Ниже вы можете увидеть переработанный шаблон register.html в качестве примера:",
            "app/templates/register.html: Шаблон регистрации пользователя.",
            "Разве это не здорово? Оператор import вверху работает аналогично импорту Python на стороне шаблона. Который добавляет макрос wtf.quick_form(), который в одной строке кода отображает полную форму, включая ошибки проверки, и все оформлено в соответствии с фреймворком Bootstrap.",
            "Еще раз, я не собираюсь показывать вам все изменения, которые я сделал для других форм в приложении, но все эти изменения внесены в шаблоны, которые вы можете загрузить или просмотреть на GitHub.",
            "Логика представления, которая отображает отдельные записи в блоге, была абстрагирована в подшаблон под названием _post.html. Все, что мне нужно сделать с этим шаблоном, это внести некоторые незначительные корректировки, чтобы он хорошо выглядел в Bootstrap.",
            "app/templates/_post.html: Переработанный подшаблон публикации.",
            "Ссылки на страницы - это еще одна область, в которой Bootstrap предоставляет поддержку. Для этого я просто еще раз обратился к документации Bootstrap и адаптировал один из их примеров. Вот как это выглядит на странице index.html:",
            "app/templates/index.html: Переработаны ссылки на страницы.",
            "Обратите внимание, что в этой реализации вместо скрытия следующей или предыдущей ссылки, когда в этом направлении больше нет содержимого, я применяю отключенное состояние, из-за которого ссылка будет отображаться серым цветом.",
            "Я не собираюсь показывать это здесь, но аналогичное изменение необходимо применить к шаблону user.html. Пакет для загрузки этой главы включает эти изменения.",
            "Чтобы внести в ваше приложение эти изменения, пожалуйста, загрузите zip-файл для этой главы и соответствующим образом обновите свои шаблоны.",
            "Ниже вы можете увидеть несколько фотографий до и после, чтобы увидеть трансформацию. Имейте в виду, что это изменение было достигнуто не затрагивая ни одной строки кода приложения!",
            "Следующая глава =>"
        ]
    },
    {
        "Название статьи": "Расширяем возможности Keras с помощью кастомных слоев",
        "Дата публикации": "2024-06-02, 18:07",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Keras предоставляет мощные инструменты для создания сложных нейронных сетей. Однако иногда стандартного набора слоев недостаточно для решения некоторых задач. В таких случаях на помощь приходят кастомные слои.",
            "Кастомные слои позволяют адаптировать архитектуру модели под особенности данных, улучшая тем самым производительность и точность моделек.",
            "Каждый кастомный слой начинается с определения нового класса, наследующего от tf.keras.layers.Layer. В __init__ происходит инициализация слоя, где можно задать параметры, необходимые для работы слоя:",
            "Тут units определяет количество нейронов, а activation указывает функцию активации. super(CustomLayer, self).__init__(**kwargs) вызывает конструктор базового класса Layer.",
            "Метод build вызывается Keras при первом использовании слоя. Его юзают для создания параметров слоя, которые зависят от размера входных данных:",
            "В методе создаются веса kernel и bias. Функция add_weight создает и регистрирует переменные слоя, которые будут обновляться во время тренировки.",
            "Метод call содержит основную логику вычислений слоя. Он принимает входные данные и возвращает выходные:",
            "В этом методе выполняется умножение входных данных на веса и добавление смещения. Если определена функция активации, она применяется к выходным данным.",
            "После определения кастомного слоя его можно использовать в моделях Keras как обычный слой:",
            "Другие полезные методы:",
            "add_weight: Добавляет переменную веса в слой.",
            "compute_output_shape: Возвращает форму выходных данных на основе формы входных.",
            "get_config: Возвращает конфигурацию слоя в виде словаря, что полезно для сериализации.",
            "Dense слой выполняет простую линейную операцию: умножение входного вектора на матрицу весов и добавление смещения, а затем применяется функция активации:",
            "Convolutional слои применяют свертку фильтра к входным данным, что позволяет выделять пространственные особенности:",
            "Recurrent слои используются для обработки последовательных данных. Один из наиболее распространнных типов рекуррентных слоев — это LSTM:",
            "Dropout слой используется для регуляризации модели, предотвращая переобучение путем случайного зануления некоторых нейронов во время тренировки:",
            "BatchNormalization слой нормализует активации предыдущего слоя, улучшая скорость обучения модельки:",
            "Больше практических инструментов и кейсов коллеги из OTUS рассматривают в рамках практических онлайн-курсов. Напомню, что с полным каталогом курсов можно ознакомиться по ссылке."
        ]
    },
    {
        "Название статьи": "Enbeddrus — обучение независящей от языка эмбеддинг-модели",
        "Дата публикации": "2024-06-02, 17:31",
        "Автор статьи": "efreelancer ",
        "Статья целиком": [
            "Приветствую, хабровчане!",
            "Сегодня хочу рассказать вам историю о том, как я обучил простую и компактную независящую от языка (language agnostic) модель-эмбеддер, которая умеет работать с техническими текстами о PHP и способна извлекать схожие эмбеддинги для параллельных текстов на английском и русском языках.",
            "Основная причина, по которой я решил заняться этим проектом, заключается в том, что мои заметки, код и документация, накопленные за более чем десять лет практики, представляют собой солянку текстов о разных технологиях, языках программирования, пометки о настройке серверов Linux и т.д. на русском и английском языках. Поэтому мне захотелось сделать Retrieval-Augmented Generation (RAG) помогалку, которая сможет принимать запросы пользователя (меня) и эффективно находить информацию в столь разношерстой базе данных, независимо от того на каком языке я сделал запрос и на каком языке написана документация.",
            "Для достижения этой цели как-раз и необходима независимая от языка модель-эмбеддер, которая будет одинаково хорошо работать с техническими текстами на русском и английском языках.",
            "Ещё одним важным аспектом было то, чтобы модель потребляла как можно меньше ресурсов и, если возможно, чтобы её можно было преобразовать в формат GGUF.",
            "Но прежде чем приступить к созданию своего собственного велосипеда, я решил поискать готовые решения, ведь подобная идея очевидна и, возможно, уже реализована другими.",
            "Спойлер: идея не нова, и подобных решений уже достаточно много.",
            "Для построения системы, которая может извлекать одинаковые эмбеддинги для схожих текстов на русском и английском языках, существует несколько решений, например...",
            "Ссылки: arxiv:1907.04307 , kaggle, github",
            "Это проект разработан инженерами Google и поддерживает 16 языков.",
            "Свойства: ~110m параметров, принимает на вход 128 токенов текста и извлекает из них 512-мерный эмбеддинг.",
            "Плюс: поддерживает русский язык.",
            "Минусы: модель основана на Tensorflow, а так же что с 2019го года не было обновлений.",
            "Ссылки: arxiv:1710.04087, github",
            "Это одна из первых попыток инженеров FB создать модель которая способна выполнять задачи по извлечению независящих от языка эмбеддингов.",
            "Плюс: поддерживает русский язык.",
            "Минусы: в наличии имеются веса для пар языков, навроде en-ru, en-de и т.д., весов нет на HuggingFace, ну и с 2018го года проект не развивается.",
            "Ссылки: arvix:2205.12654, github, pypi",
            "Ещё одна модель разработана инженерами FB и, как сказано в ридми на GitHub, поддерживает более 200 языков (хотя если пройти по ссылочкам и посчитать то получится 147 языков).",
            "Свойства: ~256m параметров, принимает 1024 токенов на вход и извлекает из них 1024-мерный эмбеддинг.",
            "Плюсы: она основана на PyTorch и имеет логику переключения между языками которая явно перекочевала из NLLB (о которой я кстати рассказывал в публикации \"Перевод на разные языки используя модель NLLB\" у себя в блоге на Дзен).",
            "Минусы: весов нет на HuggingFace, а модель несовместима с llama.cpp поэтому её не получится конвертировать в GGUF, чтобы можно было запускать на слабом железе (или же в паре с ollama).",
            "Ссылки: arXiv:1908.10084, сайт",
            "Модели Sentence-BERT представляют собой модифицированную версию предобученной BERT, специально адаптированную для генерации эмбеддингов предложений, multilingual версия позволяет извлекать эмбеддинги из текста на разных языках, а paraphrase модели позволяют извлекать похожие эмбеддинги парафраз на разных языках.",
            "Вот пару примечательных моделей, обученных разными способами:",
            "paraphrase-multilingual-MiniLM-L12-v2 имеет 118m параметров, принимает 256 токенов на вход и возвращает 384-мерный эмбеддинг.",
            "paraphrase-multilingual-mpnet-base-v2 имеет 278m параметров, принимает на вход 512 токенов и возвращает 768-мерный эмбеддинг.",
            "Обе эти модели обучены на комбинации из датасетов:",
            "SNLI о котором говорится в публикации \"A large annotated corpus for learning natural language inference\" (570k примеров)",
            "Multi-Genre NLI, подробнее в работе \"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\" (433k примера)",
            "Плюсы: поддерживает русский язык, можно конвентировать в GGUF.",
            "Минусы: модели не очень хорошо понимают технический текст (особенно русский технический жаргон), нет версии в формате GGUF, и к числу фатальных недостатков могу отнести, что эти модели обучил не я ;)",
            "Пришёл к выводу, что тему обучения подобных модей-эмбеддеров уже достаточно хорошо изучили и что можно без особых сложностей реализовать мою задумку.",
            "В качестве базовой модели решил взять модель google-bert/bert-base-multilingual-uncased, потому что:",
            "У этой крохи всего 168m параметров, что чуть больше чем у paraphrase-multilingual-MiniLM-L12-v2, но меньше чем у paraphrase-multilingual-mpnet-base-v2;",
            "На вход она принимает 512 токенов, а на выходе возвращает 768-мерный эмбеддинг, столько же у paraphrase-multilingual-mpnet-base-v2;",
            "Модель обучена на датасете wikipedia представляющем из себя Text Corpora, а там, сами понимаете, примеров текста больше, чем SNLI и Multi-Genre NLI вместе взятые;",
            "Модель uncased, то есть обучение происходило на регистронезависимых текстах (сиречь всё переводилось в lowercase).",
            "С моделью определились, теперь перейдём к вопросу выбора датасета...",
            "Изначально я хотел собрать больше датасетов, но, собирая датасет по PHP, я понял, какой это трудоёмкий процесс, и решил уменьшить свои амбиции.",
            "Итак, после поиска в интернете я нашёл только один подходящий датасет: OPUS PHP v1 на 2k примеров, содержащий пары текстов на русском и английском языках, по теме PHP.",
            "Из указанного датасеста я использовал только английский корпус (так как русский корпус был очень низкого качества), далее задействовал инстанс LibreTranslate для перевода английских текстов на русский и очистил данные от аномалий и шума (сценарий dataset_php_build.ipynb). Затем вручную перевёл кривые места с помощью Google и Yandex Translate и экспортировал результат в CSV формат. Данные отсортировал и удалил дубликаты (сценарий dataset_php_undup.py) после чего осталось 1.6k примеров.",
            "В финале попросил ChatGPT сгенерировать 100 примеров пар технического текста о PHP на русском и английском языках для сплита eval, а очищенные данные использовал для сплита train.",
            "Результат выгрузил (сценарий dataset_php_publish.ipynb) на HuggingFace: evilfreelancer/opus-php-en-ru-cleaned .",
            "Для создания эффективного эмбеддера, способного работать с техническими текстами о PHP на русском и английском языке, я решил провести обучение модели в два этапа, сначала выполнить Domain Adaptation, чтобы модель могла работать с техническими текстами на английском языке, а после этого обучить её на Parallel Corpora из русских и английских текстов.",
            "Для Domain Adaptation я использовал метод Generative Pseudo Labeling (GPL) (arXiv:2112.07577), данный метод позволяет проводить обучение модели на основе неразмеченных данных, генерируя псевдометки и улучшая качество работы модели для специфических доменов.",
            "Библиотека gpl имеет захардкоженный формат входного датасета и читает данные по определённым путям, поэтому пришлось слегка конвертировать тренировочный датасет и положить результат в директорию datasets (сценарий: dataset_php_convert.py).",
            "Для адаптации модели bert-base-multilingual-uncased к домену английских текстов про PHP я использовал в качестве шаблона скрипт, предложенный авторами проекта GPL на их странице на GitHub, получился следующего вида код:",
            "Полный скрипт тренировки train_domain.py можно найти в репозитории проекта на GitHub.",
            "Процесс обучения включает в себя несколько этапов:",
            "Используется генератор запросов, такой как BeIR/query-gen-msmarco-t5-base-v1, для создания синтетических запросов на основе текстов из корпуса;",
            "С помощью ретриверов, таких как msmarco-distilbert-base-v3 и msmarco-MiniLM-L-6-v3, которые работают с косинусным сходством, извлекаются наиболее релевантные документы для сгенерированных запросов;",
            "Кросс-энкодер, такой как cross-encoder/ms-marco-MiniLM-L-6-v2, используется для создания псевдометок, присваивая оценочные метки соответствия между запросами и документами;",
            "Модель обучается с использованием MarginMSELoss, которая позволяет модели лучше адаптироваться к новому домену.",
            "И так, наша модель обучена работать с новым доменом, поэтому переходим к следующему шагу.",
            "Для обучения модели на параллельных корпусах я использовал метод обучения моделей на разных языках, описанный в примере на сайте Sentence Transformers. Этот метод позволяет обучать мультиязычные модели, используя параллельные тексты на разных языках (заготовка скрипта make_multilingual.py).",
            "Для оценки качества модели я написал юпитер-блокнот, который загружает базовую и дообученную модель, прогоняет пары из eval сплита датасета evilfreelancer/opus-php-en-ru-cleaned и анализирует разницу между эмбеддингами, построенными для текстов на разных языках. Результаты визуализируются в виде графиков. Скрипт можно найти здесь.",
            "На графике видно, что базовая модель bert-base-multilingual-uncased распределяет русские и английские тексты в изолированные кластеры точек, ну а наша задача сделать так, чтобы эти точки были расположены как можно ближе друг к другу.",
            "Подобную задачу позволяет решать MSELoss, так как она минимизирует разницу между эмбеддингом, сгенерированным моделью-учителем (на английском языке) и эмбеддингом, сгенерированным моделью-учеником (на русском языке).",
            "Теперь пару слов про датасеты, решил остановиться на следующем наборе:",
            "evilfreelancer/opus-php-en-ru-cleaned (1.6k) - ранее созданный датасет параллельных текстов на английском и русском языках;",
            "Helsinki-NLP/opus_books (17.5k) - датасет OPUS параллельных текстов из книг.",
            "Выбрал я их потому, что мои первые эксперименты с обучением модели на только PHP датасете показали, что у модели происходит overfitting в результате чего падала общее качество работы модели, поэтому самым логичным решением было добавить ещё один Parallel Corpora общего назначения.",
            "Помимо этого в скрипт обучения я хотел сразу заложить возможность обучать на множестве разных датасетов (имеющих разные форматы данных), в результате чего получилась функция:",
            "В дальнейшем планирую добавить в неё больше датасетов на разные технические темы, но на этапе прототипирования того что есть более чем достаточно.",
            "Двигаемся дальше.",
            "Полный скрипт тренировки train_parallel.py можно найти в репозитории проекта на GitHub, в качестве модели-учителя возьмём google-bert/bert-base-multilingual-uncased, а в качестве модели-ученика ту, что мы обучили ранее на шаге Domain Adaptation.",
            "Обучение происходит в несколько этапов:",
            "Сначала мы загружаем датасеты (функция read_datasets);",
            "Далее выполняем их преобразование в нужный формат, после чего сохраняем на диске (функциия prepare_datasets)",
            "Инициализируем модель-учитель и модель-ученик (тут)",
            "Инициализируем MSELoss, передав ей на вход указатель на модель-ученика (тут)",
            "Запускаем обучение модели-ученика",
            "По завершению обучению давайте попробуем протестировать модель и понять стала ли на лучше извлекать эмбеддинги.",
            "Как видно на графике эмбеддинги извлечённые из русских и английских текстов где-то наложились друг на друга, точность похожести поднялась с 0.83 до 0.94, при этом модель также хорошо разделяет фразы различающиеся по смыслу.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1-domain",
            "Посмотрел я на этот график и пришла в голову мысль, а что если попробовать обучить базовую модель сразу на Parallel Corpora, пропустив шаг с Domain Adaptation?",
            "Правим скрипт тренировки, меняем модель-ученика, получается вот так:",
            "Опять запускаем тренировку и ждём некоторое время, по завершению прогоняем тесты и смотрим что получилось.",
            "Как видно на графиках если обучать сразу на Parallel Corpora модель быстрее, так как не нужно выполнять Domain Adaptation, и лучше обучается извлекать эмбеддинги из параллельных текстов, ведь косинусное расстояние в таком случае между близкими по смыслу фразами на разных языках в среднем в районе 0.97, что выше чем у модели изначально обученной на домене текстов про PHP.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1",
            "Отсюда можно сделать вывод, что дообучение мультиязыковой модели bert-base-multilingual-cased через Domain Adaptation с последующем обучением на Parallel Corpora не имеет особого смысла и проще сразу дообучать её на Parallel Corpora.",
            "Осталось выполнить самую малость, для начала я хочу конвертировать модель в формат GGUF, чтобы можно было использовать обученные модели через llama.cpp, но на этом моменте сильно не будем заострять внимание, сошлюсь на мою публикацию \"Как конвертировать модель BERT в формат GGUF?\" в моём блоге и PR который я создал в проекте llama.cpp.",
            "Но если кратко команды конвертации нужно выполнять и корня проекта llama.cpp и выглядят они следующим образом:",
            "По её завершению в директории models появятся файлы: enbeddrus-v0.1-f16.gguf и enbeddrus-v0.1-domain-f16.gguf.",
            "Полученные модели я выгрузил на серверы Ollama следующим образом:",
            "Выгруженные модели находятся тут и скачать их можно следующей командой:",
            "Содержимое Modelfile'ов можно найти в директории models проекта на GitHub.",
            "https://github.com/EvilFreelancer/enbeddrus",
            "https://huggingface.co/datasets/evilfreelancer/opus-php-en-ru-cleaned",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1-domain",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1",
            "https://ollama.com/evilfreelancer/enbeddrus",
            "Благодаря работе над проектом enbeddrus были достигнуты следующие цели:",
            "Удалось разобрался с тем как подобные модели устроены и как они работают, а так же с тем как их можно обучать;",
            "Был собран датасет с Parallel Corpora тематических текстов о PHP на русском и английском;",
            "Удалось разобраться с методами оценки моделей, а также с тем как эту оценку красиво визуализировать;",
            "Была обучена модель, которая эффективно работает с текстами на двух языках и может быть использована в RAG-системе для поиска и анализа информации.",
            "Полученные результаты подтверждают, что обучение мультиязычных эмбеддеров на основе параллельных корпусов является эффективным подходом для создания моделей, способных работать с текстами на разных языках.",
            "Спасибо за внимание и за что дочитал публикацию до конца! Если у вас есть вопросы или вы хотите связаться со мной, ссылки на мои контакты в социальных сетях можно найти в моём профиле на Хабре."
        ]
    },
    {
        "Название статьи": "Я научу вас неправильно играть в Hearts of iron. Оптимизация довоенной экономики: часть 2",
        "Дата публикации": "2024-06-02, 17:02",
        "Автор статьи": "Glasssparrow ",
        "Статья целиком": [
            "В прошлой части мы создали инструментарий, настало время им воспользоваться.",
            "За долю секунды мы можем провести симуляцию нескольких внутриигровых лет, что позволяет нам применить простейший метод исследования - метод перебора. И, раз уж мы всё равно будем перебирать, стоит также построить графики.",
            "В качестве испытуемой страны мы выберем, конечно, Советский Союз, условия будем выбирать близкие к реальному прохождению.",
            "Во-первых, рассмотрим торговлю. Торговля в игре зависит от многих факторов и не может быть оценена в симуляции, т.к. мы не работаем со всем миром (это потребует больших вычислительных мощностей). Таким образом, торговлю можно взять только из игры, что я и сделал, прокрутив 5 внутриигровых лет и записав количество фабрик получаемых от торговли. При этом закупки брались равными нулю (закупки сильно зависят от того что производит игрок, потому для общего случая я их просто игнорировал).",
            "1 установить_торговлю 18 120 установить_торговлю 10 210 установить_торговлю 7 365 установить_торговлю 5 730 установить_торговлю 9 1100 установить_торговлю 13 1450 установить_торговлю 25 1800 установить_торговлю 18",
            "Во-вторых, рассмотрим технологии. Нам важны технология индустрии и технология строительства. Моменты их развития также были получены из игры следующим образом: технологии исследовались без опережения по времени (с некоторыми погрешностями, конечно), в первую очередь строительство и индустрия, во вторую электроника, всё остальное исследовалось лишь для того чтобы сбить накапливающиеся во время простоя дополнительные 30 дней исследований. Никаких фокусов, решений и политик на исследования использовано не было.",
            "188 construction_tech # технология строительства 1 328 industry_tech # технология индустрии 1 511 construction_tech # технология строительства 2 511 industry_tech # технология индустрии 2 1285 construction_tech # технология строительства 3 1285 industry_tech # технология индустрии 3 1950 construction_tech # технология строительства 4 1950 industry_tech # технология индустрии 4",
            "В-третьих, обратим внимание на фокусы. Рассмотрим 4 варианта: 1) Стандартное быстрое закрытие паранойи с советником на гражданское строительство и частичной мобилизацией2) Оно же, но дополнительно поставим свободную торговлю, когда будет политка3) Также быстро закрываем паранойю, но частичную мобилизацию берем до советника на гражданское строительство (это будет стоить нам 30 политки)4) Вообще никаких фокусов не берем, только советник и частичная мобилизация.",
            "sov # тэг страны.140 добавить_гражданского_советника # если идти по пути Сталина то политки как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранойи 350 добавить_лимит_фабрик 0.1 # фокус ветки паранойи 350 добавить_бонус_строительства 0.05 # фокус ветки паранойи 350 добавить_фабрики 2 # фокус ветки паранойи 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov140 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная #320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная",
            "В общем и целом, получаем достаточно неплохие бонусы на промышленность, но отнюдь не максимальные (есть еще плакаты на -15% товаров народного потребления, есть множество других бонусов на промышленность в фокусах), но для общего случая этого и не нужно, мы (пока) не собираемся ограничивать игрока в свободе выбора фокусов.",
            "Рассмотрим получившиеся графики зависимости максимума количества военных заводов от количества фабрик которые мы строим (это важно, фабрики от фокусов в счетчик не идут) до переключения на военную промышленность.",
            "Видим что 5% от свободной торговли дают нам лишь 4 завода, при том что 2 завода мы можем получить за 30 политки, просто взяв частичную мобилизацию до советника. Скупают ресурсы у СССР или не так активно чтобы свободная торговля была в плюс, или слишком поздно, когда ресурсы уже нужны для производства.",
            "Также видим что бонусы от фокусов в ветке паранойи дают большое преимущество: 116 заводов против 145 (если мы не берем скидку, то нет смысла брать советника до мобилизации экономики). Честно говоря, не ожидал такой разницы, бонусы не выглядели для меня настолько сильными (+5% к скорости строительства, -2% тнп, 10% к лимиту зданий в провинции, 2 фабрики, 2 завода). Товаров народного потребления дают не так много, +5% это как бонус от свободной торговли, места под строительство у СССР и так хватает. Но в сумме разница выходит почти в 30 заводов.",
            "Ну и самое важное: видим малые производные на достаточно широком участке. В сущности, при строительстве от 28 до 50 фабрик, количество военных заводов на 1 января 1941 года остаётся +/- стабильным.",
            "Как было сказано выше, количество заводов относительно постоянно на участке от 28 фабрик до 50 фабрик, рассмотрим это две точки подробнее:",
            "Если начинать строить заводы раньше, то снаряжения к итоговой дате будет больше (см. площадь под графиками военных заводов. Также стоит учесть что эффективность производства будет нарастать со временем, значит, соотношение снаряжение будет больше соотношения площадей), однако я не уверен, насколько следует переходить к максимизации снаряжения. В игру добавили новую систему снабжения (да, для меня она всё еще новая), теперь она требует строительства пунктов снабжения и железных дорог. Таким образом, нельзя считать экономику эффективной если мы можем произвести снаряжение, но не можем доставить его до фронта. Портальные технологии еще более увеличивают важность логистики, т.к. вся произведенная техника сначала телепортируется в столицу и уже после этого отправляется на фронт вместе с пряниками из карамельной страны.",
            "Помимо оптимизации момента перехода с фабрик на военные заводы, можно также оптимизировать количество инфраструктуры. Можно оценить выгодность инфраструктуры при помощи следующего несложного скрипта (нужен только базовый python, никаких сторонних библиотек):",
            "Отдельно отмечу, что приведенный выше код не учитывает что построенные в процессе фабрики тоже будут строить.",
            "Строительство инфраструктуры, это совсем не то же самое что переход с фабрик на военные заводы. Наиболее выгодное количество инфраструктуры разнится в зависимости от количества слотов под строительство. Да и выгода не всегда велика. Скажем так, оптимальное строительство инфраструктуры потребует от игрока определенных усилий.",
            "Наиболее универсальная оптимизация удалась, но что делать с частными случаями пока не понятно. Баланс между логистикой и количеством снаряжения для разных государств будет разным. Понимание того как этот баланс устроен требует большего понимания самой игры (что требует играть в игру правильно, а это не наш случай).",
            "Оптимизация инфраструктуры выглядит многообещающе, но оценка желания игроков применять это оптимизацию - нет. Также сам процесс нахождения оптимального алгоритма строительства с инфраструктурой выглядит достаточно сложно.",
            "В текущей программе упущен такой момент как возможность аннексии других государств. Этот процесс не обязательно сопровождается войной (а тем более серьезной войной), так что под концепцию довоенной экономики вполне подходит. В программе уже реализован алгоритм реализации контроля провинций, остаётся лишь добавить механизм добавления провинций по тэгу страны владельца или по принципу выделяемых стран (например возвращение польских территорий можно реализовать через добавление к СССР всех провинций Белоруссии и Украины, которые еще не входят в состав Советского Союза). Подобное может потребовать расчета строительства для аннексируемых стран (они же тоже развивают свою экономику параллельно вам), что замедлит расчет, но можно упростить его просто до строительства военных заводов, думаю это не должно добавить много погрешности.",
            "С интерфейсом пока всё совсем не здорово. Идеи приходящие мне в голову или звучат очень сложно или звучат еще менее удобно чем редактирование текстовых файлов. Так что gui пока застопорился.",
            "readme.txt будет обновляться по мере внесения изменений в программу. Если вы считаете что в нём чего-то не хватает, можете мне написать в дискорде или прямо в комментарии под этой статьей. В целом, планирую постепенно работать над читаемостью кода и документацией проекта.",
            "Весь код репозитория распространяется по лицензии MIT, которая гласит следующее: \"делайте с кодом что хотите, но не надо из-за него со мной судиться\". В общем, свободное ПО и всё такое, развлекайтесь, если хотите."
        ]
    }
][
    {
        "Название статьи": "Машинное обучение с Python и TensorFlow на Windows. Быстрый старт",
        "Дата публикации": "2024-06-06, 10:10",
        "Автор статьи": "Sber ",
        "Статья целиком": [
            "Словосочетание «машинное обучение» становится всё более значимым с каждым годом и проникает во все возможные сферы жизни, а с появлением в открытом доступе таких нейронных сетей как Chat GPT [1] интерес к машинному обучению стал высок как никогда. Но при этом многих отпугивает сложность создания своих систем на основе машинного обучения, потому что нужно одновременного использовать и настраивать много разных инструментов разработки.",
            "Поэтому я хочу представить вашему вниманию максимально простую инструкцию для быстрого погружения в мир машинного обучения. Инструкция ориентирована в первую очередь на начинающих программистов, мы будем применять Python 3 [2] с библиотекой TensorFlow [3]. Это лучший выбор для начинающих из-за простоты языка и большого сообщества разработчиков, использующих TensorFlow.",
            "Ваш процессор должен поддерживать AVX-инструкции [4], а видеокарта должна поддерживать архитектуру CUDA начиная с версии 3.5 (список подходящих видеокарт [5]):",
            "AVX (Advanced Vector Extensions) — это расширение системы команд x86-архитектуры для микропроцессоров Intel и AMD, предложенное Intel в марте 2008 года.",
            "CUDA (Compute Unified Device Architecture) — это программно-аппаратная архитектура параллельных вычислений, разработанная компанией NVIDIA. Она позволяет существенно увеличить вычислительную производительность благодаря использованию графических процессоров (GPU).",
            "Далее я буду говорить о TensorFlow 2.10, так как последующие версии требуют установки Windows Subsystem for Linux (использование WSL усложнит установку и ограничит количество подходящих версий Windows). TensorFlow 2.10 будет работать на Windows 7 и более новых версиях операционной системы. Версия Python должна быть от 3.9 до 3.11.",
            "Нужно начать с пакета Microsoft Visual C++ для Visual Studio 2015, 2017 и 2019. Далее установите Python с минимальным набором библиотек и менеджером пакетов — лучше всего для этого подходит Anaconda [6]. Она содержит в себе дистрибутив языка Python, систему управления средой разработки Anaconda Navigator (визуальный менеджер пакетов, управление дополнительным софтом и т. д.) и базовый набор библиотек. Но если вы не фанат визуальных сред разработки или не хотите ставить лишние программы на компьютер, то можно ограничиться Miniconda (это то же самое, что и Anaconda, но без лишних программ, только библиотеки и консольный менеджер пакетов Conda) [7].",
            "После установки Anaconda нужно запустить Anaconda Powerschell Prompt и выполнить в консоли команду:",
            "Она создаст новую среду conda (в рамках одной среды можно использовать определённый набор библиотек, это полезно, если для разных проектов нужны разные версии библиотек) и установит для неё Python 3.9.",
            "Затем с помощью команды activate активируем созданную выше среду (deactivate возвращает базовую среду):",
            "Теперь нужно установить свежие драйверы для вашей видеокарты [8], а если они у вас установлены, то можно переходить к установке пакетов CUDA и cuDNN. Это можно сделать выполнив одну команду:",
            "Указанные выше версии пакетов лучше не менять, так как могут быть проблемы с совместимостью библиотек. Выполнение этой команды выглядит так:",
            "Для корректной работы TensorFlow требует свежая версия pip (это менеджер пакетов). Обновите его:",
            "TensorFlow можно установить и через conda (или через Anaconda Navigator), но разработчики TensorFlow рекомендуют использовать именно pip. Устанавливаем:",
            "Нужно удостовериться, правильно ли всё настроено и установлено. Для этого выполните в консоли или вашей среде разработки код:",
            "Очень удобно для разработки использовать IDE Spyder, если её установить с помощью Anaconda Navigator для нужной среды. Выглядит установка так (выбираем среду и нажимаем установить):",
            "Обратите внимание на текущую среду, ибо именно в неё будет установлено выбранное ПО. После установки вы можете запустить Spyder из Навигатора от имени выбранной среды (это значит, что будут доступны библиотеки, которые загружены именно для среды tf).",
            "Теперь выполняем код в консоли, который я приводил выше:",
            "Если всё правильно сделано, то система должна вывести список доступных для TensorFlow устройств (в моем случае это видеокарта RTX 3060).",
            "Пришло время создать первый проект, использующий библиотеки машинного обучения. Для теста возьмём стандартный проект классификации ирисов [9]. Создайте скрипт с расширением .py и таким кодом:",
            "По ссылке [9] скачайте файл IRIS.csv из раздела «Входные данные» и разместите его в одной папке со скриптом. Этот файл содержит исходные данные для обучения модели. Затем запустите выполнение скрипта и дождитесь его завершения:",
            "Если скрипт отработал корректно, в конце вы увидите график обучения модели в разделе Plots.",
            "Примечание: при первой попытке выполнения этого скрипта интерпретатор выдаст ошибку об отсутствии необходимых библиотек. При реализации своих проектов вам придётся использовать и устанавливать различные библиотеки. Устанавливать их можно разными способами консольными менеджерами пакетов (pip или conda) или визуальным менеджером пакетов Anaconda Navigator. Старайтесь использовать только один способ, иначе может возникнуть путаница и конфликты между разными версиями библиотек. Я обычно использую визуальную установку через Anaconda Navigator, это позволяет более наглядно контролировать набор установленных пакетов и их версии. Пример визуальной установки пакета для программы классификации ирисов:",
            "Для установки пакета активируем вашу среду (tf в моём случае), выбираем раздел «Неустановленные», в поле поиска (справа) вводим название требуемой библиотеки, ставим на ней галочку и нажимаем Apply. После установки всех необходимых библиотек, которые импортируются в начале скрипта, программа классификации ирисов должна работать.",
            "На этом этапе можно поздравить вас с первым запуском проекта на основе TensorFlow. Теперь вы точно готовы к реализации своих проектов с применением технологий машинного обучения.",
            "Желаю вам успехов во всех ваших будущих проектах и спасибо за внимание. До новых встреч.",
            "https://openai.com/",
            "https://www.python.org/",
            "https://www.tensorflow.org/",
            "https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX",
            "https://developer.nvidia.com/cuda-gpus",
            "https://docs.anaconda.com/free/anaconda/install/windows/",
            "https://docs.anaconda.com/free/miniconda/",
            "https://www.nvidia.com/download/index.aspx",
            "https://www.kaggle.com/code/venkatkrishnan/iris-data-tensorflow-neural-network/notebook"
        ]
    },
    {
        "Название статьи": "Введение в gRPC: Основы, применение, плюсы и минусы. Часть I",
        "Дата публикации": "2024-06-05, 23:59",
        "Автор статьи": "0xN1ck ",
        "Статья целиком": [
            "gRPC (gRPC Remote Procedure Call) — это современная высокопроизводительная фреймворк для удаленных вызовов процедур, разработанная Google. gRPC позволяет клиентам и серверам общаться напрямую, используя протокол HTTP/2 и Protocol Buffers (protobuf) в качестве языка описания интерфейсов (IDL). Эта технология предоставляет возможность эффективного взаимодействия между различными компонентами распределенных систем, независимо от языка программирования.",
            "gRPC основывается на архитектуре клиент-сервер и поддерживает множество языков программирования, включая C++, Java, Python, Go, Ruby и многие другие. В основе gRPC лежат следующие ключевые компоненты:",
            "Protocol Buffers (protobuf): Это язык описания данных и инструмент сериализации, который используется для определения сервисов и их методов, а также для обмена данными между клиентом и сервером. Protobuf позволяет описывать структуру данных в специальном .proto файле, который затем компилируется в исходный код для выбранного языка программирования.",
            "HTTP/2: Протокол транспортного уровня, который обеспечивает мультиплексирование запросов, сжатие заголовков, и другие улучшения производительности по сравнению с HTTP/1.x. HTTP/2 позволяет отправлять несколько запросов через одно соединение, что значительно уменьшает задержки и улучшает пропускную способность.",
            "Stub-генерация: gRPC автоматически генерирует клиентские и серверные stub'ы на основе protobuf-файлов, что упрощает процесс интеграции и уменьшает количество шаблонного кода. Клиенты используют сгенерированные stub'ы для вызова методов на сервере так, как если бы они вызывали локальные функции.",
            "gRPC широко используется для построения распределенных систем и микросервисных архитектур. Вот несколько типичных сценариев его применения:",
            "Микросервисы: В больших системах, где микросервисы взаимодействуют друг с другом, gRPC обеспечивает эффективное и надежное общение с низкой задержкой. Это особенно полезно в системах, где важно минимизировать время отклика и обеспечить высокую пропускную способность.",
            "Взаимодействие между разными языками: Благодаря поддержке множества языков, gRPC позволяет разрабатывать системы, где компоненты написаны на разных языках программирования, легко взаимодействуя между собой. Это упрощает интеграцию различных технологий и позволяет выбирать наиболее подходящий язык для каждого компонента системы.",
            "Реализация API: gRPC идеально подходит для создания высокопроизводительных API, где критичны низкие задержки и высокая пропускная способность. API, реализованные с помощью gRPC, могут использоваться как внутри организации, так и предоставляться внешним пользователям.",
            "Мобильные и IoT приложения: gRPC отлично подходит для мобильных и IoT приложений благодаря своей эффективности и низкому потреблению ресурсов. HTTP/2 обеспечивает минимальное использование сетевых ресурсов, что особенно важно для устройств с ограниченными возможностями.",
            "Высокая производительность: Благодаря использованию HTTP/2 и protobuf, gRPC обеспечивает низкие задержки и высокую пропускную способность. Это делает его идеальным выбором для высоконагруженных систем.",
            "Ясно определенные интерфейсы: Использование protobuf для описания сервисов и сообщений обеспечивает четкую контрактность и минимизацию ошибок на этапе компиляции. Это упрощает процесс разработки и интеграции различных компонентов системы.",
            "Поддержка различных языков: gRPC поддерживает множество языков программирования, что позволяет интегрировать компоненты, написанные на разных языках, в единую систему. Это упрощает использование существующего кода и технологий.",
            "Би-ди стриминг: gRPC поддерживает не только однонаправленные и двунаправленные потоки, но и полный дуплекс, что позволяет реализовать сложные сценарии взаимодействия. Это особенно полезно для приложений, требующих постоянного обмена данными в реальном времени, таких как чаты или системы мониторинга.",
            "Автоматическая генерация кода: gRPC генерирует клиентские и серверные stub'ы, что упрощает разработку и снижает количество шаблонного кода. Это сокращает время разработки и уменьшает количество ошибок, связанных с ручным написанием кода.",
            "Крутая кривая обучения: Для новичков gRPC может показаться сложным из-за необходимости освоения protobuf и специфических особенностей HTTP/2. Однако, с практикой и доступными ресурсами, обучение становится легче.",
            "Ограниченная поддержка браузеров: gRPC не поддерживается большинством браузеров напрямую, что требует использования дополнительных прокси-серверов или gRPC-Web. Это добавляет дополнительную сложность при создании веб-приложений, использующих gRPC.",
            "Зависимость от protobuf: Использование Protocol Buffers как основного формата сериализации может быть ограничением для тех, кто предпочитает другие форматы, такие как JSON или XML. Хотя protobuf предлагает высокую производительность и компактность, он требует дополнительных шагов для сериализации и десериализации данных.",
            "Инфраструктурные требования: Для эффективного использования gRPC необходимо обеспечить поддержку HTTP/2 на уровне сетевой инфраструктуры, что может потребовать дополнительных настроек и ресурсов. Это может стать препятствием для некоторых организаций, особенно если их существующая инфраструктура не поддерживает HTTP/2.",
            "gRPC — это мощный инструмент для построения высокопроизводительных распределенных систем и микросервисов. Он обеспечивает эффективное общение между сервисами, поддерживает множество языков программирования и предлагает ясные и контрактно-ориентированные интерфейсы. Однако, как и любая технология, gRPC имеет свои недостатки и требует определенных усилий для освоения и интеграции.",
            "Если вы строите сложную распределенную систему или ищете способ улучшить взаимодействие между микросервисами, gRPC может стать отличным выбором, обеспечивая высокую производительность и надежность. Важно тщательно взвесить плюсы и минусы этой технологии и оценить ее применимость к вашему проекту, чтобы получить максимальную пользу от использования gRPC."
        ]
    },
    {
        "Название статьи": "Майним крипто-пойнты с помощью цветового автокликера на Python",
        "Дата публикации": "2024-06-05, 20:22",
        "Автор статьи": "temabed ",
        "Статья целиком": [
            "Привет, Хабр! Я продолжаю цикл небольших статей для энтузиастов и начинающих программистов о том, как интересно, а иногда и с выгодой, можно применять свои навыки.",
            "В последнее время широкое распространение получили разные крипто-проекты, которые обещают пользователям материальные вознаграждения за определенную активность. Особенно актуальны они стали после успеха Notcoin, который очень неплохо отблагодарил своих пользователей. Не буду углубляться как это всё работает, откуда там деньги, и много ли среди таких проектов скама (да), нас интересует лишь тот момент, что большинство этих проектов можно автоматизировать, а значит не терять драгоценное время.",
            "Да, большинство из таких проектов имеют достаточно примитивную механику, но при этом отнимают слишком много времени, обещая лишь туманную перспективу возможного заработка. А нас, взрослых людей, такой расклад не устраивает, поэтому за нас должен работать хотя бы робот.",
            "Сегодня на операционном столе будет приложение в телеграм, которое в будущем должно стать полноценной децентрализованной биржей, но в настоящее время позволяет новым пользователям лишь зарабатывать внутренние очки. Хочу сразу сказать, что даже руководитель проекта честно заявляет, что обмен этих очков на что-то более материальное в будущем вовсе на обязателен, имейте это ввиду. Я его тоже не рекламирую, и уж тем более не даю никаких финансовых рекомендаций, он нам нужен лишь в качестве примера.",
            "Механика мини игры внутри этого приложения такая: в течение небольшого времени сверху падают цветочки, которые нужно ловить простым нажатием. За них и начисляют очки. Однако, недавно в игре появились еще и бомбочки, при нажатии на которые счет обнуляется.",
            "В связи с нововведением в виде бомбочек бездумный автокликер использовать стало невозможно. Поэтому я делал попытку использования для убийства цветков компьютерного зрения. Но эксперимент вышел так себе, терминатор получился достаточно убогим, и мир ему захватывать еще не скоро.",
            "А сегодня рассмотрим механику всё того же самого простого автокликера, но определяющего цвет пикселя перед нажатием. Который делает клик только при условии, что пиксель в нужном диапазоне. Нам понадобятся всего три библиотеки.",
            "Pyautogui будет управлять мышью и определять цвет пикселя, на который наведен курсор. Keyboard нужен для назначения горячих клавиш, а time для временной задержки.",
            "Назначим три переменные.",
            "Work будет отвечать за работу кода в бесконечном цикле. Если True- выполняется следующая функция, если False - всё останавливается. Points содержит координаты двадцати точек, изначально равные нулю, так как мы их будем назначать после запуска программы. А index нужен, чтобы демонстрировать пользователю, какую по счету точку он назначает. И сразу напишем функцию присваивания координат точкам для кликов.",
            "Эта функция будет срабатывать при нажатии на горячую клавишу и последовательно присваивать координаты каждой из двадцати точек. Координаты будут браться с позиции, на которую в этот момент наведен курсор мыши.",
            "Следующая функция будем изменять значение переменной work и срабатывать так же при нажатии горячей клавиши. Она будет ставить на паузу работу автокликера.",
            "Теперь функция самого кликера.",
            "В цикле for программа пробегает по всем координатам для клика мышью, но на каждой итерации делается снимок экрана, и перед нажатием определяется цвет пикселя, на который указывает курсор. Важно, что метод getpixel возвращает кортеж с тремя числами, которые характеризую цвет по системе RGB. Методом тыка я пришел к выводу, что для определения цветка нам достаточно, чтобы первое число в картеже было в диапазоне от 134 до 218, а вот если второе и третье число равны, это уже свидетельствует о сером оттенке, в которые выкрашены бомбы, и на такое кликать не стоит. Соответствующие условия я и передал в конструкцию if.",
            "Ну и основная функция, которая будет записывать координаты для кликов нажатием на горячую клавишу \"=\" и запускать кликер нажатием на \"~\".",
            "Вот такая простенькая, но многофункциональная программа получилась. Код целиком:",
            "Применить такой код сможет любой, даже не обладая навыками программирования, и не только по назначению, указанному в статье. Вариантов масса, включая самые обычные компьютерные игры.",
            "Поэкспериментировать же с озвученным проектом возможно только лишь в случае, если у вас есть инвайт (пригласительная ссылка). К сожалению, у меня они уже закончились, поэтому поспрашивайте у знакомых либо в моем телеграм-канале под соответствующими постами.",
            "И последнее, эффективность описанного кода сейчас зависит только от стратегии, по которой вы разместили точки для кликов. У меня результат был так себе, но я нашел самый эффективный способ для фарма в таких простых играх - это дети. Хотя это уже читерство, согласен."
        ]
    },
    {
        "Название статьи": "Магия динамического маппинга. Реализация универсальной обработки файлов нефиксированной структуры на Python",
        "Дата публикации": "2024-06-05, 15:58",
        "Автор статьи": "spectr_dev ",
        "Статья целиком": [
            "Привет! На связи Никита Ильин из Spectr, Backend-разработчик с опытом более 5 лет.",
            "Один из проектов, с которым мы работаем, — IBP-платформа для планирования и прогнозирования спроса и продаж в ритейле. В статье поговорим о конкретной реализации для одной из задач в рамках этой платформы на Python и Django. При этом сама концепция может быть реализована абсолютно на любом фреймворке или платформе: Spring, .NET, Laravel.",
            "Мы разрабатываем IBP-платформу для крупной корпорации, где на основе данных, которые поступают из смежных систем, строятся прогнозы и аналитика. И одна из областей — работа с огромным количеством файлов из внешних источников: чтение, обработка, загрузка и запись всех этих данных в БД. При этом существует большое количество различных источников и форматов этих файлов.",
            "Глобально задача состоит в том, чтобы осуществить загрузку из внешних источников в единую внутреннюю систему для последующего анализа и прогноза.",
            "Информация об источниках.",
            "Сейчас в системе около 350 источников. При этом одновременно может поступать до 100 штук новых.",
            "Важное уточнение: один источник — это файл уникальной структуры. Если названия столбцов различаются, то это уже новый файл, новый источник данных.",
            "Информация о файле. Это обычный классический csv-формат. Разделителем может быть либо «;» либо «,». Число колонок — от 5 до 200, но распарсить нужно только количество, которое обозначено в техническом задании. В нашем случае доходило до 40 колонок, все остальное — системные поля. Число строк всегда большое — от 5 млн до 20 млн.",
            "Ниже представлен классический пример файла с условными обозначениями.",
            "Отлично, вся информация о данных у нас на руках! Что же делаем? Тут возможны два пути.",
            "Первая идея, которая у нас возникла, — статическая реализация или, по-другому, «решение в лоб». При такой реализации мы для каждого источника пишем свой парсер и применяем его — эффективно и быстро. Поговорим о преимуществах чуть подробнее.",
            "Быстрая разработка. Раньше уже существовало какое-то количество источников и мы уже знали, как работать по этому пайплайну: без лишних оптимизаций и размышлений. То есть мы приоритезируем источники, отдаем наиболее важные по значимости в первую очередь, а сами занимаемся другими.",
            "Работает надежно, как швейцарские часы. Когда мы пишем парсер на конкретный источник, мы можем написать на него тест. И перед тем как выкатывать, надо посмотреть — а точно ли поведение такое же, как ожидается? Тут мы говорим про стабильность и проверенность того, что наш парсер работает.",
            "Без непредвиденных side-эффектов. Все парсеры строго императивны и изолированы и, благодаря этому, более стабильны. Ведь когда мы говорим про enterprise-разработку, мы точно не хотим, чтобы один наш парсер сломал всю систему.",
            "Сходить на нужный внешний сервер по SFTP.",
            "Именно SFTP, потому что это спецификация нашей задачи, мы так договорились, нам так комфортно общаться между серверами.",
            "Забрать файл.",
            "Применить парсер.",
            "Сохранить в БД.",
            "Работа аналитика. Сверка с локальной структурой БД и проектирование перевода из внешнего имени во внутреннее. Нам нужно узнать, что, где и как хранится, какая между всем этим связь. Данные формируются в виде спецификации, требований и отдаются разработчику.",
            "Работа Backend-разработчика.",
            "Написание кода для валидации файла и его перевода. То есть это сам код, который осуществляет разбор данных в соответствии с ожидаемой структурой.",
            "Добавление конфигурации в общую структуру парсеров: на какой сервер идем, куда и как данные сохраняем. Мы обобщили все на уровне кода, а теперь надо это явно прописать, куда мы идем, чтобы получить данные о конкретном источнике.",
            "Например, когда у нас SFTP-сервер, я не думаю, что вы будете писать одни и те же четыре строчки кода в каждом парсере. Скорее всего, это будет какая-то функция или класс, которому передаем имя файла и доступы для SFTP-сервера, чтобы он пустил в систему.",
            "Работа тестировщика. Отладка, тесты на деве, стейдже на обрезанных данных. На этом этапе мы выявляем и устраняем ошибки в коде парсера. То есть тестировщик должен проверить две вещи:",
            "работает ли код. То есть он берет маленький кусочек файла, например 100–1000 строчек, и смотрит, встали ли данные в БД, в интерфейс, не свалилось ли что-то и работает ли функционал в целом;",
            "какая скорость. Нужно понять, удовлетворяем ли мы скорости загрузки и нет ли каких-то проблем.",
            "Работа тестировщика и DevOps-инженера. Пуско-наладка на реальных данных. Далее мы все выкатываем на прод, делаем первые итерации, проверяем, все ли работает: встали ли данные в интерфейс, ничего ли не потерялось, выдерживаем ли по ресурсам.",
            "Время разработки пропорционально количеству парсеров. Для разработки одного парсера нам нужно 5 дней. Сверка со структурой БД занимает 1 день, написание парсера — 2 дня, на отладку/тесты и пуско-наладку закладываем еще по одномуу дню.",
            "На один парсер — 5 дней, а на 100 — целых 1,5 года. Процесс, конечно, можно оптимизировать и вести параллельно: аналитику не ждать разработчика, а разработчику не ждать тестировщика. Но тем не менее все это в сумме — большой объем очень рутинной работы.",
            "Время доработок пропорционально количеству парсеров. Классический пример: есть 200 источников, при этом появилось требование о том, что столбец — это не целое число, а число с плавающей запятой. И теперь нужно это отвалидировать, чтобы все было в порядке, иначе данные не будут сходиться. А в случае переиспользования кода (DRY) нужно еще приложить усилия к тому, чтобы подумать, как это сделать. К тому же нужно заново пройти по пайплайну. Мы сделали изменения — значит, нам нужно все заново проверить, посмотреть, выкатить и замерить все парсеры.",
            "По итогу формула будет такой: время множественного изменения = время одного изменения * число парсеров. То есть время на расширение числа парсеров или валидируемых ими полей эквивалентно времени разработки. Не получится делать быстрее дублирующуюся работу, придется делать с такой же скоростью.",
            "Расширение числа парсеров или валидируемых ими полей запускает пайплайн заново. Подобная ситуация случилась лично с нами. Изначально было N парсеров, далее убрали 10 и потом добавили еще 15 сверху. А после этого в 20 имеющихся парсерах изменился состав файла и добавился еще один внешний сервер. Приходится начинать все сначала: аналитика –> разработка –> тестирование.",
            "Ключевой вопрос в этой всей ситуации — как преодолеть проблемы «решения в лоб» и сделать результат нашей работы максимально самодостаточным? Мы подумали об этом и пришли к другой идее — динамической реализации.",
            "Давайте вспомним No-code-приложения, например Tilda. Или такой конструктор мобильных приложений, где вы тащите формы, а затем система сама выполняет работу. Код генерируется — вы наслаждаетесь. Примерно то же самое мы сделали в рамках нашего проекта.",
            "В один момент мы подумали: «А что, если разработать пользовательский интерфейс, который позволит самостоятельно решать задачи, минуя аналитика, разработчика и тестировщика?» То есть пользователь сможет сам создать описание для своих действий в админке в виде шаблона. Затем наш магический механизм обработает созданный шаблон. А динамический парсер интерпретирует файлы, соответствующие структуре, описанной в шаблоне, без необходимости дополнительной ручной обработки.",
            "Эту идею мы назвали — шаблон динамического маппинга.",
            "На картинке ниже представлено, как это все можно изобразить с точки зрения пользовательского интерфейса. В этом списке темплейт — наш источник. Мы его назвали шаблоном.",
            "Далее представлено, какие примерно атрибуты могут быть у этого шаблона:",
            "название (name) — на что смотреть в интерфейсе;",
            "внешняя система (external system) — на какой сервер нужно пойти, чтобы достать конкретный файлик (уникальное системное имя шаблона);",
            "имя файлика и директория, в которой он лежит (filepath) — путь к файлу на сервере, с которым связан этот шаблон, и здесь же и имя файла;",
            "системное обозначение для источника (system name) — выбор из внешних систем, куда мы будем подключаться, чтобы туда идти по пути выше.",
            "Дальше мы уже говорим о том, что внутри этого источника. Это находится в отдельной сущности — attributes. Эта сущность включает в себя такие элементы, как:",
            "name — чтобы человеку было на что смотреть в интерфейсе;",
            "system_name — уникальное системное имя поля шаблона, которое мы должны искать в файле;",
            "type — тип данных поля, такие как float, str, str_alpha_numeric, date, int, bool;",
            "field_representations (представления поля) — JSON-структура, представляющая отображение поля на БД;",
            "template (шаблон, Foreign Key) — связь шаблона с общей инфой, к которому относится данное поле.",
            "На каждом этапе присутствует валидация, которая проверяет, например, наличие файлика, полей в файлике, соответствие типу. И, в конечном итоге, это все может записаться в БД.",
            "Сейчас мы построили чисто концептуальное решение. Давайте разберемся, какой результат нам бы принес этот подход — поговорим о его преимуществах.",
            "Элемент продукта закончен",
            "Такой формат реализации сокращает все возможные согласования. Вместо написания отдельного парсера для каждого нового файла и его источника создаем шаблоны, описывающие структуру файла и определяющие соответствие полей самостоятельно. Это существенно сокращает процесс работы, к тому же позволяет сразу же проверить результат. И это будет работать уже завтра. Сегодня написал — завтра это уже готово, сегодня придумал — завтра уже на проде валидируешь новый файл.",
            "Настройка и поддержка, которая оптимизирует время",
            "В случае со статической реализацией каждый день на протяжении полутора лет придется заниматься переводом спецификаций в код. Естественно, это не творческая и скучная задача. Мы все-таки хотим закрыть эту задачу и, конечно, сделать это наиболее интересным для нас способом.",
            "При динамической реализации настройка и поддержка будут намного интереснее, чем просто сконструировать «решение в лоб» и сидеть полтора года переводить спецификации.",
            "Возникает ряд вопросов. А что если где-то что-то отвалится? А почему данные не загружаются? А как вообще это все сохранить в табличку и как это все будет выглядеть? А как эти 20 млн строчек обработать? Явно придется над всем этим поразмыслить. 5 часов подумать — 1 час написать код.",
            "Масштабируемость",
            "Появляются новые требования:",
            "добавить в N-количестве источников проверку на дубликаты и действие, которое надо совершать, если они есть;",
            "а еще в M-источниках добавить проверку на отрицательность.",
            "При этом непонятно, будут ли все эти поля в файле.",
            "Так у нас появились дополнительные атрибуты у поля шаблона:",
            "required (обязательное) — флаг, указывающий, является ли поле обязательным для заполнения;",
            "can_be_negative (может быть отрицательным) — флаг, указывающий, может ли поле содержать отрицательные значения;",
            "contained_in_md_table (содержится в таблице md) — имя таблицы md, в которой содержится это поле (если применимо);",
            "contained_in_md_attribute (содержится в атрибуте md) — имя атрибута md, в котором содержится это поле (если применимо);",
            "duplicates_in_table (дубликаты в таблице) — имя таблицы, в которой разрешены дубликаты этого поля (если применимо);",
            "duplicates_attribute (атрибут дубликатов) — атрибут, определяющий дубликаты этого поля (если применимо);",
            "duplicates_action (действие с дубликатами) — выбор из действий по обработке дубликатов: обновление или пропуск.",
            "И на все это есть 5 дней — вспоминаем сроки разработки одного парсера. В такой системе мы на этапе валидации прописываем новые условия один раз, а дальше клиент уже сам работает с шаблонами и сам отвечает за выбранные им параметры.",
            "admin API (CRUD — в админке) — могут вносить изменения и создавать новые записи;",
            "user API (Read — для всех пользователей) — есть возможность только читать.",
            "Python и Django — это решение удобно для нас, к тому же мы используем его с начала работы над проектом.",
            "Blazingly-Fast Polars — о том, почему мы выбрали именно этот инструмент, рассказывал наш тимлид в статье Битва медведей: Pandas против Polars. Если кратко: этот вариант для наших вариантов использования работает быстрее, чем Pandas.",
            "Paramiko — библиотека для подключения по SFTP. Очень красиво и надежно.",
            "SQLAlchemy — в качестве дополнительной ОRМ. Удобный интерфейс, быстро и красиво.",
            "Здесь мы перевели сущность модели в сущность шаблона, которая была до этого. Из интересного — здесь есть функция temporary_table_name, в ней мы получаем temporary-имя. Это название временной таблицы. О том, для чего нам нужна временная таблица, поговорим чуть позже.",
            "Далее то же самое делаем для TemplateField. Можно просто взять, перевести на другой язык — и все готово.",
            "Как я и говорил, у админов будет полный набор CRUD-операций: чтение, создание, обновление. Но следующий момент более интересный.",
            "Вот эти два поинта ниже нам нужны для того, чтобы у клиента в интерфейсе была возможность посмотреть, какие у нас есть таблички и атрибуты и поля этой таблички.",
            "Классические представления — обычные классы доступа. Здесь идет пагинация по страницам. Далее, когда мы предоставляем список шаблонов, мы передаем только основную информацию о них — название. А когда отдаем только один шаблон, мы его отдаем вместе с атрибутивным составом, чтобы пользователь мог убедиться, что у него все правильно загружается.",
            "На этом этапе мы получаем какой-то путь файла у этого темплейта, то есть не пишем явно — идти на такой-то сервер. А просто говорим — взять у темплейта название файла. И на выходе получается путь файла.",
            "Далее через библиотеку мы подключаемся по SFTP, забираем файл.",
            "У темплейта есть филды — template.fields. Мы их забираем — это и будут наши правила валидации.",
            "django_file = InMemoryUploadedFile",
            "validate_file(django_file, list(template.fields.all()))",
            "BasicFileHeadersValidator. В процессе валидации проверяем наличие данных, дублирование колонок, наличие требуемых колонок. Если какой-то из этих пунктов не проходит, мы отправляем пользователю ошибку, так как нам незачем загружать файл, у которого нет требуемых нам колонок. Мы знаем, что он заведомо сохранит его туда, куда нам не надо.",
            "BasicFileReaderValidator / GeneralValuesFieldsValidator. Базовое чтение (проверка на кодировку, соответствие строк размеру), перевод названий колонок согласно шаблону, нормализация данных и проверка их на соответствие типам, генерация дата-фрейма.",
            "Даже если взять 10 млн строк, с учетом того, что параллельно работает 100 источников, cкорее всего, они упадут по памяти.",
            "Что с этим делать? Классический вариант — разбиение на чанки. В результате этого получается кусок файла, c которым можно продолжать работать. Далее с ним проводим соответствующие этапы валидации и формируем соответствующий дата-фрейм. Но в такой структуре важно, чтобы мы не захотели сделать дополнительную логику — например, агрегацию, дезагрегацию, суммирование, фильтрацию, категоризацию. Для всех этих работ мы используем сохранение во временную таблицу, а потом при необходимости все это сохраняем в основную БД.",
            "Если требование состоит в том, что решение нужно кастомное, то это делает уже другой разработчик в рамках другой задачи. Он идет во временную таблицу, забирает все нужные данные и, соответственно, с ними делает то, что нужно ему. При этом временную таблицу нужно каждый раз чистить перед загрузкой, иначе вы упадете по памяти в БД или будете работать со старыми данными.",
            "Что будем использовать дальше? Во-первых, SQLAlchemy. Собираем из дата-фрейма название колонок, делаем сущности колонок, берем название таблицы и с помощью контекстного менеджера вставляем какое-то количество записей. В нашем случае — 1 млн.",
            "Что такое контекстный менеджер",
            "В Python можно использовать удобный декоратор для этого паттерна. Перед началом мы напишем такой connection_url, где мы вставляем доступ к БД. Здесь конструкция try-finally говорит о том, что мы пытаемся отдать наш движок подключения к БД. Но в любом случае, какие бы ошибки ни были, этот движок будет в конечном итоге закрываться. Это нужно для того, чтобы в БД не висело открытое подключение.",
            "Берем нужную нам модель представления данных, написанную на Django.",
            "Здесь есть специфичный для Django код. Но я почти уверен, что таким образом можно получить табличку из основной БД на любом другом языке.",
            "Дальше мы смотрим поля у этой таблицы и пытаемся найти ее первичный ключ. Для нашего проекта специфично то, что может быть три разных первичных ключа, в зависимости от таблички: обычный ID, External ID либо Name ID.",
            "Итак, мы получили название первичного ключа. Далее мы идем в БД и проверяем, какие записи с этими ID уже есть.",
            "Мы берем их, кладем в оперативную память.",
            "После этого мы идем по дата-фрейму и проверяем, есть ли у нас такая запись.",
            "Если не смогли найти с этим ID объект среди тех, что мы вытащили из БД, то сохраняем ее как новый объект, кладем в какую-то структуру (в нашем случае список) и затем позже создадим их в БД.",
            "Но если мы все-таки смогли найти этот ID, то берем и меняем у найденного объекта все атрибуты.",
            "Условно, в файле одно значение, в БД другое значение для этой строчки — поменяли, сохранили в структуру. И здесь происходит множественное сохранение/обновление. Важно использовать batch_size. Потому что создавать запрос на миллион строчек — невыгодно. Делаем batch_size, разбиваем структуру на множество запросов и делаем с ним.",
            "batch_size=1000 — число объектов в каждом запросе.",
            "Заклинание освоено! А теперь поговорим о недостатках такого решения.",
            "Трудности в отладке и тестировании. Это интересно, но приходится думать, тратить время, пытаться понять, где ошибка, почему не можем распарсить — затратно дебажить.",
            "Строгость в соблюдении принципов. Важно соблюдать всю последовательность действий, про которую мы говорили ранее (этапы реализации динамического решения). Мы не должны запихнуть весь файл в систему, а на выходе говорить, что сделаем полностью кастомную работу.",
            "Ограничение в функциональности. Здесь как раз идет речь об отсутствии некоторых кастомных полей, все динамическое. И если вы тронете хотя бы одну строчку, то для всех остальных это изменение автоматически применится. Один файл, но его тяжело поддерживать.",
            "Дополнительное время на валидацию. Многие знают, что функции в Python вызывать довольно дорого, поэтому вы можете столкнуться с тем, что валидация может занимать много времени. Ни в коем случае не пытайтесь запустить код, который кажется примерно рабочим. Посмотрите, какие есть практики использования функции, почему нельзя использовать лямбду-функцию в цикле и т. д.",
            "Дополнительное время на сохранение во временную БД и/или основную БД. Мы говорим, что у нас есть временная таблица, и после нее вы можете делать некоторые кастомные операции, которые вы хотите. Но в то же время мы тратим минуту-две на то, чтобы сохранить эти данные, и, плюс ко всему, они занимают какую-то дополнительную память на диске.",
            "Как мы можем обойти эти минусы:",
            "Расширение поддерживаемых форматов файлов. Здесь имеются в виду Excel и все подобные структуры. Нам не потребуется писать отдельный парсер для каждого такого формата. Можно просто добавить новый вариант ридера, добавить его в общую структуру и смотреть на расширение файла перед загрузкой.",
            "Оптимизация производительности. Пытаемся ускорить валидацию, оптимизировав все возможные куски, меняя пайплайн вызова валидаторов.",
            "Развитие интерфейсов и возможных конфигураций. Админу нужно дать возможность делать batch_size для источника. То есть он знает, что, например, у него будет 100 валидированных колонок, и здесь batch_size в 100 тыс. — это слишком много, а 1,5 тыс. — уже нормально. Пусть у него будет возможность заменить на любое значение, которое он сам посчитает нужным. Дальше, если говорить о других возможных интерфейсах, у нас есть проверка на отрицательность на случай дублирования (мы меняем эту строчку или пропускаем ее и говорим, что нам все равно, есть ли она).",
            "И на этом все! В статье я поделился своим опытом, основанным на решении конкретной задачи. Надеюсь, что материал поможет вам выбрать правильное решение в аналогичной ситуации и покажет, как можно творчески подходить к задачам. А если у вас появятся вопросы — буду рад на них ответить в комментариях к этой статье!",
            "Статья подготовлена по мотивам доклада Никиты Ильина, Backend-разработчика в Spectr, на митапе #DevTalks. Ссылка на запись доклада:"
        ]
    },
    {
        "Название статьи": "Получение списка людей, посещающих определенные места",
        "Дата публикации": "2024-06-05, 15:12",
        "Автор статьи": "fire64 ",
        "Статья целиком": [
            "Представьте: вы ведете Telegram-канал о животных и хотите пригласить в него посетителей зоопарка. Или вам нужно собрать контакты потенциальных клиентов, посещающих определенный торговый центр. Как это сделать?",
            "Полиция может легко получить такую информацию от мобильных операторов, но что делать обычному человеку?",
            "Ответ – использовать Telegram и его функцию \"Люди рядом\" в сочетании с Python-скриптом.",
            "\"Люди рядом\": эта функция Telegram показывает контакты пользователей, находящихся поблизости, с примерным расстоянием до них (500 м, 1 км, 2 км и 3 км). Отображаются первые 100 ближайших контактов.",
            "Python-скрипт: с помощью библиотеки telethon можно получить доступ к этой информации и автоматизировать процесс сбора контактов.",
            "Установка:",
            "Скачайте и установите Python с официального сайта: https://www.python.org/downloads/",
            "Установите необходимые модули:",
            "Регистрация приложения Telegram:",
            "Зарегистрируйте свое приложение на сайте Telegram: https://core.telegram.org/api/obtaining_api_id",
            "Важно: используйте свой реальный номер телефона, привязанный к Telegram-аккаунту, а не бота.",
            "Создание скрипта:",
            "Создайте файл с расширением .py и вставьте код скрипта (https://pastebin.com/pYPA8PF0).",
            "Замените следующие значения:",
            "api_id = (ваш API ID)",
            "api_hash = (ваш API Hash)",
            "phone_number = '' (ваш номер телефона)",
            "Запустите скрипт.",
            "Выберите на карте нужное местоположение.",
            "Укажите радиус поиска (500, 1000, 2000 или 3000 метров).",
            "Нажмите кнопку \"Начать поиск\".",
            "Скрипт автоматически получит список пользователей Telegram, находящихся в заданном радиусе, и добавит их в ваши контакты.",
            "Данные обновляются Telegram каждые 30 минут.",
            "Отображаются только первые 100 пользователей.",
            "Поиск работает только в регионе, к которому привязан ваш номер телефона.",
            "Отображаются только пользователи со включенной видимостью. В среднем это 10-15%",
            "Важно использовать этот метод этично и уважительно по отношению к другим пользователям. Не рассылайте спам и не используйте полученную информацию в незаконных целях.",
            "Помните: эта информация предназначена только для ознакомления. Перед использованием подобных скриптов убедитесь, что вы не нарушаете правила Telegram и законодательство вашей страны."
        ]
    },
    {
        "Название статьи": "Как в Tele2 автоматизировали тестирование SAP ERP с помощью Python",
        "Дата публикации": "2024-06-05, 13:34",
        "Автор статьи": "a_valeeva ",
        "Статья целиком": [
            "Привет, Хабр! Меня зовут Анастасия Валеева, я – руководитель группы обеспечения качества в Tele2. Наша команда работает в большинстве своём с SAP ERP, и мы не понаслышке знаем, что автоматизация данной платформы — дело далеко не тривиальное. В этой статье я хочу поделиться с вами, как и зачем мы автоматизировали тестирование с помощью Python.",
            "SAP ERP – гибкий инструмент в руках нашей команды. Мы дорабатываем функциональность системы под потребности конкретного бизнес сегмента. Эти изменения производятся по запросу бизнес-пользователей. Объём и влияние доработок могут быть различными, но одно остаётся неизменным – каждая доработка является уникальной. Таким образом, это не простое устранение багов и улучшения текущего функционала, не изменение версионности продукта после оптимизации, а, как правило, абсолютно новый «продукт» в системе. В случае автоматизации функционального тестирования нам потребуется писать автотест на каждую доработку/разработку, что занимает больше времени, чем ручное тестирование (написание автотеста, отладка, оптимизация) + данный автотест с каждой новой разработкой будет уже неактуален, и нужно будет создавать новые и новые из раза в раз. Делаем выводы, что автоматизировать функциональные тексты для нас нерелевантно. А вот регрессионные тесты, которые мы проводим после каждого изменения системы, представляют собой более шаблонные варианты, шаги повторяются, и от их автоматизации есть профит.",
            "Сейчас мы работаем с SAP ERP и интегрированными продуктами (FileNet, BW, Fiori), однако, импортозамещение идёт полным ходом, и мы проводим пилотный проект по миграции на новую платформу. Так или иначе, созданный нами инструмент для автотестов универсален и может быть применён в работе с новой системой.",
            "Из множества инструментов автоматизации мы выбрали для ознакомления четыре наиболее совместимых с SAP ERP:",
            "SAP Scripting;",
            "Tricentis Tosca;",
            "eCatt;",
            "CBTA.",
            "Анализируя, мы исходили из трёх основных для нас факторов: скорость освоения, простота и гибкость, а также бюджет. По каждому из инструментов мы отметили свои плюсы и минусы, собрали информацию в единую таблицу. И вот что у нас получилось.",
            "По количеству зелёных блоков мы увидели, что нашим критериям в большей степени соответствует SAP Scripting.",
            "Принцип работы данного инструмента состоит в том, что он записывает все действия пользователя в системе, на выходе формирует файл в формате .vbs, который в последующем можно запускать в SAP. Соответственно, при запуске этого файла система будет повторять ваши предварительно записанные шаги. Кроме того, данный файл можно корректировать: удалять лишнее, дописывать недостающее или даже полностью переписать. Для этого необходимо открыть файл либо в блокноте, либо в любом другом редакторе, работающем с кодом.",
            "В процессе пилотирования SAP Scripting помимо технических вопросов мы решали несколько административных задач: удобство использования, гибкость, кастомизация, универсальность, прозрачность.",
            "Мы хотели внедрить такой инструмент, который будет полезен не только группе тестирования, но и другим смежным группам нашего подразделения. И поскольку мы говорим об автоматизации, одним из основополагающих факторов для нас было минимальное участие человека в этом процессе. Согласитесь, часто хочется просто нажать на волшебную кнопку \"РАБОТАТЬ\", чтобы оно всё само заработало :)",
            "Добиться данного магического эффекта «работает само» нам помог Python. За это отвечала коллега из моей команды — она написала скрипт для робота, который сейчас работает буквально по одному клику.",
            "Что касается прозрачности, то мы пошли по пути, доступному для любого пользователя. Для этого «прикрутили» Python к файлу Excel. Это означает, что сейчас провести регресс может любой сотрудник — достаточно зайти в файл автотеста и нажать кнопку «СТАРТ».",
            "Бизнес-процесс состоит из набора бизнес-операций. Например, создание логистического заказа состоит из заведения заказа, смены статуса подписания договора, деблокирования заказа и создания счёта-фактуры. Для обеспечения полного регрессионного тестирования мы автоматизируем всю цепочку шагов. На выходе получаем Excel-документ со скриншотами и подробной информацией по каждому шагу тестирования. Причём регресс может запустить любой пользователь, не только тестировщик, это доступно в том числе для менеджеров со стороны бизнеса. А полученные данные (скрипты) можно использовать также для генерации тестовых данных.",
            "Существует несколько способов выполнения автотестов.",
            "1. Отдельно по каждому бизнес-процессу. По каждому модулю финансовой системы SAP ERP создан файл Excel, в котором есть кнопка вызова макроса. По вызову этой кнопки запускается Visual Basic for Applications. VBA обращается к системе SAP и вызывает на выполнение ранее записанный скрипт vbs.Таким образом, мы можем выполнять тестирование по отдельному модулю или бизнес-операции.",
            "2. По всему модулю или нескольким модулям.Для этих нужд как раз используется Python. Наш робот обращается к SAP, открывая рабочее окно. Далее вызывает необходимые файлы Excel, которые работают по описанному принципу макросов на VBA. Таким образом, мы получаем следующую цепочку:",
            "При этом пользователю необходимо только единожды нажать кнопку ВЫПОЛНИТЬ.",
            "Запуск SAP GUI",
            "Заведение функции для чтения файла Excel",
            "Подключение к Excel",
            "На каждом листе в Excel есть подробная входная и выходная информация, при этом входную информацию можно корректировать. Большая часть листов связана между собой, чтобы можно было провести всю цепочку на одних данных, а последующие шаги не зависели от дополнительных действий пользователя.",
            "Все скриншоты, которые создаются в процессе регресса, генерируются вместе с документами и проводками. Лишние скриншоты можно удалить прямо на странице в Excel. При необходимости сотрудник может по номеру документа найти нужную проводку или операцию в SAP. Это является прозрачным и удобным способом анализа логов тестирования.",
            "Рядом с каждым шагом в файле появляется текстовое описание, статус «успешно» или «не успешно» пройден шаг и цветовой индикатор — зеленый означает успешно пройденный этап, красный сигнализирует об ошибках.",
            "Если ошибка является блокирующей для системы и дальнейшее прохождение шагов невозможно, то скрипт остановится, выдаст информационное сообщение и сохранит изменения в файл. Если ошибка не влияет на последующие шаги, то скрипт продолжит работу, а в конце выдаст лог в Excel с отображением корректных и некорректных шагов. При таком раскладе у нас появляется возможность увидеть проблему в моменте и исправить её.",
            "Также, завершение работы скрипта сопровождается звуковым оповещением.",
            "Дополнительно мы настроили автоматическое удаление листов из общей папки через три дня после их создания.",
            "Что в итоге",
            "Мы посчитали, сколько рабочего времени ручных тестировщиков мы экономим при использовании инструмента автоматизации. Получилось, что на один кейс при использовании SAP Scripting мы тратим 31 секунду против 148 секунд при ручном тестировании. Таким образом, 80% времени инженеров высвободилось на другие задачи, и мы смогли повысить эффективность тестирования.",
            "Данный вариант автоматизации является гибким к изменениям. В случае переезда на другую финансовую систему мы перенаправим нашего робота на Python на вызов нужной нам программы. Сейчас одна из наших основных задач – обеспечить качество работы текущего функционала и уже на этой надёжной основе реализовывать улучшения и внедрять новые фичи. Для нашей команды автоматизация тестирования SAP ERP стала интересным и полезным опытом, а бизнесу предоставила доступную, понятную и безотказную систему проверки рабочих процессов."
        ]
    },
    {
        "Название статьи": "Быстрый интерфейс, быстрый деплой",
        "Дата публикации": "2024-06-05, 11:01",
        "Автор статьи": "funtastick ",
        "Статья целиком": [
            "Салют! Не так давно создатели знаменитого pydantic выпустили новый фреймворк — FastUI, который позволяет создавать пользовательские интерфейсы с помощью декларативного кода на Python. В этой статье рассмотрим создание простого приложения и деплой его в Cloud Apps. ❯ Обзор По заявлению авторов фреймворка, фронтенду не нужно (и не следует) знать ничего о приложении, которое вы создаете, вместо этого он должен просто предоставить все компоненты, необходимые для создания интерфейса, а затем бэкенд может предоставить необходимые данные и параметры компонентов. Реализовано это таким образом, что FastUI инкапсулирует описание компонентов интерфейса и в виде классов, затем запускается простое React приложение, которое обращается к эндпоинтам за данными и компонентами. ❯ Пример Для примера давайте напишем простое приложение, предоставляющее информацию о городах из списка с возможностью пагинации. Данные для экспериментов любезно предоставили создатели фреймворка. Для начала опишем pydantic модель и функцию для чтения данных. from pydantic import BaseModel, Field, TypeAdapter import json from pathlib import Path class City(BaseModel): id: int city: str = Field(title=\"Name\") city_ascii: str = Field(title=\"City Ascii\") lat: float = Field(title=\"Latitude\") lng: float = Field(title=\"Longitude\") country: str = Field(title=\"Country\") iso2: str = Field(title=\"ISO2\") iso3: str = Field(title=\"ISO3\") admin_name: str = Field(title=\"Admin Name\") capital: str = Field(title=\"Capital\") population: float = Field(title=\"Population\") def cities_list() -> list[City]: cities_file = Path(__file__).parent / \"cities.json\" with open(cities_file, \"r\", encoding=\"utf-8\") as f: data = json.load(f) cities = [City(**city) for city in data] return cities Далее напишем каркас для нашего примера, с помощью FastAPI. Опишем два роута, первый возвращает необходимые компоненты и данные, а второй — простое React приложение, которое отвечает за запрос и отображение компонентов, полученных из предыдущего. from fastapi import FastAPI from fastapi.responses import HTMLResponse from fastui import AnyComponent, FastUI from fastui import components as c, prebuilt_html from fastui.components.display import DisplayLookup, DisplayMode from fastui.events import BackEvent, GoToEvent app = FastAPI() @app.get(\"/api/cities\", response_model=FastUI, response_model_exclude_none=True) def cities_view(page: int = 1, country=None): cities = cities_list() page_size = 10 # Количество записей в таблице, отображаемых на странице filter_form_initial = {} return c.Page( # Page - базовый контейнер для остальных компонентов components=[ c.Table( # Table - базовая разметка таблицы data=cities[(page - 1) * page_size : page * page_size], #Создаём срез данных для заполнения таблицы data_model=City, #Передаём модель данных columns=[ # Описываем столбцы таблицы DisplayLookup( #Указываем содержимое и размер столбца в процентах field=\"city\", table_width_percent=33 ), DisplayLookup(field=\"country\", table_width_percent=33), DisplayLookup(field=\"population\", table_width_percent=33), ], ), c.Pagination(page=page, page_size=page_size, total=len(cities)), #Кнопки для пагинации ] ) @app.get(\"/{path:path}\") async def html_landing() -> HTMLResponse: \"\"\"Простое React приложение, идёт последним, т.к. соответствует всем маршрутам\"\"\" return HTMLResponse(prebuilt_html(title=\"Большие города\")) Результат работы представлен на рисунке ниже: ❯ Деплой Для деплоя приложений на FastUI можно воспользоваться сервисом Apps, к сожалению рассмотренный фреймворк только набирает популярность, поэтому мы воспользуемся опцией: «деплой из Dockerfile». Для этого достаточно создать Dockerfile и разместить его в корне репозитория. FROM python:3.11 COPY . /app WORKDIR /app RUN pip install -r requirements.txt CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] EXPOSE 8000 Обратите внимание, что при отсутствии в Dockerfile параметра EXPOSE, APPS будет слушать порт 8080 контейнера. Далее достаточно предоставить сервису доступ к аккаунту на github. Затем остаётся следить за логами деплоя: В случае успешного развёртывания появиться удобный дашборд с графиками нагрузки на виртуальную машину: ❯ Заключение В данной статье мы рассмотрели лишь малую часть возможностей фреймворка, однако можно отметить, что FastUI предоставляет новый подход к созданию веб-приложений и позволяет существенно ускорить разработку. Возможно, захочется почитать и это: ➤ Timeweb Cloud CLI ➤ Бесплатный прокси к Docker Hub ➤ Фантастически быстрый деплой веб-приложения ➤ Учимся летать: симуляция эволюции на Rust ➤ Age of Empires – культовая попытка сделать Цивилизацию в реал-тайме Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩"
        ]
    },
    {
        "Название статьи": "Как я создавал аудиоплеер на python с FFmpeg",
        "Дата публикации": "2024-06-04, 18:10",
        "Автор статьи": "Niamorro ",
        "Статья целиком": [
            "Всех приветствую. Сегодня хочу поделиться опытом создания своего первого проекта на Python. Мой проект — это простой аудиоплеер, и я хочу рассказать, как я его создавал, с какими сложностями столкнулся и что из этого вышло.",
            "Выбор языка для первого проекта — это всегда непросто. Я выбрал Python по нескольким причинам:",
            "Простота синтаксиса. Python очень читабельный и понятный, что идеально подходит для новичков.",
            "Богатая стандартная библиотека и сообщество. Множество готовых решений и библиотек, которые можно использовать в своих проектах.",
            "Популярность в разработке. Python — один из самых популярных языков программирования, и навыки работы с ним будут полезны в будущем.",
            "Моя цель была написать простой аудиоплеер, который мог бы играть основные аудиоформаты. Я хотел, чтобы пользователь мог выбирать треки, ставить их на паузу и останавливать, так же изменять скорость проигрывания.",
            "Выбор библиотек занял действительно много времени, так как нужно было выбрать библиотеки которые обновляются, и имеют необходимый мне функционал. Я использовал несколько библиотек:",
            "PySide6: библиотека для создания интерфейсов созданная разработчиками Qt, имеет хорошую поддержку сообщества и регулярные обновления, в дополнение к ней использовал qdarktheme для стилизации интерфейса.",
            "FFmpeg: Универсальный инструмент для обработки видео и аудио.",
            "Sounddevice: Библиотека для воспроизведения и записи звука в Python.",
            "Mutagen: Библиотека для извлечения данных из аудиофайлов.",
            "Выбор файла:",
            "Пользователь выбирает аудиофайл из меню \"Файл\". Поддерживаемые форматы включают MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Выбранный файл передаётся в FFmpeg через подпроцесс для извлечения необработанных аудиоданных. Используемая команда:",
            "Чтение аудиоданных:Аудиоданные считываются блоками и сохраняются в массив NumPy для эффективной обработки.",
            "Регулировка громкости:Регулировка громкости осуществляется путём умножения аудиомассива на коэффициент громкости.",
            "Регулировка скорости воспроизведения:Скорость воспроизведения (например, 2x) управляется через библиотеку sounddevice путём изменения частоты дискретизации.",
            "Поток вывода:Обработанные аудиоданные передаются на аудиовыход через библиотеку sounddevice.",
            "Управление воспроизведением:Элементы управления, такие как воспроизведение/пауза, следующий/предыдущий трек и перемотка, обрабатываются через класс AudioTrigger.",
            "Воспроизведение/Пауза:Использует класс AudioTrigger для начала/остановки аудиопотока.",
            "Следующий/Предыдущий трек:Обновляет текущий индекс трека и загружает следующий/предыдущий трек в плейлисте.",
            "Перемотка:Регулирует позицию воспроизведения, пересчитывая индекс позиции на основе значения ползунка.",
            "Виджет очереди треков:Отображает добавленные ранее папки.",
            "Виджет плейлиста:Отображает содержимое папки.",
            "Виджет информации о треке:Показывает метаданные и обложку для воспроизводимого трека.",
            "Если хотите ознакомиться с исходным кодом или внести свой вклад в проект,",
            "приглашаю вас посетить страницу GitHub проекта. Там вы найдёте весь исходный код аудиоплеера.",
            "Также у проекта есть веб-сайт, где вы можете скачать готовые .exe и .deb пакеты для Windows и Linux. Здесь же доступна подробная документация по установке и использованию программы.",
            "Работа с FFmpeg требовала правильной организации буферизации аудиоданных, чтобы избежать прерываний и задержек при воспроизведении.",
            "Решение: Буферизация данных в массив NumPy.",
            "Треки воспроизводились с неправильной скоростью из-за некорректной частоты дискретизации.",
            "Решение: Я считываю частоту дискретизации трека и открываю аудиопоток с настройками именно для того трека, который в данный момент должен воспроизводиться.",
            "В результате я создал аудиоплеер с основными функциональными возможностями:",
            "Проигрывание аудиофайлов: Поддерживаются популярные форматы MP3, WAV, FLAC, OGG, M4A, AAC и WMA.",
            "Управление воспроизведением: Воспроизведение, пауза, остановка, перемотка, следующий/предыдущий трек.",
            "Регулировка скорости воспроизведения: Возможность воспроизводить треки быстрее или медленнее.",
            "Плейлист: Добавление папок с треками.",
            "Информация о треках: Отображение метаданных и обложек альбомов.",
            "Тёмная тема: Благодаря qdarktheme, аудиоплеер имеет современный и стильный интерфейс.",
            "Добавить поддержку потокового аудио: Возможность воспроизводить музыку из интернет-радиостанций и стриминговых сервисов.",
            "Расширить функциональность плейлиста: Добавить возможность создания и сохранения пользовательских плейлистов.",
            "Поддержка эквалайзера: Добавить эквалайзер для настройки звука.",
            "Создание аудиоплеера на Python оказалось полезным опытом. Я научился работать с аудио на низком уровне, обрабатывать потоки и создавать пользовательский интерфейс.",
            "Буду рад любым отзывам и предложениям по улучшению плеера. Спасибо за внимание!",
            "Ссылки:",
            "Исходный код на GitHub",
            "Сайт проекта с загрузкой пакетов"
        ]
    },
    {
        "Название статьи": "Как мониторинг связан с тестированием. Преимущества мониторинга для бизнеса: как экономить время и деньги",
        "Дата публикации": "2024-06-04, 15:59",
        "Автор статьи": "luffity ",
        "Статья целиком": [
            "Привет! Проходя множество собеседований, я не раз слышал вопросы по типу: «Что такое мониторинг?», «Как это связано с тестированием?», «Зачем это нужно?». Для меня, волей случая ставшего специалистом по мониторингу чуть больше года назад, это тривиальные вопросы, однако многие компании либо не знают, что это такое, либо не видят в этом пользы. На одном из последних интервью я услышал интересное мнение от QA Lead о том, что assert должен быть в каждом тесте. Смелое заявление, подумал я. Поэтому, собственно, вы и читаете эту статью.Разберёмся, что такое мониторинг и с чем его едят. А главное, зачем он нужен вообще.",
            "Думаю, начать с небольшого введения обо мне будет наиболее верным для погружения в тему. Сейчас я занимаю должность middle SDET в ООО «МИТ» (если проще, то DIXY Group). Попал я туда как AQA, причём единственным. Я находился в группе по мониторингу корпоративных сервисов, соответственно, кроме меня там были только специалист по мониторингу (мой Lead), DevOps и системный админ. Похоже на стандартное начало анекдота…",
            "Моим заданием на испытательный срок стало написание сервиса по мониторингу интернет-соединения на торговых точках компании. И тут я подумал: какой, блин, мониторинг? С другой стороны, работа на дороге не валяется, тем более настолько приятная. Дело было вечером, делать было нечего…",
            "По итогу этого задания ко мне пришло осознание того, как тесты могут трансформироваться в нечто большее, чем проверки, прогоняемые раз в релиз. Они могут быть целой экосистемой, даже можно сказать: «Глазом Бога» (кто понял отсылку к Форсажу, спасибо).",
            "Начнём мы, конечно же, с поверхности и определим для себя, что такое мониторинг и как он связан с тестированием. В самом распространённом смысле под тестированием понимают сверку ожидаемого и фактического результатов на конечном наборе тестов. Если мы берём стандартные парадигмы тестирования, то такие проверки могут выполняться при добавлении новой фичи, раз в спринт, при релизе и много когда ещё. Однако все эти тестовые прогоны не преследуют цель отслеживать состояние продукта (программного или бизнесового) постоянно.",
            "Как раз здесь и вступает в игру мониторинг. Если о-о-очень грубо сказать, то это, по сути, те же наборы тестов, только несущие цель постоянного наблюдения за состоянием продукта. Но тут стоит уточнить, что это не тестовые кейсы в их привычном понимании.",
            "Думаю, будет проще понять наглядно.",
            "Это всем понятный простой тест. Мы переходим на страницу и ожидаем заголовок. Как можно заметить, для того чтобы исполнить свою главную функцию – сопоставить ожидаемый и фактический результаты, тест содержит в себе assert. Это несомненно верный подход к написанию тестов, так как это позволит более точно валидировать ошибку, а также правильно отобразить её в отчётах, например Allure.",
            "А теперь взглянем на код скрипта мониторинга, который проверяет доступность ресурса.",
            "Сразу бросается в глаза отсутствие assert. Но в таком случае, как такой скрипт вообще можно считать информативным, если он не выводит ошибки? Именно поэтому мы добавим дополнительное действие. Например, найдём какую-то кнопку и нажмём на неё. Теперь, если ресурс не прогрузился или сломался, мы получим TimeoutException и сообщение о том, что именно скрипт не смог сделать.",
            "Возникает вопрос: почему бы тогда точно так же не поставить assert и не ждать лишнее время для выпадения TimeoutException ? Справедливо! Однако возьмём во внимание, что данный скрипт не нацелен на то, чтобы просто проверить доступность ресурса и отследить ошибку в отчёте. Если мы предполагаем, что скрипт гоняется бесконечно, пока смерть сервера не разлучит вас, то отчётом в данном случае будет не Allure, например (хотя я и его прикрутил к скриптам для Project Manager’а), а сервисы для графического отображения типа Grafana или сервисы мониторинга типа Prometheus. Да и сам скрипт, помимо успеха или провала теста, должен собирать ещё кучу полезных данных. В данном примере это может быть время прохождения скрипта, что может дать нам представление о том, в каком состоянии находится сервис. Особенно если учесть, что всегда можно настроить параметры интернет-соединения или любые другие моменты, имитирующие пользователя. И тут мы плавно перейдём к другому вопросу.",
            "Теперь стоит сказать и о том, что мониторинг может быть как на микро-, так и на макроуровне. Под микроуровнем обычно понимают низкоуровневый мониторинг, например, физического оборудования, элемента большого сервиса или что-то подобное. На макроуровне мониторинг предстает как UX-тестирование или тестирование пользовательских путей.",
            "Немного про микроуровень. Вернёмся к проекту по мониторингу интернет-соединения на торговых точках компании. По сути, скрипт достаёт из базы данных ID магазинов, конвертирует их в IP-адреса маршрутизаторов в магазинах и пингует их несколько раз. Помимо того, что в таблице в Grafana этот скрипт отображает «Up» или «Down» в зависимости от доступности каналов, он также собирает время отклика, хранит историю падений и содержит в себе данные об операторе SIM-карты, номере телефона и многое другое. Не очень-то похоже на тест.",
            "Теперь про макроуровень. Высокоуровневый мониторинг уже больше похож на UI/UX-тестирование. В его основе лежит постоянное отслеживание пользовательского пути через UI. Например, для сайта доставки продуктов — от захода пользователя на сайт и выбора товаров до оплаты. Помимо прочего, такой скрипт также собирает множество данных.",
            "В чём, собственно, разница? Основными критериями, отличающими мониторинг от тестирования, являются:",
            "Цель в постоянном наблюдении. Мониторинг — большой брат для ваших сервисов, который безустанно следит за ними;",
            "Сбор данных. Помимо отчётов о тестировании, мониторинг собирает ещё кучу данных;",
            "Быстрое реагирование. Думаю, тут и пояснять не надо. Если у вас есть тестовый сервер или синтетика, то критической баге будет сложно пролезть в прод;",
            "Имитация пользователя. Хоть UX-тесты и тесты пользовательских путей позволяют имитировать действия пользователя, но пишут их далеко не в первую очередь (информация со 100+ собеседований. Всем API подавай, а на пользователей мы кладём...).",
            "Что в итоге польза? Ну, тут я расскажу лучше пару «До и После» примеров.",
            "До разработки мною сервиса мониторинга интернет-соединения на валидацию проблем и выезд оперативной группы на точку уходило два-три рабочих дня. Более того, часто это были ложные вызовы, так как при неработающем основном канале включался резервный. Мониторинг позволил проходить весь процесс за два часа. Процент ложных вызовов за год его работы составляет не более 0,2%. А уж сколько денег это экономит компании, говорить не приходится, если учитывать, что к этому мониторингу подключена вся первая линия поддержки. Во всех магазинах Дикси. По всей России. Даже не думал, что час простоя торговой точки может стоить так много…",
            "А как вам такая новость в ленте: «Основной сайт и сайт доставки магазина Дикси не работают!»? Именно такую новость прочло руководство компании, заваривая утренний кофе. Да, узнавать о падении основных сервисов компании из новостей — это, видимо, не весело. Мне кажется, кофе точно не полезет после такого. Стоит ли говорить, что после этого случая мониторинг был внедрён на все сервисы?",
            "Забавно, правда?",
            "Но остался ещё один вопрос. Специалист по мониторингу и специалист по тестированию — это один и тот же профессионал? Мне кажется, специалист по мониторингу ближе к SDET, чем к AQA. Всё-таки я считаю, что автоматизатор тестирования должен знать и уметь меньше. AQA как бы и должен иметь представление о контейнеризации, но как бы и просто собрать контейнер в Docker достаточно. Специалист по мониторингу должен бы и под каждый свой мониторинг собрать контейнер, и доставить его, и обслужить если что, и k8s знать бы по-хорошему, ноды и воркеры – лучшие друзья. И опять-таки, ты же не знаешь, что может быть важно для бизнеса. Возможно, придётся выйти за рамки PyTest, Selenium и Appium. Уметь разобраться в различных библиотеках, знать асинхронные подходы, парадигмы проектирования, сильные и слабые стороны рабочего языка программирования — всё это важные навыки специалиста по мониторингу. Так что да, SDET более подходящее описание для его деятельности.",
            "Ссылочка на телегу"
        ]
    },
    {
        "Название статьи": "Кратко про Seq2Seq-модели",
        "Дата публикации": "2024-06-04, 09:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Seq2Seq модели — это архитектуры ML, предназначенные для задач, связанных с последовательными данными, типо машинного перевода, суммирования текста, создания описаний к пикчам и прочие задачи, где требуется преобразование одной последовательности в другую.",
            "В этой статье в общих деталях рассмотрим то, как реализуются Seq2Seq модели.",
            "Seq2Seq модели состоят из двух основных частей: энкодера и декодера.",
            "Энкодер преобразует входную последовательность в контекстный вектор, содержащий обобщённое представление всей входной информации. Этот вектор затем используется декодером для генерации выходной последовательности, о декодере чуть ниже.",
            "Перед тем, как подать данные в энкодер, текстовые данные преобразуются в числовые представления с помощью эмбеддинга. Это делается с помощью слоя Embedding, который преобразует каждый токен во входной последовательности в вектор фиксированной размерности. Например, слово milk может быть представлено как вектор размерности 300.",
            "Основу энкодера RNN, обычно реализованные с использованием LSTM или GRU. Эти сети обрабатывают входную последовательность пошагово:",
            "На каждом шаге RNN принимает эмбеддинговое представление текущего токена и скрытое состояние от предыдущего шага.",
            "Выход каждого шага включает новое скрытое состояние, которое передаётся на следующий шаг вместе со следующим токеном.",
            "В конце последовательности RNN генерирует контекстный вектор, который является финальным скрытым состоянием. Этот вектор обобщает всю информацию из входной последовательности и передаётся в декодер для дальнейшей генерации выходной последовательности. Контекстный вектор — это своего рода сжатая версия входной последовательности, включающая в себя её смысл.",
            "Для улучшения качества представления входной последовательности часто используются двунаправленные RNN. В этом случае два RNN работают параллельно: один — слева направо, другой — справа налево. Их состояния объединяются на каждом шаге, что позволяет учитывать как предшествующие, так и последующие слова для каждого токена в последовательности.",
            "Пример реализации энкодера на Keras с LSTM:",
            "Здесь входные данные сначала проходят через эмбеддинговый слой, который преобразует их в векторы фиксированной размерности. Затем эти векторы подаются в LSTM, который на выходе даёт финальные скрытые состояния, использующиеся в качестве контекстного вектора.",
            "В отличие от энкодера, декодер генерирует данные на основе предыдущих предсказаний и контекстного вектора, предоставленного энкодером.",
            "Подобно энкодеру, декодер принимает токены, которые сначала преобразуются в числовые представления с помощью эмбеддинга. Однако, в случае декодера на вход подаются не только реальные данные, но и предсказанные токены на предыдущих шагах.",
            "Декодер также реализован с использованием RNN, как LSTM или GRU. На каждом шаге декодер принимает:",
            "Контекстный вектор от энкодера.",
            "Предыдущий предсказанный токен (или начальный токен для первого шага).",
            "Скрытое состояние от предыдущего шага декодера. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности.",
            "На каждом шаге RNN декодера производит новое скрытое состояние и выходной вектор. Этот выходной вектор затем преобразуется в вероятности через слой Softmax, который указывает на вероятность каждого возможного токена в выходной последовательности",
            "Для улучшения качества генерации используется механизм внимания, который позволяет декодеру фокусироваться на различных частях входной последовательности на каждом шаге генерации выходной последовательности. Механизм внимания вычисляет веса для каждого состояния энкодера, определяя важность каждого токена входной последовательности в текущий момент времени.",
            "Пример реализации декодера на Keras с LSTM и механизмом внимания:",
            "Декодер принимает начальные состояния от энкодера и генерирует выходную последовательность.",
            "Машинный перевод — это одна из наиболее базовых задач для Seq2Seq моделей. Реализуем Seq2Seq модельку для перевода с английского на французский язык с использованием Keras.",
            "Для этой задачи будем использовать датасеты французских и английских предложений (они есть на kaggle):",
            "Следующий пример - текстовое суммирование. Это задача генерации краткого представления текста. Реализуем Seq2Seq модель с использованием механизма внимания.",
            "Для этой задачи будем использовать датасет новостей, где заголовок является суммарным представлением статьи:",
            "Реализуем генерацию описаний к изображениям — это задача, где Seq2Seq модели используются для генерации текста, описывающего содержание изображения. Будем использовать предобученную модель InceptionV3 для экстракции признаков изображения и Seq2Seq модельку для генерации текста:",
            "Seq2Seq модели - это очень мощный инструмент для решения задач, связанных с последовательными данными. Они позволяют преобразовывать входные последовательности в выходные с высокой точностью, в особенности при использовании механизмов внимания (об этом не забываем).",
            "В завершение хочу порекомендовать бесплатные вебинары курса ML Advanced:",
            "Современные модели прогнозирования типа TimesNet и TimeGPT",
            "H2O, TPOT, Autokeras - а вы что, за меня и модели строить будете?"
        ]
    },
    {
        "Название статьи": "Как подружить Llama-3 и YouTube имея всего 40 строк кода",
        "Дата публикации": "2024-06-03, 21:57",
        "Автор статьи": "evg_dc ",
        "Статья целиком": [
            "Сделаем Телеграм бота которому можно кинуть ссылку на YouTube видео и поговорить с ним о содержимом этого видео.",
            "За основу возьмем бота работающего на Llama 3-70b из моей прошлой статьи. Можно использовать и любую другую языковую модель включая ChatGPT или локальную запущенную на Ollama.",
            "Создать Телеграм бота и получить его токен (как это сделать, смотрите инструкции на просторах интернета, например здесь).",
            "Зарегистрироваться на Groq и получить api key (нужен VPN).Заходим по этой ссылке, регистрируем аккаунт, генерим ключ. Платежная карта не требуется.",
            "Замените в коде GROQ_API_KEY на api ключ полученный в Groq и TELEGRAM_BOT_TOKEN на токен телеграм бота, все должно быть в кавычках.",
            "После получения сообщения от пользователя ищем в тексте сообщения ссылку на YouTube видео. Делаем это перебирая все слова и проверяя их на наличие URL. Если ссылка на видео найдена, используя библиотеку youtube-transcript-api забираем транскрипцию.",
            "Далее, полученную транскрипцию подставляем языковой модели в виде сообщения от функции. Здесь мы немного обманываем модель, потому что такой функции нет, но лучше делать так чем ставить транскрипцию в системное сообщение. Модель заточена под работу с функциями и все правильно поймет.",
            "Как и в предыдущей версии, бот будет запоминать последние 6 сообщений и поддерживать диалог.",
            "Запускаем скрипт и в Телеграм боте задаем вопрос со ссылкой на видео:",
            "Посмотреть как это работает можно в YouTubeGPT.",
            "Еще есть НашGPT - это как ChatGPT только модель Llama 3-70b."
        ]
    },
    {
        "Название статьи": "Python в Excel жив?",
        "Дата публикации": "2024-06-03, 17:24",
        "Автор статьи": "Gonchar_POTT ",
        "Статья целиком": [
            "Уже больше месяца экспериментирую, исследую, как разные схемы (паттерны) осознанного дыхания влияют на вариабельность сердечного ритма (Heart Rate Variability, HRV на чуждом языке). В скромной, но не совсем уж крошечной Excel-таблице со столбцами “Паттерн”, “HRV”, “Пульс” набралось 258 записей и мне понадобилось выбрать победителя -- дыхательный паттерн, дающий на выходе максимальное значение HRV. Не вручную же сортировать эти записи!",
            "Да, я знаю: есть Pivot Table & Power Query. Но Pivot Table мне не по душе необходимостью после каждого изменения таблицы-источника делать REFRESH, во-первых, избыточной сложностью создания, во-вторых. И просто так не нравятся Pivot Table, что главное. Что же касается Power Query, то сочетание слов вызывает у меня трепет и учащенное сердцебиение: не понимаю, что это за зверь такой и насколько он страшный или полезный.",
            "Поэтому для подсчета результатов -- и выбора победителя -- применил относительно недавно появившуюся в Excel функцию GROUPBY в связке с XLOOKUP. И, раз уж пришлось группировать данные, решил сравнить нативные функции Excel с GROUPBY от Pandas (мы ведь помним, что сейчас Python можно запустить внутри Excel).",
            "Написал простой код:",
            "Поместил код через =PY( в ячейку А1 Excel",
            "И он прекрасно справился с задачей и выдал таблицу с результатами:",
            "breathing_pattern",
            "HRV",
            "HR",
            "8",
            "physiological sighs moderate",
            "59",
            "65",
            "7",
            "physiological sighs light",
            "57",
            "62",
            "1",
            "4.4-6.6",
            "56",
            "59",
            "6",
            "following pulse",
            "55",
            "61",
            "0",
            "4.2-0-6.4-0",
            "53",
            "62",
            "3",
            "6-6",
            "53",
            "61",
            "4",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "61",
            "2",
            "5-5",
            "52",
            "63",
            "5",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "Комбинация функций GROUPBY и XLOOKUP тоже отработала без изъянов:",
            "breathing_pattern",
            "HRV",
            "HR",
            "physiological sighs moderate",
            "59",
            "65",
            "physiological sighs light",
            "57",
            "63",
            "4.4-6.6",
            "56",
            "60",
            "following pulse",
            "55",
            "61",
            "4.2-0-6.4-0",
            "54",
            "63",
            "6-6",
            "54",
            "61",
            "calming breathing: inhale through nose, slow exhale through mouth",
            "53",
            "62",
            "5-5",
            "53",
            "63",
            "count: 4 inhale nose, 6 exhale mouth",
            "52",
            "63",
            "* Для внимательных: разница в данных между двумя таблицами -- плод Python-овского округления до целых чисел.",
            "* В “нативном” подходе нет отсечки паттернов с количеством замеров менее шести.",
            "Выводы и наблюдения по теме:",
            "В Python итоговая таблица сама автоматически изменяет размеры при добавлении новых паттернов. GROUPBY by Excel ведет себя так же, а вот связка GROUPBY&XLOOKUP уже потребует редактирования формул: нужно изменять адреса диапазонов ячеек, к которым обращается XLOOKUP.",
            "Вопреки большему размеру Python-код мне кажется проще и для написания, и для чтения-понимания. Хотя писать код в ячейке Excel -- весьма извращенное удовольствие.",
            "Исполнение кода Python требует интернет-соединения.",
            "Выводы и наблюдения не совсем по теме:",
            "Для меня лично схема дыхания “physiological sighs light” (легкие физиогические вздохи) -- оптимальный выбор.",
            "Более шести месяцев я придумывал, зачем мне может понадобиться живущий в Excel Python и наконец нашел.",
            "Буду благодарен за советы и критику. Постараюсь ответить на вопросы."
        ]
    },
    {
        "Название статьи": "Майним крипто-коины с помощью Python и компьютерного зрения",
        "Дата публикации": "2024-06-03, 13:58",
        "Автор статьи": "temabed ",
        "Статья целиком": [
            "После внезапного обогащения энтузиастов, которые поиграли в начале года в приложение Notcoin в телеграм, подобные проекты стали расти как грибы. Да и грибников заметно поприбавилось. Но в данной статье мы не будем касаться тем блокчейна или финансов, а рассмотрим простой пример применения компьютерного зрения для фарма поинтов в самом популярном, после Notcoin, проекте - хомяке комбате. Название явно на что-то намекает, но да ладно.",
            "Это не первый проект, который я автоматизирую, и не самый нуждающийся в этом. Да и без компьютерного зрения с автоматизацией хомяка можно спокойно обойтись. Но с ним, во-первых, интереснее, а во-вторых - это просто хороший пример с минимумом строк кода для демонстрации возможностей библиотеки cv2. Статья, соответственно, предназначена для энтузиастов и начинающих специалистов.",
            "Начнем с того, что мы установим все необходимые зависимости и импортируем их в свой проект. Вот они, слева направо.",
            "С помощью pyautogui наш бот будет управлять мышью. Keyboard пригодится для назначения горячих клавиш, чтобы управлять работой бота. cv2 наградит бота зрением, пусть и компьютерным, с помощью которого тот будет находить совпадения с искомым изображением. А numpy пригодится для работы с большими массивами, но тут он почти для галочки, не бойтесь. Модуль time тоже понадобится, чтобы ставить таймауты в работе программы.",
            "Далее напишем небольшую конструкцию- переключатель.",
            "Функция change при вызове всего лишь меняет значение переменной work, которая будет использована в бесконечном цикле. И если work будет False, работа нашего кода будет останавливаться. И наоборот запускаться, в противоположном случае.",
            "Кстати, забыл упомянуть, что разработчики проекта, над которым мы сейчас проводим эксперимент, большие молодцы, и убрали возможность пользоваться приложением на десктопных устройствах. Поэтому для его запуска понадобится эмулятор Android.",
            "Теперь определим основную логику работы бота:",
            "Он ищет совпадение с изображением полностью заполненной энергии.",
            "Если находит совпадение, ищет изображение монеты и кликает на неё энное количество раз.",
            "И всё это работает в бесконечном цикле.",
            "Значит со скриншота приложения необходимо вырезать две области, которые помечены красным, и разместить их в отдельные файлы, конечно же.",
            "Теперь напишем функцию для кликов по монетке. Она будет принимать путь к исходному изображению, а так же порог чувствительности для компьютерного зрения и интервал (таймаут) в секундах.",
            "Переменной template будет присвоено исходное изображение монетки, но в оттенках серого, так как мы указали в параметрах 0. Это необходимость, так как в оттенках серого компьютер зрит лучше. Сразу вычисляем высоту и ширину исходника, и присваиваем переменным. А далее по ходу исполнения кода он делает скриншот, сравнивает с исходником, получает координаты области с совпадением, и делает 260 даблкликов по ней. Координаты я ищу немного кривовато и в итоге loc содержит большой массив, из которого я использую лишь самые первые координаты, после чего цикл прерываю. Но лучше сделать не смог, извините.",
            "А теперь напишем аналогичную функцию, но с задачей искать совпадение с картинкой полной энергии, после чего вызывать функцию click.",
            "В целом всё аналогично. Добавил лишь ожидание горячей клавиши, чтобы можно было остановить программу в любое время нажатием на тильду (Ё). Ну и для красоты заключил в try-except.",
            "И это всё. Пишем последние строки и запускаем скрипт (не забыв нажать на Ё для запуска логики в цикле).",
            "По сути, этот код многофункционален, и его без труда, с минимальными изменениями, можно переделать под любые другие задачи. На всякий случай оставлю и полную версию кода:",
            "Не судите меня строго по этому скрипту, большую часть жизни я вообще бегал с пистолетиком, и пристрастился к разработке сравнительно недавно, поэтому всего лишь юный падаван в возрасте. Хотя могу писать и большие скучные штуки, но вот писать о них получится дольше, чем сам код. А если будут вопросы- добро пожаловать в телеграм. У меня там небольшой клуб по интересам."
        ]
    },
    {
        "Название статьи": "Сравниваем популярные алгоритмы кластеризации DBSCAN и OPTICS",
        "Дата публикации": "2024-06-03, 13:51",
        "Автор статьи": "evaclick ",
        "Статья целиком": [
            "Привет, Хабр)",
            "Поговорим сегодня о 2 популярных алгоритмах кластеризации — DBSCAN и OPTICS, посмотрим их особенности и сравним",
            "Поехали!",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Ну, DBSCAN в особом в представлении не нуждается, всё-таки один из самых популярных алгоритмов кластеризации. Поэтому по минимуму теории.",
            "DBSCAN (Density-based spatial clustering of applications with noise, плотностной алгоритм пространственной кластеризации с присутствием шума), как следует из названия, оперирует плотностью данных. На вход он просит матрицу близости точек и два параметра — радиус -окрестности и количество соседей .",
            "Эпсилон-окрестность для любого вектора в метрическом признаковом пространстве определяется как множество точек, отстоящих от не более чем на :",
            "где — выбранная метрика (например, евклидовое расстояние).",
            "В общих чертах, алгоритм DBSCAN можно представить как последовательность следующих этапов:",
            "найти новые точки в -окрестности каждой точки и определить основные точки с более чем соседями.",
            "найти связные компоненты основных точек на графе соседей, игнорируя все неосновные точки.",
            "назначить каждую неосновную точку ближайшему кластеру, если кластер является -соседним, в противном случае считаем точку шумом.",
            "Вот так можно использовать DBSCAN из Sci-Kit Learn + с интерактивными ползунками, работает в Colab'е (в Jupyter Notebook какие-то траблы с этим, если кто знает — please, help):",
            "С использованием DBSCAN в Julia и R особых проблем тоже не возникает —",
            "— Julia:",
            "— R:",
            "В идеальном случае DBSCAN может иметь линейную сложность , но не стоит особо на это рассчитывать. Если не пересчитывать каждый раз точек, то ожидаемая сложность — . Худший случай (плохие данные или брутфорс-реализация) — . Наивные реализации DBSCAN любят отъедать памяти под матрицу расстояний — это явно избыточно. Многие версии DBSCAN умеют работать и с более щадящими структурами данных: sklearn и R реализации можно оптимизировать при помощи KD-tree прямо из коробки.",
            "DBSCAN не вычисляет самостоятельно центры кластеров, однако вряд ли это проблема, особенно учитывая произвольную форму кластеров. Зато DBSCAN автоматически определяет выбросы, что довольно здорово.",
            "Соотношение , где — размерность пространства, можно интуитивно рассматривать как пороговую плотность точек данных в области пространства. Ожидаемо, что при одинаковом соотношении , и результаты будут примерно одинаковы. Иногда это действительно так, но есть причина, почему алгоритму нужно задать два параметра, а не один. Во-первых типичное расстояние между точками в разных датасетах разное — явно задавать радиус приходится всегда. Во-вторых, играют роль неоднородности датасета. Чем больше и , тем больше алгоритм склонен «прощать» вариации плотности в кластерах. С одной стороны, это может быть полезно: неприятно увидеть в кластере «дырки», где просто не хватило данных. С другой стороны, это вредно, когда между кластерами нет чёткой границы или шум создаёт «мост» между скоплениями. Тогда DBSCAN запросто соединит две разные группы. В балансе этих параметров и кроется сложность применения DBSCAN: реальные наборы данных содержат кластеры разной плотности с границами разной степени размытости. В условиях, когда плотность некоторых границ между кластерами больше или равна плотности каких-то обособленных кластеров, приходится чем-то жертвовать.",
            "Существуют варианты DBSCAN, способные смягчить эту проблему. Идея состоит в подстраивании в разных областях по ходу работы алгоритма. К сожалению, возрастает количество параметров алгоритма.",
            "Ок, теперь давайте немного поговорим о плюсах и минусах DBSCAN.",
            "Плюсы DBSCAN",
            "• DBSCAN не требует указания числа кластеров в отличие, скажем, от метода k-средних",
            "• DBSCAN может найти кластеры произвольной формы. Он может найти даже кластеры полностью окружённые (но не связанные с) другими кластерами.",
            "• DBSCAN имеет понятие шума и устойчив к выбросам.",
            "• DBSCAN требует лишь двух параметров ( и ) и большей частью нечувствителен к порядку точек в датасете. Однако, точки, находящиеся на границе двух различных кластеров могут оказаться в другом кластере, если изменить порядок точек, а назначение кластеров единственно с точностью до изоморфизма.",
            "Проблемы DBSCAN",
            "• DBSCAN не полностью однозначен — краевые точки, которые могут быть достигнуты из более чем одного кластера, могут принадлежать любому из этих кластеров, что зависит от порядка просмотра точек (тут стоит сказать, что существует DBSCAN❋, который трактует краевые точки как шум и тем самым достигается полностью однозначный результат)",
            "• Качество DBSCAN зависит от способа измерения расстояния. Наиболее часто используемой метрикой расстояний является евклидова метрика. В случае кластеризации данных высокой размерности эта метрика может оказаться почти бесполезной, что делает трудным делом нахождение подходящего значения . Этот эффект, однако, присутствует в любом другом алгоритме, основанном на евклидовом расстоянии.",
            "• DBSCAN не может хорошо разделить на кластеры наборы данных с большой разницей в плотности, поскольку не удается выбрать приемлемую для всех кластеров комбинацию и .",
            "Что ж, теперь давайте теперь переключимся на алгоритм OPTICS (Ordering Points To Identify the Clustering Structure).",
            "Основная идея OPTICS похожа на DBSCAN, но алгоритм предназначен для избавления от одной из главных слабостей алгоритма DBSCAN — проблемы обнаружения кластеров в данных, имеющих различные плотности. Для этого используется граф достижимости, который определяет достижимое расстояние для каждой точки, которая в дальнейшем будет относиться к ближайшему кластеру. Такой подход позволяет ещё лучше определять кластеры разной плотности, особенно если они расположены близко друг к другу, однако это увеличивает время работы алгоритма.",
            "Реализация OPTICS есть в библиотеке Sci-Kit Learn; вот как можно её импортировать и использовать:",
            "С R тоже проблем нет:",
            "Хорошо, давайте немного об особенностях OPTICS",
            "Плюсы OPTICS:",
            "Устойчивость к шуму (впрочем как и у DBSCAN): OPTICS способен обрабатывать данные с шумом и выбросами.",
            "Способность обнаруживать кластеры любой формы",
            "Не требует заранее заданного числа кластеров",
            "Проблемы OPTICS:",
            "Не всегда эффективен для плотных кластеров: OPTICS может иметь проблемы с эффективным обнаружением плотных кластеров, особенно если они имеют сложные формы.",
            "А вот несколько сфер, где регулярно используется OPTICS:",
            "Анализ сетей и обнаружение аномалий: OPTICS используется для анализа социальных сетей, транспортных сетей и других сетевых структур для выявления кластеров и аномалий.",
            "Биоинформатика: OPTICS применяется в биоинформатике для кластеризации геномных данных, выявления генных паттернов и классификации биологических образцов.",
            "Медицинская диагностика: OPTICS может быть применен для кластеризации медицинских данных, таких как результаты тестов, симптомы пациентов и история заболеваний, с целью выявления паттернов заболеваний или групп пациентов схожего профиля. .",
            "Итак, пришло время сравнить DBSCAN и OPTICS",
            "Вот DBSCAN:",
            "...а вот и OPTICS:",
            "И давайте возьмём для начала , , потом поменяем.",
            "Что мы видим? Для данного датасета DBSCAN выделяет кластеры более логичным и понятным способов, но в кластеризации OPTICS тоже есть пара интересных моментов. Как можно увидеть, точки вокруг главных кластеров DBSCAN безнадёжно отмечает как шум, в то время как OPTICS пытается нащупать кластеры и среди этих точек тоже. Это одна из главных фишек OPTICS — метод способен видеть кластеры разной плотности одновременно за счёт того, что он менее чувствителен к параметру .",
            "Вот довольно показательный пример — и тут OPTICS тоже выделил кластер в точках, которые забраковал DBSCAN:",
            "DBSCAN",
            "OPTICS",
            "Время выполнения DBSCAN в худшем случае составляет , где — количество точек данных. Однако при использовании индексов пространственного поиска (например, KD-деревьев или R-деревьев) производительность может быть улучшена до в среднем случае.",
            "Оптимизированная версия OPTICS также имеет временную сложность при использовании индексов пространственного поиска. Однако из-за необходимости построения упорядоченного представления данных (reachability plot) алгоритм может быть медленнее в реальных сценариях.",
            "DBSCAN проще в реализации. Он требует настройки двух параметров: (радиус поиска соседей) и (минимальное количество точек для формирования кластера).",
            "OPTICS сложнее в реализации, так как включает дополнительный шаг упорядочивания точек по достижимости (reachability). Он также использует параметры и , но результат не так чувствителен к выбору , что упрощает настройку.",
            "DBSCAN хорошо подходит для кластеризации данных с четко определенными плотными областями и шумом. Широко используется в различных областях, таких как географические информационные системы (ГИС) и анализ социальных сетей.",
            "OPTICS предпочтителен при необходимости анализа кластерной структуры данных на различных масштабах плотности. Подходит для исследования данных, где кластеры имеют различные плотности.",
            "Способен распознавать кластеры произвольной формы и размерности. Однако может не справляться с кластерами переменной плотности, так как использует фиксированное значение .",
            "Более гибок в отношении кластеров переменной плотности. За счет упорядочения точек по достижимости алгоритм может выявлять кластеры на разных уровнях плотности.",
            "Эффективно идентифицирует и отбрасывает шум и выбросы.",
            "Также эффективно справляется с шумом, но благодаря дополнительной информации о плотности позволяет лучше различать шум и кластеры.",
            "Требует настройки двух параметров, которые могут существенно влиять на результаты. Неправильный выбор может привести к объединению или разделению кластеров.",
            "Менее чувствителен к параметру ε. Основной параметр оказывает влияние на результаты, но не так критично, как в DBSCAN.",
            "Результаты могут быть непосредственно визуализированы как кластеры и шумовые точки.",
            "Результаты визуализируются с помощью графика достижимости (reachability plot), который может быть использован для определения кластера на различных уровнях плотности.",
            "Описание алгоритма DBSCAN от Sci-Kit Learn",
            "Описание алгоритма OPTICS от Sci-Kit Learn",
            "Наглядная визуализация DBSCAN",
            "Что ж, надеюсь, статья была полезной)",
            "Кстати, я веду телеграм-канал по ML, в котором описываю интересные фреймворки, библиотеки, open-source инструменты и не только Вероятно, там вы сможете найти что-то полезное для себя, так что welcome)"
        ]
    },
    {
        "Название статьи": "Реализация принципа единственной ответственности на Python",
        "Дата публикации": "2024-06-03, 07:15",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Сегодня мы рассмотрим одну из основополагающих концепций SOLID-принципов — принцип единственной ответственности или сокращенно - SRP. Разберем, что такое SRP и как правильно его применять в Python.",
            "Принцип единственной ответственности гласит, что каждый класс, метод или модуль должен иметь только одну причину для изменения. Проще говоря, каждый компонент вашей системы должен отвечать только за одну функциональность. Т.е если вам нужно внести изменение, связанное с этой функциональностью, вам придется изменить только один компонент.",
            "Когда каждый класс или модуль выполняет одну четко определенную задачу, становится гораздо проще понять его назначение и взаимодействие с другими частями системы.",
            "Что будет, если не соблюдать SRP?",
            "Если класс или модуль берет на себя несколько обязанностей, это приводит к увеличению сложности кода. Такой код сложнее читать, понимать и поддерживать. Также, когда один класс выполняет несколько задач, изменение в одной из них может непредсказуемо повлиять на другие.",
            "Классы, которые нарушают SRP, обычно плохо масштабируются и трудно переиспользуются. Их невозможно легко адаптировать для других целей или проектов.",
            "Для начала рассмотрим класс, который нарушает принцип единственной ответственности. Представим себе класс UserManager, который одновременно отвечает за создание юзера, валидацию данных и сохранение юзера в БД:",
            "Класс нарушает SRP, т.к выполняет несколько задач: валидацию email, создание пользователя и сохранение его в базу данных.",
            "Для исправления нарушения SRP нужно разделить обязанности на отдельные классы: User, UserValidator, UserDatabase, и UserCreator. Каждый класс будет отвечать только за одну задачу:",
            "Теперь каждый класс отвечает за одну конкретную задачу, что соответствует принципу единственной ответственности.",
            "Рассмотрим другой пример, обработку заказов в интернет-магазине. Изначально есть класс, который нарушает SRP, т.к он одновременно обрабатывает заказ, валидирует данные и отправляет уведомления:",
            "Рефакторинг этого класса для соответствия SRP:",
            "Фасадный паттерн помогает упростить взаимодействие между сложными подсистемами, предоставляя простой интерфейс для клиента. С фасадом можно скрыть сложность подсистем и предоставлять единый интерфейс для взаимодействия с ними.",
            "Предположим, есть система обработки заказов, включающая несколько классов для управления заказами, оплатами и уведомлениями. Без фасадного паттерна клиенту пришлось бы взаимодействовать с каждым из этих классов напрямую:",
            "А с использованием фасадного паттерна все будет выглядеть так:",
            "Интерфейсы и абстрактные классы помогают разделить обязанности и четко определить контракт, который должен реализовать класс.",
            "Создание интерфейсов для валидации, сохранения и уведомления:",
            "Разделяем обязанности на интерфейсы, что позволяет каждому классу реализовывать только свои специфические методы, соответствующие SRP.",
            "Для поддержки SRP и других принципов SOLID в Python можно использовать различные библиотеки.",
            "Pylint помогает анализировать код на наличие ошибок и несоответствий стилю, а также выявляет нарушения принципов SOLID, включая SRP.",
            "Mypy - статический анализатор типов для Python, который помогает обнаруживать типовые ошибки и улучшать структуру кода.",
            "Pytest помогает создавать модульные тесты для каждого отдельного компонента.",
            "Dataclasses модуль позволяет создавать классы данных, которые следуют SRP, отделяя логику данных от поведения.",
            "Про другие архитектурные принципы и инструменты коллеги из OTUS рассказывают в рамках практических онлайн-курсов. Также хочу напомнить о том, что в календаре мероприятий вы можете зарегистрироваться на ряд интересных и абсолютно бесплатных вебинаров."
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 12: Дата и время (издание 2024)",
        "Дата публикации": "2024-06-02, 19:47",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это двенадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как работать с датами и временем таким образом, чтобы это работало для всех ваших пользователей, независимо от того, где они проживают.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения",
            "Глава 12: Дата и время (Эта статья)",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Один из аспектов моего приложения для ведения микроблогов, который я долгое время игнорировал, - это отображение дат и времени. До сих пор я просто позволял Python отображать объект datetime в модели User и даже не потрудился отобразить его в модели Post. В этой главе вы узнаете, как работать с этими временными метками.",
            "Ссылки на GitHub для этой главы: Browse, Zip, Diff.",
            "Использование Python на сервере для отображения дат и времени, которые отображаются пользователям в их веб-браузерах, на самом деле не очень хорошая идея, потому что то, что сервер считает своим местным временем, не будет иметь смысла для пользователей, которые живут в другом часовом поясе.",
            "Совершенно ясно, что сервер должен управлять временем, которое является согласованным и независимым от его собственного местоположения и местоположения пользователей. Если это приложение разрастется до такой степени, что потребуется несколько производственных серверов в разных регионах мира, я бы не хотел, чтобы каждый сервер записывал временные метки в базу данных в разных часовых поясах, потому что это сделало бы невозможной работу с этими временами. Поскольку UTC является наиболее используемым единым часовым поясом и поддерживается в классе datetime, именно его я и собираюсь использовать.",
            "В главе 4 вы видели, как создавать временные метки UTC для записей в блоге. В качестве напоминания, вот краткий пример, показывающий, как это было сделано.:",
            "Но с этим подходом связана важная проблема. Пользователям из разных мест будет ужасно сложно определить, когда была сделана публикация, если они будут видеть время в часовом поясе UTC. Им нужно было бы заранее знать, что время указано в UTC, чтобы они могли мысленно подогнать его к своему собственному часовому поясу. Представьте пользователя, скажем, в часовом поясе PDT на Западном побережье США, который публикует что-то в 15: 00 и сразу видит, что сообщение появляется в 10: 00 по времени UTC, или, если быть более точным, в 22: 00. Это будет очень запутанно.",
            "Хотя стандартизация временных меток в соответствии с UTC имеет большой смысл с точки зрения сервера, это создает проблему удобства использования для пользователей. Цель этой главы - представить решение, которое сохраняет все временные метки, управляемые сервером, в часовом поясе UTC, не отталкивая пользователей.",
            "Очевидным решением проблемы является преобразование всех временных меток из сохраненных единиц UTC в местное время каждого пользователя при их отображении. Это позволяет серверу продолжать использовать UTC для обеспечения согласованности, в то время как преобразование \"на лету\", адаптированное к каждому пользователю, решает проблему удобства использования. Сложная часть этого решения - знать местоположение каждого пользователя.",
            "На многих веб-сайтах есть страница конфигурации, где пользователи могут указывать свой часовой пояс. Для этого мне потребуется добавить новую страницу с формой, в которой я представляю пользователям раскрывающийся список часовых поясов. При первом входе на сайт пользователей могут попросить ввести их часовой пояс в рамках регистрации.",
            "Хотя это достойное решение, решающее проблему, немного странно просить пользователей вводить часть информации, которую они уже настроили в своей операционной системе. Кажется, было бы эффективнее, если бы я мог просто получить настройки часового пояса с их компьютеров.",
            "Как выясняется, веб-браузер знает часовой пояс пользователя и предоставляет его через стандартные API JavaScript даты и времени. На самом деле есть два способа воспользоваться информацией о часовом поясе, доступной через JavaScript:",
            "Подход \"старой школы\" заключался бы в том, чтобы веб-браузер каким-то образом отправлял информацию о часовом поясе на сервер, когда пользователь впервые входит в приложение. Это можно было бы сделать с помощью вызова Ajax или гораздо проще с помощью мета-тега обновления. Как только сервер узнает часовой пояс, он может сохранить его в сеансе пользователя или записать в таблицу users в базе данных, и с этого момента корректировать с его помощью все временные метки во время отображения шаблонов.",
            "Подход \"новой школы\" заключается в том, чтобы ничего не менять на сервере и позволить преобразованию UTC в местный часовой пояс происходить в браузере с использованием JavaScript.",
            "Оба варианта допустимы, но второй имеет большое преимущество. Знания часового пояса пользователя не всегда достаточно для представления дат и времени в формате, ожидаемом пользователем. Браузер также имеет доступ к конфигурации языкового стандарта системы, которая определяет такие параметры, как время утра / вечера в сравнении с 24-часовыми часами, формат отображения даты DD / MM / ГГГГ в сравнении с MM / DD / ГГГГ и многие другие культурные или региональные стили.",
            "И если этого недостаточно, у подхода новой школы есть еще одно преимущество. Есть библиотека с открытым исходным кодом, которая выполняет всю эту работу!",
            "Moment.js это небольшая библиотека JavaScript с открытым исходным кодом, которая выводит отображение даты и времени на новый уровень, поскольку предоставляет все мыслимые варианты форматирования, а затем и некоторые другие. Некоторое время назад я создал Flask-Moment, небольшое расширение Flask, которое позволяет очень легко интегрировать moment.js в ваше приложение.",
            "Итак, давайте начнем с установки Flask-Moment:",
            "Это расширение добавляется в приложение Flask обычным способом:",
            "app/__init__.py: Пример Flask-Moment.",
            "В отличие от других расширений, Flask-Moment работает вместе с moment.js, поэтому все шаблоны приложения должны включать эту библиотеку. Чтобы гарантировать, что эта библиотека всегда доступна, я собираюсь добавить ее в базовый шаблон. Это можно сделать двумя способами. Самый прямой способ - явно добавить тег <script>, который импортирует библиотеку, но Flask-Moment упрощает задачу, предоставляя функцию moment.include_moment(), которая генерирует тег <script>:",
            "app/templates/base.html: Включить moment.js в базовый шаблон.",
            "В большинстве случаев библиотеки JavaScript, используемые приложением, включены в конец содержимого <body>, где находится загрузочный JavaScript-код.",
            "Moment.js делает класс moment доступным для браузера. Первым шагом для отображения временной метки является создание объекта этого класса, передающего желаемую временную метку в формате ISO 8601. Вот пример, запущенный в консоли JavaScript браузера:",
            "Если вы не знакомы со стандартным форматом даты и времени ISO 8601, этот формат выглядит следующим образом:",
            "Я уже решил, что буду работать только с часовыми поясами UTC, поэтому последней частью всегда будет +00:00 или в некоторых случаях эквивалент Z, который представляет UTC в стандарте ISO 8601.",
            "В объекте moment предусмотрено несколько методов для различных вариантов рендеринга. Ниже приведены некоторые из наиболее распространенных вариантов:",
            "В этом примере создается объект moment, инициализированный 28 июня 2021 года в 21:45 по Гринвичу. Вы можете видеть, что все параметры, которые я пробовал выше, отображаются в UTC + 1, который является часовым поясом, настроенным на моем компьютере. Вы можете ввести вышеуказанные команды в консоли вашего браузера, убедившись, что в странице, на которой вы открываете консоль, включена moment.js. Вы можете сделать это в микроблоге, при условии, что вы внесли вышеуказанные изменения для включения moment.js, или также на https://momentjs.com/.",
            "Обратите внимание, как разные методы создают разные представления. С помощью метода format() вы управляете форматом выходных данных с помощью строки формата. Метод fromNow() интересен тем, что он отображает временную метку по отношению к текущему времени, поэтому вы получаете выходные данные, такие как \"минуту назад\" или \"через два часа\" и т.д.",
            "Если вы работали непосредственно в JavaScript, приведенные выше вызовы возвращают строку с отображаемой временной меткой. Затем вам предстоит вставить этот текст в нужное место на странице, что, к сожалению, требует работы с DOM. Расширение Flask-Moment значительно упрощает использование moment.js за счет включения в ваши шаблоны объекта moment, аналогичного объекту JavaScript.",
            "Давайте посмотрим на временную метку, которая отображается на странице профиля. Текущий шаблон user.html позволяет Python генерировать строковое представление времени. Теперь я могу отобразить эту временную метку с помощью Flask-Moment следующим образом:",
            "app/templates/user.html: Отрисовка временной метки с помощью moment.js.",
            "Итак, как вы можете видеть, Flask-Moment использует синтаксис, аналогичный синтаксису библиотеки JavaScript, с одним отличием, заключающимся в том, что аргументом для moment() теперь является объект Python datetime, а не строка ISO 8601. Вызов moment(), выполняемый из шаблона, автоматически генерирует необходимый код JavaScript для вставки отображаемой временной метки в нужное место DOM.",
            "Второе место, где я могу воспользоваться преимуществами Flask-Moment, находится во вложенном шаблоне _post.html, который вызывается с главной страницы и страницы пользователя. В текущей версии шаблона каждому сообщению предшествует строка \"username says:\". Теперь я могу добавить временную метку, отображаемую с помощью fromNow():",
            "app/templates/_post.html: Отрисовка временной метки во вложенном шаблоне post.",
            "Ниже вы можете увидеть, как выглядят обе эти временные метки при рендеринге с помощью Flask-Moment и moment.js:"
        ]
    },
    {
        "Название статьи": "Мега-Учебник Flask Глава 11: Дизайн приложения (издание 2024)",
        "Дата публикации": "2024-06-02, 19:46",
        "Автор статьи": "Alex_Mer5er ",
        "Статья целиком": [
            "Это одиннадцатая часть серии мега-учебника Flask, в которой я собираюсь рассказать вам, как заменить базовые HTML-шаблоны новым набором, основанным на платформе пользовательского интерфейса Bootstrap.",
            "Глава 1: Привет, мир!",
            "Глава 2: Шаблоны",
            "Глава 3: Веб-формы",
            "Глава 4: База данных",
            "Глава 5: Логины пользователей",
            "Глава 6: Страница профиля и аватары",
            "Глава 7: Обработка ошибок",
            "Глава 8: Подписчики",
            "Глава 9: Разбивка на страницы",
            "Глава 10: Поддержка электронной почты",
            "Глава 11: Дизайн приложения (Эта статья)",
            "Глава 12: Даты и время",
            "Глава 13: I18n и L10n",
            "Глава 14: Ajax",
            "Глава 15: Улучшенная структура приложения",
            "Глава 16: Полнотекстовый поиск",
            "Глава 17: Развертывание в Linux",
            "Глава 18: Развертывание на Heroku",
            "Глава 19: Развертывание в контейнерах Docker",
            "Глава 20: Немного магии JavaScript",
            "Глава 21: Уведомления пользователей",
            "Глава 22: Фоновые задания",
            "Глава 23: Интерфейсы прикладного программирования (API)",
            "Вы уже некоторое время играете с моим приложением для ведения микроблогов, поэтому, я уверен, вы заметили, что я не потратил слишком много времени на то, чтобы оно выглядело хорошо, или, лучше сказать, я вообще не тратил на это времени. Шаблоны, которые я собрал, довольно простые, без какого-либо пользовательского оформления. Мне было полезно сосредоточиться на реальной логике приложения, не отвлекаясь на написание красивых HTML и CSS.",
            "Но я уже долго сосредоточен на серверной части этого приложения. Итак, в этой главе я сделаю перерыв и потрачу некоторое время на то, чтобы показать вам, что можно сделать, чтобы приложение выглядело немного более отточенным и профессиональным.",
            "Эта глава будет немного отличаться от предыдущих, потому что я не собираюсь так подробно, как обычно, описывать сторону Python, которая, в конце концов, является основной темой этого туториала. Создание красивых веб-страниц - обширная тема, которая в значительной степени не связана с веб-разработкой на Python, но я расскажу о некоторых основных рекомендациях и идеях о том, как подойти к этой задаче, и у вас также будет приложение с измененным дизайном, которое можно изучить и перенять опыт.",
            "Ссылки на GitHub для этой главы: Обзор, Zip, Diff.",
            "Хотя мы можем утверждать, что программирование - это сложно, наши усилия ничто по сравнению с усилиями веб-дизайнеров, которым приходится создавать веб-страницы, которые красиво и единообразно выглядят в списке веб-браузеров. За последние годы они стали лучше, но в некоторых браузерах все еще есть непонятные ошибки или причуды, которые сильно усложняют задачу создания веб-страниц, которые везде выглядят красиво. Это еще сложнее, если вам также нужно настроить для браузеров планшетов и смартфонов с ограниченным количеством ресурсов и экранов.",
            "Если вы, как и я, разработчик, который просто хочет создавать прилично выглядящие веб-страницы, но у вас нет времени или интереса изучать низкоуровневые механизмы для эффективного достижения этой цели путем написания необработанного HTML и CSS, то единственным практическим решением является использование CSS фреймворка для упрощения задачи. Выбрав этот путь, вы потеряете некоторую творческую свободу, но, с другой стороны, ваши веб-страницы будут хорошо выглядеть во всех браузерах без особых усилий. Фреймворк CSS предоставляет коллекцию высокоуровневых классов CSS с готовыми стилями для распространенных типов элементов пользовательского интерфейса. Большинство этих фреймворков также предоставляют дополнения JavaScript для вещей, которые нельзя выполнить строго с помощью HTML и CSS.",
            "Одним из самых популярных CSS-фреймворков является Bootstrap. Если вы хотите увидеть, какие страницы можно создавать с помощью этого фреймворка, в документации есть несколько примеров.",
            "Вот некоторые преимущества, которые вы получаете при использовании Bootstrap для оформления ваших веб-страниц:",
            "Аналогично выглядит во всех основных веб-браузерах",
            "Настройка размеров для экранов настольных компьютеров, планшетов и телефонов",
            "Настраиваемые макеты",
            "Красиво оформленные панели навигации, формы, кнопки, оповещения, всплывающие окна и т.д.",
            "Самый простой способ использовать Bootstrap - это просто импортировать файл bootstrap.min.css в ваш базовый шаблон. Вы можете либо загрузить копию этого файла и добавить его в свой проект, либо импортировать его непосредственно из CDN. Затем вы можете начать использовать CSS-классы общего назначения, которые он предоставляет, согласно документации, что довольно неплохо. Возможно, вы также захотите импортировать JavaScript-код фреймворка, чтобы использовать самые продвинутые функции.",
            "Как и большинство проектов с открытым исходным кодом, Bootstrap постоянно развивается. Оригинальная версия мега-учебника Flask была создана для Bootstrap 3. Редакция, которую вы сейчас читаете, создана для Bootstrap 5.3. Текущий подход к интеграции Bootstrap является довольно общим и может быть адаптирован к более новым версиям Bootstrap.",
            "Первым шагом в интеграции Bootstrap с Microblog является добавление его файлов CSS и JavaScript в базовый шаблон. На странице быстрого запуска Bootstrap в качестве примера приведена короткая, но полная HTML-страница, которую я копирую ниже для вашего удобства:",
            "Подход, который я могу применить, чтобы объединить это с моим шаблоном base.html, заключается в том, чтобы использовать приведенный выше в качестве нового базового шаблона, заменив теги <title> и <h1> заголовком и основным содержимым исходного базового шаблона соответственно.",
            "Следующий шаг - заменить базовую панель навигации на более удобную из Bootstrap. На странице документации по панели навигации Bootstrap вверху показан хороший пример. Используя этот пример в качестве руководства, я создал панель навигации со ссылками \"Index\", \"Explore\", \"Profile\", \"Login\" и \"Logout\" из микроблога. Для удобства я настроил профиль, а также ссылки для входа и выхода так, чтобы они отображались в крайнем правом углу.",
            "При использовании Bootstrap полезно знать о некоторых базовых примитивах компоновки. Одним из наиболее важных является контейнер, который определяет область содержимого страницы. Два основных контейнера называются container и container-fluid. В первом случае страница настраивается на использование одной из пяти предопределенных ширин страницы и центрирует содержимое в окне браузера. С другой стороны обтекающий контейнер дает вам доступ ко всей ширине страницы. Для этого приложения я решил использовать контейнер по умолчанию, потому что он предотвращает слишком широкое расширение страницы независимо от размера экрана, поэтому часть содержимого страницы будет заключена в один из этих контейнеров следующим образом:",
            "Последняя часть HTML-разметки в шаблоне base.html, которую необходимо адаптировать, - это раздел, отображающий отображаемые сообщения. Компонент Alert от Bootstrap прекрасно подходит для этой задачи.",
            "Вы можете получить полностью переработанный шаблон base.html из репозитория Github для этой главы. Ниже вы можете увидеть упрощенную структуру, если хотите иметь представление о том, как она выглядит.:",
            "app/templates/base.html: Переработанный базовый шаблон.",
            "Благодаря обновленному базовому шаблону внешний вид приложения уже заметно улучшен без необходимости изменять строки кода Python. Если вы хотите убедиться в этом сами, загрузите копию base.html из репозитория GitHub по ссылкам, приведенным в начале этой главы.",
            "Область, в которой Bootstrap проделывает фантастическую работу, заключается в рендеринге полей формы, которые выглядят намного приятнее и чище, чем поля по умолчанию, предоставляемые браузером. В документации по Bootstrap также есть раздел о формах. В начале этого раздела приведен пример формы входа в систему, который показывает базовую структуру HTML.",
            "HTML-код, необходимый для каждого поля, довольно длинный. Ниже вы можете увидеть одно из текстовых полей из примера формы в документации:",
            "Но это слишком просто для нужд Microblog, который включает проверку полей и, возможно, потребуется показывать пользователю ошибки проверки. На странице документации есть раздел о проверке на стороне сервера, в котором показано, как оформить поля с сообщением об ошибке. Вот пример.:",
            "К сожалению, о необходимости вводить такое количество шаблонов для каждого поля в каждой форме не может быть и речи. Это заняло бы слишком много времени и чревато ошибками. Одним из решений является использование макросов Jinja, которые позволяют вам определять повторно используемые фрагменты HTML, а затем вызывать их из ваших шаблонов, как если бы они были функциями.",
            "Например, макрос Jinja для текстового поля, подобного показанному выше, будет иметь вид:",
            "Обратите внимание, как используются условные обозначения для выборочного добавления стиля ошибки, если поле содержит одно или несколько сообщений об ошибках.",
            "Поскольку макрос определен в файле с именем bootstrap_wtf.html, который расположен в каталоге templates, он может быть вызван, когда потребуется отобразить поле. Например:",
            "Макрос отображения полей можно расширить, чтобы он также поддерживал отображение флажков, раскрывающихся списков выбора, кнопок отправки и других типов полей. Он также может принимать второй аргумент с логическим значением, указывающим, следует ли автоматически переводить поле в фокус страницы, что должно быть сделано для первого поля формы. Для еще большего удобства можно создать другой макрос для рендеринга всей формы, просто перебрав поля формы и вызвав form_field() макрос для каждого из них.",
            "Полный bootstrap_wtf.html файл доступен в репозитории GitHub, ссылка на который приведена в начале этой главы. Он включает в себя более полную версию макроса form_field(), показанного выше, и второй макрос с именем quick_form(), который принимает объект формы и отображает все его поля с помощью первого макроса.",
            "Как это выглядит, когда реализовано в реальной форме? Ниже вы можете увидеть переработанный шаблон register.html в качестве примера:",
            "app/templates/register.html: Шаблон регистрации пользователя.",
            "Разве это не здорово? Оператор import вверху работает аналогично импорту Python на стороне шаблона. Который добавляет макрос wtf.quick_form(), который в одной строке кода отображает полную форму, включая ошибки проверки, и все оформлено в соответствии с фреймворком Bootstrap.",
            "Еще раз, я не собираюсь показывать вам все изменения, которые я сделал для других форм в приложении, но все эти изменения внесены в шаблоны, которые вы можете загрузить или просмотреть на GitHub.",
            "Логика представления, которая отображает отдельные записи в блоге, была абстрагирована в подшаблон под названием _post.html. Все, что мне нужно сделать с этим шаблоном, это внести некоторые незначительные корректировки, чтобы он хорошо выглядел в Bootstrap.",
            "app/templates/_post.html: Переработанный подшаблон публикации.",
            "Ссылки на страницы - это еще одна область, в которой Bootstrap предоставляет поддержку. Для этого я просто еще раз обратился к документации Bootstrap и адаптировал один из их примеров. Вот как это выглядит на странице index.html:",
            "app/templates/index.html: Переработаны ссылки на страницы.",
            "Обратите внимание, что в этой реализации вместо скрытия следующей или предыдущей ссылки, когда в этом направлении больше нет содержимого, я применяю отключенное состояние, из-за которого ссылка будет отображаться серым цветом.",
            "Я не собираюсь показывать это здесь, но аналогичное изменение необходимо применить к шаблону user.html. Пакет для загрузки этой главы включает эти изменения.",
            "Чтобы внести в ваше приложение эти изменения, пожалуйста, загрузите zip-файл для этой главы и соответствующим образом обновите свои шаблоны.",
            "Ниже вы можете увидеть несколько фотографий до и после, чтобы увидеть трансформацию. Имейте в виду, что это изменение было достигнуто не затрагивая ни одной строки кода приложения!",
            "Следующая глава =>"
        ]
    },
    {
        "Название статьи": "Расширяем возможности Keras с помощью кастомных слоев",
        "Дата публикации": "2024-06-02, 18:07",
        "Автор статьи": "badcasedaily1 ",
        "Статья целиком": [
            "Привет, Хабр!",
            "Keras предоставляет мощные инструменты для создания сложных нейронных сетей. Однако иногда стандартного набора слоев недостаточно для решения некоторых задач. В таких случаях на помощь приходят кастомные слои.",
            "Кастомные слои позволяют адаптировать архитектуру модели под особенности данных, улучшая тем самым производительность и точность моделек.",
            "Каждый кастомный слой начинается с определения нового класса, наследующего от tf.keras.layers.Layer. В __init__ происходит инициализация слоя, где можно задать параметры, необходимые для работы слоя:",
            "Тут units определяет количество нейронов, а activation указывает функцию активации. super(CustomLayer, self).__init__(**kwargs) вызывает конструктор базового класса Layer.",
            "Метод build вызывается Keras при первом использовании слоя. Его юзают для создания параметров слоя, которые зависят от размера входных данных:",
            "В методе создаются веса kernel и bias. Функция add_weight создает и регистрирует переменные слоя, которые будут обновляться во время тренировки.",
            "Метод call содержит основную логику вычислений слоя. Он принимает входные данные и возвращает выходные:",
            "В этом методе выполняется умножение входных данных на веса и добавление смещения. Если определена функция активации, она применяется к выходным данным.",
            "После определения кастомного слоя его можно использовать в моделях Keras как обычный слой:",
            "Другие полезные методы:",
            "add_weight: Добавляет переменную веса в слой.",
            "compute_output_shape: Возвращает форму выходных данных на основе формы входных.",
            "get_config: Возвращает конфигурацию слоя в виде словаря, что полезно для сериализации.",
            "Dense слой выполняет простую линейную операцию: умножение входного вектора на матрицу весов и добавление смещения, а затем применяется функция активации:",
            "Convolutional слои применяют свертку фильтра к входным данным, что позволяет выделять пространственные особенности:",
            "Recurrent слои используются для обработки последовательных данных. Один из наиболее распространнных типов рекуррентных слоев — это LSTM:",
            "Dropout слой используется для регуляризации модели, предотвращая переобучение путем случайного зануления некоторых нейронов во время тренировки:",
            "BatchNormalization слой нормализует активации предыдущего слоя, улучшая скорость обучения модельки:",
            "Больше практических инструментов и кейсов коллеги из OTUS рассматривают в рамках практических онлайн-курсов. Напомню, что с полным каталогом курсов можно ознакомиться по ссылке."
        ]
    },
    {
        "Название статьи": "Enbeddrus — обучение независящей от языка эмбеддинг-модели",
        "Дата публикации": "2024-06-02, 17:31",
        "Автор статьи": "efreelancer ",
        "Статья целиком": [
            "Приветствую, хабровчане!",
            "Сегодня хочу рассказать вам историю о том, как я обучил простую и компактную независящую от языка (language agnostic) модель-эмбеддер, которая умеет работать с техническими текстами о PHP и способна извлекать схожие эмбеддинги для параллельных текстов на английском и русском языках.",
            "Основная причина, по которой я решил заняться этим проектом, заключается в том, что мои заметки, код и документация, накопленные за более чем десять лет практики, представляют собой солянку текстов о разных технологиях, языках программирования, пометки о настройке серверов Linux и т.д. на русском и английском языках. Поэтому мне захотелось сделать Retrieval-Augmented Generation (RAG) помогалку, которая сможет принимать запросы пользователя (меня) и эффективно находить информацию в столь разношерстой базе данных, независимо от того на каком языке я сделал запрос и на каком языке написана документация.",
            "Для достижения этой цели как-раз и необходима независимая от языка модель-эмбеддер, которая будет одинаково хорошо работать с техническими текстами на русском и английском языках.",
            "Ещё одним важным аспектом было то, чтобы модель потребляла как можно меньше ресурсов и, если возможно, чтобы её можно было преобразовать в формат GGUF.",
            "Но прежде чем приступить к созданию своего собственного велосипеда, я решил поискать готовые решения, ведь подобная идея очевидна и, возможно, уже реализована другими.",
            "Спойлер: идея не нова, и подобных решений уже достаточно много.",
            "Для построения системы, которая может извлекать одинаковые эмбеддинги для схожих текстов на русском и английском языках, существует несколько решений, например...",
            "Ссылки: arxiv:1907.04307 , kaggle, github",
            "Это проект разработан инженерами Google и поддерживает 16 языков.",
            "Свойства: ~110m параметров, принимает на вход 128 токенов текста и извлекает из них 512-мерный эмбеддинг.",
            "Плюс: поддерживает русский язык.",
            "Минусы: модель основана на Tensorflow, а так же что с 2019го года не было обновлений.",
            "Ссылки: arxiv:1710.04087, github",
            "Это одна из первых попыток инженеров FB создать модель которая способна выполнять задачи по извлечению независящих от языка эмбеддингов.",
            "Плюс: поддерживает русский язык.",
            "Минусы: в наличии имеются веса для пар языков, навроде en-ru, en-de и т.д., весов нет на HuggingFace, ну и с 2018го года проект не развивается.",
            "Ссылки: arvix:2205.12654, github, pypi",
            "Ещё одна модель разработана инженерами FB и, как сказано в ридми на GitHub, поддерживает более 200 языков (хотя если пройти по ссылочкам и посчитать то получится 147 языков).",
            "Свойства: ~256m параметров, принимает 1024 токенов на вход и извлекает из них 1024-мерный эмбеддинг.",
            "Плюсы: она основана на PyTorch и имеет логику переключения между языками которая явно перекочевала из NLLB (о которой я кстати рассказывал в публикации \"Перевод на разные языки используя модель NLLB\" у себя в блоге на Дзен).",
            "Минусы: весов нет на HuggingFace, а модель несовместима с llama.cpp поэтому её не получится конвертировать в GGUF, чтобы можно было запускать на слабом железе (или же в паре с ollama).",
            "Ссылки: arXiv:1908.10084, сайт",
            "Модели Sentence-BERT представляют собой модифицированную версию предобученной BERT, специально адаптированную для генерации эмбеддингов предложений, multilingual версия позволяет извлекать эмбеддинги из текста на разных языках, а paraphrase модели позволяют извлекать похожие эмбеддинги парафраз на разных языках.",
            "Вот пару примечательных моделей, обученных разными способами:",
            "paraphrase-multilingual-MiniLM-L12-v2 имеет 118m параметров, принимает 256 токенов на вход и возвращает 384-мерный эмбеддинг.",
            "paraphrase-multilingual-mpnet-base-v2 имеет 278m параметров, принимает на вход 512 токенов и возвращает 768-мерный эмбеддинг.",
            "Обе эти модели обучены на комбинации из датасетов:",
            "SNLI о котором говорится в публикации \"A large annotated corpus for learning natural language inference\" (570k примеров)",
            "Multi-Genre NLI, подробнее в работе \"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\" (433k примера)",
            "Плюсы: поддерживает русский язык, можно конвентировать в GGUF.",
            "Минусы: модели не очень хорошо понимают технический текст (особенно русский технический жаргон), нет версии в формате GGUF, и к числу фатальных недостатков могу отнести, что эти модели обучил не я ;)",
            "Пришёл к выводу, что тему обучения подобных модей-эмбеддеров уже достаточно хорошо изучили и что можно без особых сложностей реализовать мою задумку.",
            "В качестве базовой модели решил взять модель google-bert/bert-base-multilingual-uncased, потому что:",
            "У этой крохи всего 168m параметров, что чуть больше чем у paraphrase-multilingual-MiniLM-L12-v2, но меньше чем у paraphrase-multilingual-mpnet-base-v2;",
            "На вход она принимает 512 токенов, а на выходе возвращает 768-мерный эмбеддинг, столько же у paraphrase-multilingual-mpnet-base-v2;",
            "Модель обучена на датасете wikipedia представляющем из себя Text Corpora, а там, сами понимаете, примеров текста больше, чем SNLI и Multi-Genre NLI вместе взятые;",
            "Модель uncased, то есть обучение происходило на регистронезависимых текстах (сиречь всё переводилось в lowercase).",
            "С моделью определились, теперь перейдём к вопросу выбора датасета...",
            "Изначально я хотел собрать больше датасетов, но, собирая датасет по PHP, я понял, какой это трудоёмкий процесс, и решил уменьшить свои амбиции.",
            "Итак, после поиска в интернете я нашёл только один подходящий датасет: OPUS PHP v1 на 2k примеров, содержащий пары текстов на русском и английском языках, по теме PHP.",
            "Из указанного датасеста я использовал только английский корпус (так как русский корпус был очень низкого качества), далее задействовал инстанс LibreTranslate для перевода английских текстов на русский и очистил данные от аномалий и шума (сценарий dataset_php_build.ipynb). Затем вручную перевёл кривые места с помощью Google и Yandex Translate и экспортировал результат в CSV формат. Данные отсортировал и удалил дубликаты (сценарий dataset_php_undup.py) после чего осталось 1.6k примеров.",
            "В финале попросил ChatGPT сгенерировать 100 примеров пар технического текста о PHP на русском и английском языках для сплита eval, а очищенные данные использовал для сплита train.",
            "Результат выгрузил (сценарий dataset_php_publish.ipynb) на HuggingFace: evilfreelancer/opus-php-en-ru-cleaned .",
            "Для создания эффективного эмбеддера, способного работать с техническими текстами о PHP на русском и английском языке, я решил провести обучение модели в два этапа, сначала выполнить Domain Adaptation, чтобы модель могла работать с техническими текстами на английском языке, а после этого обучить её на Parallel Corpora из русских и английских текстов.",
            "Для Domain Adaptation я использовал метод Generative Pseudo Labeling (GPL) (arXiv:2112.07577), данный метод позволяет проводить обучение модели на основе неразмеченных данных, генерируя псевдометки и улучшая качество работы модели для специфических доменов.",
            "Библиотека gpl имеет захардкоженный формат входного датасета и читает данные по определённым путям, поэтому пришлось слегка конвертировать тренировочный датасет и положить результат в директорию datasets (сценарий: dataset_php_convert.py).",
            "Для адаптации модели bert-base-multilingual-uncased к домену английских текстов про PHP я использовал в качестве шаблона скрипт, предложенный авторами проекта GPL на их странице на GitHub, получился следующего вида код:",
            "Полный скрипт тренировки train_domain.py можно найти в репозитории проекта на GitHub.",
            "Процесс обучения включает в себя несколько этапов:",
            "Используется генератор запросов, такой как BeIR/query-gen-msmarco-t5-base-v1, для создания синтетических запросов на основе текстов из корпуса;",
            "С помощью ретриверов, таких как msmarco-distilbert-base-v3 и msmarco-MiniLM-L-6-v3, которые работают с косинусным сходством, извлекаются наиболее релевантные документы для сгенерированных запросов;",
            "Кросс-энкодер, такой как cross-encoder/ms-marco-MiniLM-L-6-v2, используется для создания псевдометок, присваивая оценочные метки соответствия между запросами и документами;",
            "Модель обучается с использованием MarginMSELoss, которая позволяет модели лучше адаптироваться к новому домену.",
            "И так, наша модель обучена работать с новым доменом, поэтому переходим к следующему шагу.",
            "Для обучения модели на параллельных корпусах я использовал метод обучения моделей на разных языках, описанный в примере на сайте Sentence Transformers. Этот метод позволяет обучать мультиязычные модели, используя параллельные тексты на разных языках (заготовка скрипта make_multilingual.py).",
            "Для оценки качества модели я написал юпитер-блокнот, который загружает базовую и дообученную модель, прогоняет пары из eval сплита датасета evilfreelancer/opus-php-en-ru-cleaned и анализирует разницу между эмбеддингами, построенными для текстов на разных языках. Результаты визуализируются в виде графиков. Скрипт можно найти здесь.",
            "На графике видно, что базовая модель bert-base-multilingual-uncased распределяет русские и английские тексты в изолированные кластеры точек, ну а наша задача сделать так, чтобы эти точки были расположены как можно ближе друг к другу.",
            "Подобную задачу позволяет решать MSELoss, так как она минимизирует разницу между эмбеддингом, сгенерированным моделью-учителем (на английском языке) и эмбеддингом, сгенерированным моделью-учеником (на русском языке).",
            "Теперь пару слов про датасеты, решил остановиться на следующем наборе:",
            "evilfreelancer/opus-php-en-ru-cleaned (1.6k) - ранее созданный датасет параллельных текстов на английском и русском языках;",
            "Helsinki-NLP/opus_books (17.5k) - датасет OPUS параллельных текстов из книг.",
            "Выбрал я их потому, что мои первые эксперименты с обучением модели на только PHP датасете показали, что у модели происходит overfitting в результате чего падала общее качество работы модели, поэтому самым логичным решением было добавить ещё один Parallel Corpora общего назначения.",
            "Помимо этого в скрипт обучения я хотел сразу заложить возможность обучать на множестве разных датасетов (имеющих разные форматы данных), в результате чего получилась функция:",
            "В дальнейшем планирую добавить в неё больше датасетов на разные технические темы, но на этапе прототипирования того что есть более чем достаточно.",
            "Двигаемся дальше.",
            "Полный скрипт тренировки train_parallel.py можно найти в репозитории проекта на GitHub, в качестве модели-учителя возьмём google-bert/bert-base-multilingual-uncased, а в качестве модели-ученика ту, что мы обучили ранее на шаге Domain Adaptation.",
            "Обучение происходит в несколько этапов:",
            "Сначала мы загружаем датасеты (функция read_datasets);",
            "Далее выполняем их преобразование в нужный формат, после чего сохраняем на диске (функциия prepare_datasets)",
            "Инициализируем модель-учитель и модель-ученик (тут)",
            "Инициализируем MSELoss, передав ей на вход указатель на модель-ученика (тут)",
            "Запускаем обучение модели-ученика",
            "По завершению обучению давайте попробуем протестировать модель и понять стала ли на лучше извлекать эмбеддинги.",
            "Как видно на графике эмбеддинги извлечённые из русских и английских текстов где-то наложились друг на друга, точность похожести поднялась с 0.83 до 0.94, при этом модель также хорошо разделяет фразы различающиеся по смыслу.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1-domain",
            "Посмотрел я на этот график и пришла в голову мысль, а что если попробовать обучить базовую модель сразу на Parallel Corpora, пропустив шаг с Domain Adaptation?",
            "Правим скрипт тренировки, меняем модель-ученика, получается вот так:",
            "Опять запускаем тренировку и ждём некоторое время, по завершению прогоняем тесты и смотрим что получилось.",
            "Как видно на графиках если обучать сразу на Parallel Corpora модель быстрее, так как не нужно выполнять Domain Adaptation, и лучше обучается извлекать эмбеддинги из параллельных текстов, ведь косинусное расстояние в таком случае между близкими по смыслу фразами на разных языках в среднем в районе 0.97, что выше чем у модели изначально обученной на домене текстов про PHP.",
            "Веса обученной модели доступны тут: evilfreelancer/enbeddrus-v0.1",
            "Отсюда можно сделать вывод, что дообучение мультиязыковой модели bert-base-multilingual-cased через Domain Adaptation с последующем обучением на Parallel Corpora не имеет особого смысла и проще сразу дообучать её на Parallel Corpora.",
            "Осталось выполнить самую малость, для начала я хочу конвертировать модель в формат GGUF, чтобы можно было использовать обученные модели через llama.cpp, но на этом моменте сильно не будем заострять внимание, сошлюсь на мою публикацию \"Как конвертировать модель BERT в формат GGUF?\" в моём блоге и PR который я создал в проекте llama.cpp.",
            "Но если кратко команды конвертации нужно выполнять и корня проекта llama.cpp и выглядят они следующим образом:",
            "По её завершению в директории models появятся файлы: enbeddrus-v0.1-f16.gguf и enbeddrus-v0.1-domain-f16.gguf.",
            "Полученные модели я выгрузил на серверы Ollama следующим образом:",
            "Выгруженные модели находятся тут и скачать их можно следующей командой:",
            "Содержимое Modelfile'ов можно найти в директории models проекта на GitHub.",
            "https://github.com/EvilFreelancer/enbeddrus",
            "https://huggingface.co/datasets/evilfreelancer/opus-php-en-ru-cleaned",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1-domain",
            "https://huggingface.co/evilfreelancer/enbeddrus-v0.1",
            "https://ollama.com/evilfreelancer/enbeddrus",
            "Благодаря работе над проектом enbeddrus были достигнуты следующие цели:",
            "Удалось разобрался с тем как подобные модели устроены и как они работают, а так же с тем как их можно обучать;",
            "Был собран датасет с Parallel Corpora тематических текстов о PHP на русском и английском;",
            "Удалось разобраться с методами оценки моделей, а также с тем как эту оценку красиво визуализировать;",
            "Была обучена модель, которая эффективно работает с текстами на двух языках и может быть использована в RAG-системе для поиска и анализа информации.",
            "Полученные результаты подтверждают, что обучение мультиязычных эмбеддеров на основе параллельных корпусов является эффективным подходом для создания моделей, способных работать с текстами на разных языках.",
            "Спасибо за внимание и за что дочитал публикацию до конца! Если у вас есть вопросы или вы хотите связаться со мной, ссылки на мои контакты в социальных сетях можно найти в моём профиле на Хабре."
        ]
    },
    {
        "Название статьи": "Я научу вас неправильно играть в Hearts of iron. Оптимизация довоенной экономики: часть 2",
        "Дата публикации": "2024-06-02, 17:02",
        "Автор статьи": "Glasssparrow ",
        "Статья целиком": [
            "В прошлой части мы создали инструментарий, настало время им воспользоваться.",
            "За долю секунды мы можем провести симуляцию нескольких внутриигровых лет, что позволяет нам применить простейший метод исследования - метод перебора. И, раз уж мы всё равно будем перебирать, стоит также построить графики.",
            "В качестве испытуемой страны мы выберем, конечно, Советский Союз, условия будем выбирать близкие к реальному прохождению.",
            "Во-первых, рассмотрим торговлю. Торговля в игре зависит от многих факторов и не может быть оценена в симуляции, т.к. мы не работаем со всем миром (это потребует больших вычислительных мощностей). Таким образом, торговлю можно взять только из игры, что я и сделал, прокрутив 5 внутриигровых лет и записав количество фабрик получаемых от торговли. При этом закупки брались равными нулю (закупки сильно зависят от того что производит игрок, потому для общего случая я их просто игнорировал).",
            "1 установить_торговлю 18 120 установить_торговлю 10 210 установить_торговлю 7 365 установить_торговлю 5 730 установить_торговлю 9 1100 установить_торговлю 13 1450 установить_торговлю 25 1800 установить_торговлю 18",
            "Во-вторых, рассмотрим технологии. Нам важны технология индустрии и технология строительства. Моменты их развития также были получены из игры следующим образом: технологии исследовались без опережения по времени (с некоторыми погрешностями, конечно), в первую очередь строительство и индустрия, во вторую электроника, всё остальное исследовалось лишь для того чтобы сбить накапливающиеся во время простоя дополнительные 30 дней исследований. Никаких фокусов, решений и политик на исследования использовано не было.",
            "188 construction_tech # технология строительства 1 328 industry_tech # технология индустрии 1 511 construction_tech # технология строительства 2 511 industry_tech # технология индустрии 2 1285 construction_tech # технология строительства 3 1285 industry_tech # технология индустрии 3 1950 construction_tech # технология строительства 4 1950 industry_tech # технология индустрии 4",
            "В-третьих, обратим внимание на фокусы. Рассмотрим 4 варианта: 1) Стандартное быстрое закрытие паранойи с советником на гражданское строительство и частичной мобилизацией2) Оно же, но дополнительно поставим свободную торговлю, когда будет политка3) Также быстро закрываем паранойю, но частичную мобилизацию берем до советника на гражданское строительство (это будет стоить нам 30 политки)4) Вообще никаких фокусов не берем, только советник и частичная мобилизация.",
            "sov # тэг страны.140 добавить_гражданского_советника # если идти по пути Сталина то политки как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранойи 350 добавить_лимит_фабрик 0.1 # фокус ветки паранойи 350 добавить_бонус_строительства 0.05 # фокус ветки паранойи 350 добавить_фабрики 2 # фокус ветки паранойи 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov140 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает245 продвинуть_экономику # ранняя 245 продвинуть_экономику # частичная 320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная #320 pull_trade # можно поставить свободную торговлю 350 добавить_товары_народного_потребления -0.02 # фокус ветки паранои 350 добавить_лимит_фабрик 0.1 # фокус ветки паранои 350 добавить_бонус_строительства 0.05 # фокус ветки паранои 350 добавить_фабрики 2 # фокус ветки паранои 540 продвинуть_экономику # фокус на военную экономику 540 добавить_военные_заводы 2 # +2 военные фабрики от фокуса на военную экономику",
            "sov245 добавить_гражданского_советника # если идти по пути сталина то полики как раз хватает140 продвинуть_экономику # ранняя 140 продвинуть_экономику # частичная",
            "В общем и целом, получаем достаточно неплохие бонусы на промышленность, но отнюдь не максимальные (есть еще плакаты на -15% товаров народного потребления, есть множество других бонусов на промышленность в фокусах), но для общего случая этого и не нужно, мы (пока) не собираемся ограничивать игрока в свободе выбора фокусов.",
            "Рассмотрим получившиеся графики зависимости максимума количества военных заводов от количества фабрик которые мы строим (это важно, фабрики от фокусов в счетчик не идут) до переключения на военную промышленность.",
            "Видим что 5% от свободной торговли дают нам лишь 4 завода, при том что 2 завода мы можем получить за 30 политки, просто взяв частичную мобилизацию до советника. Скупают ресурсы у СССР или не так активно чтобы свободная торговля была в плюс, или слишком поздно, когда ресурсы уже нужны для производства.",
            "Также видим что бонусы от фокусов в ветке паранойи дают большое преимущество: 116 заводов против 145 (если мы не берем скидку, то нет смысла брать советника до мобилизации экономики). Честно говоря, не ожидал такой разницы, бонусы не выглядели для меня настолько сильными (+5% к скорости строительства, -2% тнп, 10% к лимиту зданий в провинции, 2 фабрики, 2 завода). Товаров народного потребления дают не так много, +5% это как бонус от свободной торговли, места под строительство у СССР и так хватает. Но в сумме разница выходит почти в 30 заводов.",
            "Ну и самое важное: видим малые производные на достаточно широком участке. В сущности, при строительстве от 28 до 50 фабрик, количество военных заводов на 1 января 1941 года остаётся +/- стабильным.",
            "Как было сказано выше, количество заводов относительно постоянно на участке от 28 фабрик до 50 фабрик, рассмотрим это две точки подробнее:",
            "Если начинать строить заводы раньше, то снаряжения к итоговой дате будет больше (см. площадь под графиками военных заводов. Также стоит учесть что эффективность производства будет нарастать со временем, значит, соотношение снаряжение будет больше соотношения площадей), однако я не уверен, насколько следует переходить к максимизации снаряжения. В игру добавили новую систему снабжения (да, для меня она всё еще новая), теперь она требует строительства пунктов снабжения и железных дорог. Таким образом, нельзя считать экономику эффективной если мы можем произвести снаряжение, но не можем доставить его до фронта. Портальные технологии еще более увеличивают важность логистики, т.к. вся произведенная техника сначала телепортируется в столицу и уже после этого отправляется на фронт вместе с пряниками из карамельной страны.",
            "Помимо оптимизации момента перехода с фабрик на военные заводы, можно также оптимизировать количество инфраструктуры. Можно оценить выгодность инфраструктуры при помощи следующего несложного скрипта (нужен только базовый python, никаких сторонних библиотек):",
            "Отдельно отмечу, что приведенный выше код не учитывает что построенные в процессе фабрики тоже будут строить.",
            "Строительство инфраструктуры, это совсем не то же самое что переход с фабрик на военные заводы. Наиболее выгодное количество инфраструктуры разнится в зависимости от количества слотов под строительство. Да и выгода не всегда велика. Скажем так, оптимальное строительство инфраструктуры потребует от игрока определенных усилий.",
            "Наиболее универсальная оптимизация удалась, но что делать с частными случаями пока не понятно. Баланс между логистикой и количеством снаряжения для разных государств будет разным. Понимание того как этот баланс устроен требует большего понимания самой игры (что требует играть в игру правильно, а это не наш случай).",
            "Оптимизация инфраструктуры выглядит многообещающе, но оценка желания игроков применять это оптимизацию - нет. Также сам процесс нахождения оптимального алгоритма строительства с инфраструктурой выглядит достаточно сложно.",
            "В текущей программе упущен такой момент как возможность аннексии других государств. Этот процесс не обязательно сопровождается войной (а тем более серьезной войной), так что под концепцию довоенной экономики вполне подходит. В программе уже реализован алгоритм реализации контроля провинций, остаётся лишь добавить механизм добавления провинций по тэгу страны владельца или по принципу выделяемых стран (например возвращение польских территорий можно реализовать через добавление к СССР всех провинций Белоруссии и Украины, которые еще не входят в состав Советского Союза). Подобное может потребовать расчета строительства для аннексируемых стран (они же тоже развивают свою экономику параллельно вам), что замедлит расчет, но можно упростить его просто до строительства военных заводов, думаю это не должно добавить много погрешности.",
            "С интерфейсом пока всё совсем не здорово. Идеи приходящие мне в голову или звучат очень сложно или звучат еще менее удобно чем редактирование текстовых файлов. Так что gui пока застопорился.",
            "readme.txt будет обновляться по мере внесения изменений в программу. Если вы считаете что в нём чего-то не хватает, можете мне написать в дискорде или прямо в комментарии под этой статьей. В целом, планирую постепенно работать над читаемостью кода и документацией проекта.",
            "Весь код репозитория распространяется по лицензии MIT, которая гласит следующее: \"делайте с кодом что хотите, но не надо из-за него со мной судиться\". В общем, свободное ПО и всё такое, развлекайтесь, если хотите."
        ]
    }
]